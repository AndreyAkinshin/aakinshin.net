<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quantile on Andrey Akinshin</title><link>https://aakinshin.net/tags/quantile/</link><description>Recent content in Quantile on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 27 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/quantile/index.xml" rel="self" type="application/rss+xml"/><item><title>Avoiding over-trimming with the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-overtrimming/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-overtrimming/</guid><description>&lt;p>Previously, I already discussed the
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed modification of the Harrell-Davis quantile estimator&lt;/a> several times.
I performed several numerical simulations that compare the statistical efficiency of this estimator
with the efficiency of the &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">classic Harrell-Davis quantile estimator&lt;/a> (HDQE)
and its &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized modification&lt;/a>;
I showed how we can improve the efficiency using &lt;a href="https://aakinshin.net/posts/customized-wthdqe/">custom trimming strategies&lt;/a>
and how to choose a &lt;a href="https://aakinshin.net/posts/thdqe-threshold/">good trimming threshold value&lt;/a>.&lt;/p>
&lt;p>In the heavy-tailed cases, the trimmed HDQE provides better estimations than the classic HDQE
because of its higher breakdown point.
However, in the light-tailed cases, we could get efficiency that is worse than
the baseline Hyndman-Fan Type 7 (HF7) quantile estimator.
In many cases, such an effect arises because of the over-trimming effect.
If the trimming percentage is too high or if the evaluated quantile is too far from the median,
the trimming strategy based on the highest-density interval may lead to an estimation
that is based on single order statistics.
In this case, we get an efficiency level similar to the Hyndman-Fan Type 1-3 quantile estimators
(which are also based on single order statistics).
In the light-tailed case, such a result is less preferable than Hyndman-Fan Type 4-9 quantile estimators
(which are based on two subsequent order statistics).&lt;/p>
&lt;p>In order to improve the situation, we could introduce the lower bound for the number of order statistics
that contribute to the final quantile estimations.
In this post, I look at some numerical simulations
that compare trimmed HDQEs with different lower bounds.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png" target="_blank" alt="LightAndHeavy__N05_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-dark.png" target="_blank" alt="LightAndHeavy__N05_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Optimal threshold of the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-threshold/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-threshold/</guid><description>&lt;p>The traditional quantile estimators (which are based on 1 or 2 order statistics) have great robustness.
However, the statistical efficiency of these estimators is not so great.
The Harrell-Davis quantile estimator has much better efficiency (at least in the light-tailed case),
but it&amp;rsquo;s not robust (because it calculates a weighted sum of all sample values).
I already wrote a &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">post about trimmed Harrell-Davis quantile estimator&lt;/a>:
this approach suggest dropping some of the low-weight sample values to improve robustness
(keeping good statistical efficiency).
I also perform a numerical simulations that &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">compare efficiency&lt;/a>
of the original Harrell-Davis quantile estimator against its trimmed and winsorized modifications.
It&amp;rsquo;s time to discuss how to choose the optimal trimming threshold
and how it affects the estimator efficiency.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png" target="_blank" alt="LightAndHeavy__N40_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-dark.png" target="_blank" alt="LightAndHeavy__N40_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Estimating quantile confidence intervals: Maritz-Jarrett vs. jackknife</title><link>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</guid><description>&lt;p>When it comes to estimating quantiles of the given sample,
my estimator of choice is the Harrell-Davis quantile estimator
(to be more specific, its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed version&lt;/a>).
If I need to get a confidence interval for the obtained quantiles,
I use the &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/#the-maritz-jarrett-method">Maritz-Jarrett method&lt;/a>
because it provides a &lt;a href="https://aakinshin.net/posts/quantile-ci-coverage/">decent coverage percentage&lt;/a>.
Both approaches work pretty nicely together.&lt;/p>
&lt;p>However, in the original paper by &lt;a href="https://doi.org/10.2307/2335999">Harrell and Davis (1982)&lt;/a>,
the authors suggest using the jackknife variance estimator in order to get the confidence intervals.
The obvious question here is which approach better: the Maritz-Jarrett method or the jackknife estimator?
In this post, I perform a numerical simulation that compares both techniques using different distributions.&lt;/p></description></item><item><title>Comparing the efficiency of the Harrell-Davis, Sfakianakis-Verginis, and Navruz-Özdemir quantile estimators</title><link>https://aakinshin.net/posts/hd-sv-no-efficiency/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sv-no-efficiency/</guid><description>&lt;p>In the previous posts, I discussed the statistical efficiency of different quantile estimators
(&lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">Efficiency of the Harrell-Davis quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators&lt;/a>).&lt;/p>
&lt;p>In this post, I continue this research and compare the efficiency of
the Harrell-Davis quantile estimator,
the &lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis quantile estimators&lt;/a>, and
the &lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir quantile estimator&lt;/a>.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Dispersion exponential smoothing</title><link>https://aakinshin.net/posts/dispersion-exponential-smoothing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dispersion-exponential-smoothing/</guid><description>&lt;p>In this &lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">previous post&lt;/a>,
I showed how to apply exponential smoothing to quantiles
using the &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted Harrell-Davis quantile estimator&lt;/a>.
This technique allows getting smooth and stable moving median estimations.
In this post, I&amp;rsquo;m going to discuss how to use the same approach
to estimate moving dispersion.&lt;/p></description></item><item><title>Quantile exponential smoothing</title><link>https://aakinshin.net/posts/quantile-exponential-smoothing/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-exponential-smoothing/</guid><description>&lt;p>One of the popular problems in time series analysis is estimating the moving &amp;ldquo;average&amp;rdquo; value.
Let&amp;rsquo;s define the &amp;ldquo;average&amp;rdquo; as a central tendency metric like the mean or the median.
When we talk about the moving value, we assume that we are interested in
the average value &amp;ldquo;at the end&amp;rdquo; of the time series
instead of the average of all available observations.&lt;/p>
&lt;p>One of the most straightforward approaches to estimate the moving average is the &lt;em>simple moving mean&lt;/em>.
Unfortunately, this approach is not robust: outliers can instantly spoil the evaluated mean value.
As an alternative, we can consider &lt;em>simple moving median&lt;/em>.
I already discussed a few of such methods:
&lt;a href="https://aakinshin.net/posts/mp2-quantile-estimator/">the MP² quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/">a moving quantile estimator based on partitioning heaps&lt;/a>
(a modification of the Hardle-Steiger method).
When we talk about &lt;em>simple moving averages&lt;/em>, we typically assume
that we estimate the average value over the last $k$ observations ($k$ is the &lt;em>window size&lt;/em>).
This approach is also known as &lt;em>unweighted moving averages&lt;/em> because
all target observations have the same weight.&lt;/p>
&lt;p>As an alternative to the simple moving average, we can also consider the &lt;em>weighted moving average&lt;/em>.
In this case, we assign a weight for each observation and aggregate the whole time series according to these weights.
A famous example of such a weight function is &lt;em>exponential smoothing&lt;/em>.
And the simplest form of exponential smoothing is the &lt;em>exponentially weighted moving mean&lt;/em>.
This approach estimates the weighted moving mean using exponentially decreasing weights.
Switching from the simple moving mean to the exponentially weighted moving mean provides some benefits
in terms of smoothness and estimation efficiency.&lt;/p>
&lt;p>Although exponential smoothing has advantages over the simple moving mean,
it still estimates the mean value which is not robust.
We can improve the robustness of this approach if we reuse the same idea for weighted moving quantiles.
It&amp;rsquo;s possible because the quantiles also can be estimated for weighted samples.
In one of my previous posts, I &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">showed&lt;/a> how to adapt
the Hyndman-Fan Type 7 and Harrell-Davis quantile estimators to the weighted samples.
In this post, I&amp;rsquo;m going to show how we can use this technique to estimate
the weighted moving quantiles using exponentially decreasing weights.&lt;/p></description></item><item><title>Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators</title><link>https://aakinshin.net/posts/wthdqe-efficiency/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wthdqe-efficiency/</guid><description>&lt;p>In previous posts, I suggested two modifications of the Harrell-Davis quantile estimator:
&lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>.
Both modifications have a higher level of robustness in comparison to the original estimator.
Also, I &lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">discussed&lt;/a> the &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>
of the Harrell-Davis quantile estimator.
In this post, I&amp;rsquo;m going to continue numerical simulation and estimate the efficiency of
the winsorized and trimmed modifications.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Trimmed modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/trimmed-hdqe/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/trimmed-hdqe/</guid><description>&lt;p>In one of &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">the previous posts&lt;/a>, I discussed winsorized Harrell-Davis quantile estimator.
This estimator is more robust than the classic Harrell-Davis quantile estimator.
In this post, I want to suggest another modification that may be better for some corner cases:
the &lt;em>trimmed&lt;/em> Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Efficiency of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/hdqe-efficiency/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hdqe-efficiency/</guid><description>&lt;p>One of the most essential properties of a quantile estimator is
its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
In simple words, the efficiency describes the estimator accuracy.
The Harrell-Davis quantile estimator is a good option to achieve higher efficiency.
However, this estimator may provide lower efficiency in some special cases.
In this post, we will conduct a set of simulations that show the actual efficiency numbers.
We compare different distributions (symmetric and right-skewed, heavy-tailed and light-tailed),
quantiles, and sample sizes.&lt;/p></description></item><item><title>Navruz-Özdemir quantile estimator</title><link>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</guid><description>&lt;p>The Navruz-Özdemir quantile estimator
suggests the following equation to estimate the $p^\textrm{th}$ quantile of sample $X$:&lt;/p>
$$
\begin{split}
\operatorname{NO}_p =
&amp; \Big( (3p-1)X_{(1)} + (2-3p)X_{(2)} - (1-p)X_{(3)} \Big) B_0 +\\
&amp; +\sum_{i=1}^n \Big((1-p)B_{i-1}+pB_i\Big)X_{(i)} +\\
&amp; +\Big( -pX_{(n-2)} + (3p-1)X_{(n-1)} + (2-3p)X_{(n)} \Big) B_n
\end{split}
$$
&lt;p>where $B_i = B(i; n, p)$ is probability mass function of the binomial distribution $B(n, p)$,
$X_{(i)}$ are order statistics of sample $X$.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1111/bmsp.12198">&amp;ldquo;A new quantile estimator with weights based on a subsampling approach&amp;rdquo;&lt;/a> (2020)
by Gözde Navruz and A. Fırat Özdemir.
Also, I add some additional explanations,
simplify the final equation,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Sfakianakis-Verginis quantile estimator</title><link>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</guid><description>&lt;p>There are dozens of different ways to estimate quantiles.
One of these ways is to use the Sfakianakis-Verginis quantile estimator.
To be more specific, it&amp;rsquo;s a family of three estimators.
If we want to estimate the $p^\textrm{th}$ quantile of sample $X$,
we can use one of the following equations:&lt;/p>
$$
\begin{split}
\operatorname{SV1}_p =&amp;
\frac{B_0}{2} \big( X_{(1)}+X_{(2)}-X_{(3)} \big) +
\sum_{i=1}^{n} \frac{B_i+B_{i-1}}{2} X_{(i)} +
\frac{B_n}{2} \big(- X_{(n-2)}+X_{(n-1)}-X_{(n)} \big),\\
\operatorname{SV2}_p =&amp; \sum_{i=1}^{n} B_{i-1} X_{(i)} + B_n \cdot \big(2X_{(n)} - X_{(n-1)}\big),\\
\operatorname{SV3}_p =&amp; \sum_{i=1}^n B_i X_{(i)} + B_0 \cdot \big(2X_{(1)}-X_{(2)}\big).
\end{split}
$$
&lt;p>where $B_i = B(i; n, p)$ is probability mass function of the binomial distribution $B(n, p)$,
$X_{(i)}$ are order statistics of sample $X$.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1080/03610910701790491">&amp;ldquo;A new family of nonparametric quantile estimators&amp;rdquo;&lt;/a> (2008)
by Michael E. Sfakianakis and Dimitris G. Verginis.
Also, I add some additional explanations,
reconstruct missing steps,
simplify the final equations,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Winsorized modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/winsorized-hdqe/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/winsorized-hdqe/</guid><description>&lt;p>The Harrell-Davis quantile estimator is one of my favorite quantile estimators
because of its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
It has a small mean square error which allows getting accurate estimations.
However, it has a severe drawback: it&amp;rsquo;s not robust.
Indeed, since the estimator includes all sample elements with positive weights,
its &lt;a href="https://en.wikipedia.org/wiki/Robust_statistics#Breakdown_point">breakdown point&lt;/a> is zero.&lt;/p>
&lt;p>In this post, I want to suggest modifications of the Harrell-Davis quantile estimator
which increases its &lt;em>robustness&lt;/em> keeping almost the same level of &lt;em>efficiency&lt;/em>.&lt;/p></description></item><item><title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/unbiased-mad-hd/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad-hd/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> ($\textrm{MAD}$)
is a robust measure of scale.
In the previous post, I &lt;a href="https://aakinshin.net/posts/unbiased-mad/">showed&lt;/a>
how to use the &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased&lt;/a>
version of the $\textrm{MAD}$ estimator
as a robust alternative to the standard deviation.
&amp;ldquo;Unbiasedness&amp;rdquo; means that such estimator&amp;rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the &lt;a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias–variance tradeoff&lt;/a>:
when we remove the bias of the $\textrm{MAD}$ estimator,
we increase its variance and mean squared error ($\textrm{MSE}$).&lt;/p>
&lt;p>In this post, I want to suggest a more &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficient&lt;/a>
unbiased $\textrm{MAD}$ estimator.
It&amp;rsquo;s also a consistent estimator for the standard deviation, but it has smaller $\textrm{MSE}$.
To build this estimator,
we should replace the classic &amp;ldquo;straightforward&amp;rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&amp;rsquo;s discuss this approach in detail.&lt;/p></description></item><item><title>Unbiased median absolute deviation</title><link>https://aakinshin.net/posts/unbiased-mad/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> ($\textrm{MAD}$)
is a robust measure of scale.
For distribution $X$, it can be calculated as follows:&lt;/p>
$$
\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
$$
&lt;p>where $C$ is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the $\textrm{MAD}$ as a &lt;a href="https://en.wikipedia.org/wiki/Consistent_estimator">consistent estimator&lt;/a>
for the standard deviation under the normal distribution,
we should set&lt;/p>
$$
C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
$$
&lt;p>where $\Phi^{-1}$ is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If $X$ is the normal distribution, we get $\textrm{MAD} = \sigma$ where $\sigma$ is the standard deviation.&lt;/p>
&lt;p>Now let&amp;rsquo;s consider a sample $x = \{ x_1, x_2, \ldots x_n \}$.
Let&amp;rsquo;s denote the median absolute deviation for a sample of size $n$ as $\textrm{MAD}_n$.
The corresponding equation looks similar to the definition of $\textrm{MAD}$ for a distribution:&lt;/p>
$$
\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
$$
&lt;p>Let&amp;rsquo;s assume that $\textrm{median}$ is the straightforward definition of the median
(if $n$ is odd, the median is the middle element of the sorted sample,
if $n$ is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use $C_n = C_{\infty}$ for extremely large sample sizes.
However, for small $n$, $\textrm{MAD}_n$ becomes a &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased estimator&lt;/a>.
If we want to get an unbiased version, we should adjust the value of $C_n$.&lt;/p>
&lt;p>In this post, we look at the possible approaches and learn the way to get the exact value of $C_n$
that makes $\textrm{MAD}_n$ unbiased estimator of the median absolute deviation for any $n$.&lt;/p></description></item><item><title>Comparing distribution quantiles using gamma effect size</title><link>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</guid><description>&lt;p>There are several ways to describe the difference between two distributions.
Here are a few examples:&lt;/p>
&lt;ul>
&lt;li>Effect sizes based on differences between means (e.g., Cohen&amp;rsquo;s d, Glass&amp;rsquo; Δ, Hedges&amp;rsquo; g)&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/">The shift and ration functions&lt;/a> that
estimate differences between matched quantiles.&lt;/li>
&lt;/ul>
&lt;p>In one of the previous post, I &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">described&lt;/a>
the gamma effect size which is defined not for the mean but for quantiles.
In this post, I want to share a few case studies that demonstrate
how the suggested metric combines the advantages of the above approaches.&lt;/p></description></item><item><title>A single outlier could completely distort your Cohen's d value</title><link>https://aakinshin.net/posts/cohend-and-outliers/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cohend-and-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&amp;rsquo;s d&lt;/a> is a popular way to estimate
the &lt;a href="https://en.wikipedia.org/wiki/Effect_size">effect size&lt;/a> between two samples.
It works excellent for perfectly normal distributions.
Usually, people think that slight deviations from normality
shouldn&amp;rsquo;t produce a noticeable impact on the result.
Unfortunately, it&amp;rsquo;s not always true.
In fact, a single outlier value can completely distort the result even in large samples.&lt;/p>
&lt;p>In this post, I will present some illustrations for this problem and will show how to fix it.&lt;/p></description></item><item><title>Better moving quantile estimations using the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">have discussed&lt;/a> the Hardle-Steiger method.
This algorithm allows estimating &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>
using $O(L)$ memory and $O(log(L))$ element processing complexity (where $L$ is the window size).
Also, I have shown how to adapt this approach to estimate &lt;em>any&lt;/em> moving quantile.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the &lt;a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics&lt;/a>
which is the $k\textrm{th}$ smallest element from the sample.
It means that the estimated quantile value always equals one of the last $L$ observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for $x = \{4, 5, 6, 7\}$,
some estimators return $5.5$ (which is the arithmetical mean of $5$ and $6$)
instead of $5$ or $6$ (which are order statistics).&lt;/p>
&lt;p>Let&amp;rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.&lt;/p></description></item><item><title>MP² quantile estimator: estimating the moving median without storing values</title><link>https://aakinshin.net/posts/mp2-quantile-estimator/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mp2-quantile-estimator/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">described&lt;/a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.&lt;/p>
&lt;p>After the publication, I got a lot of questions about &lt;em>moving&lt;/em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">wrote&lt;/a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last $L$ numbers
($L$ is known as the window size).
However, it requires $O(L)$ memory, and it takes $O(log(L))$ time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&amp;rsquo;t allow implementing low-overhead telemetry in the case of large $L$.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present a moving modification of the P² quantile estimator.
Let&amp;rsquo;s call it MP² (moving P²).
It requires $O(1)$ memory, it takes $O(1)$ to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.&lt;/p>
&lt;p>Let&amp;rsquo;s discuss MP² in detail!&lt;/p></description></item><item><title>Case study: Accuracy of the MAD estimation using the Harrell-Davis quantile estimator (Gumbel distribution)</title><link>https://aakinshin.net/posts/cs-mad-hd-gumbel/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cs-mad-hd-gumbel/</guid><description>&lt;p>In some of my previous posts, I used
the &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (MAD)
to describe the distribution dispersion:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/">DoubleMAD outlier detector based on the Harrell-Davis quantile estimator&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">Nonparametric Cohen&amp;rsquo;s d-consistent effect size&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/qad/">Quantile absolute deviation: estimating statistical dispersion around quantiles&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The MAD estimation depends on the chosen median estimator:
we may get different MAD values with different median estimators.
To get better accuracy,
I always encourage readers to use the Harrell-Davis quantile estimator
instead of the classic Type 7 quantile estimator.&lt;/p>
&lt;p>In this case study, I decided to compare these two quantile estimators using
the &lt;a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution&lt;/a>
(it&amp;rsquo;s a good model for slightly right-skewed distributions).
According to the performed Monte Carlo simulation,
the Harrell-Davis quantile estimator always has better accuracy:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-light.png" target="_blank" alt="summary">
 &lt;img
 src="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-dark.png" target="_blank" alt="summary">
 &lt;img
 src="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Fast implementation of the moving quantile based on the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</guid><description>&lt;p>Imagine you have a time series.
Let&amp;rsquo;s say, after each new observation, you want to know an &amp;ldquo;average&amp;rdquo; value across the last $L$ observations.
Such a metric is known as &lt;a href="https://en.wikipedia.org/wiki/Moving_average">a moving average&lt;/a>
(or rolling/running average).&lt;/p>
&lt;p>The most popular moving average example is &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average">the moving mean&lt;/a>.
It&amp;rsquo;s easy to efficiently implement this metric.
However, it has a major drawback: it&amp;rsquo;s not robust.
Outliers can easily spoil the moving mean and transform it into a meaningless and untrustable metric.&lt;/p>
&lt;p>Fortunately, we have a good alternative: &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>.
Typically, it generates a stable and smooth series of values.
In the below figure, you can see the difference between the moving mean and the moving median on noisy data.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>The moving median also has a drawback: it&amp;rsquo;s not easy to efficiently implement it.
Today we going to discuss the Hardle-Steiger method to estimate the median
(memory: $O(L)$, element processing complexity: $O(log(L))$, median estimating complexity: $O(1)$).
Also, we will learn how to calculate &lt;em>the moving quantiles&lt;/em> based on this method.&lt;/p>
&lt;p>In this post, you will find the following:&lt;/p>
&lt;ul>
&lt;li>An overview of the Hardle-Steiger method&lt;/li>
&lt;li>A simple way to implement the Hardle-Steiger method&lt;/li>
&lt;li>Moving quantiles inspired by the Hardle-Steiger method&lt;/li>
&lt;li>How to process initial elements&lt;/li>
&lt;li>Reference C# implementation&lt;/li>
&lt;/ul></description></item><item><title>Coverage of quantile confidence intervals</title><link>https://aakinshin.net/posts/quantile-ci-coverage/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-ci-coverage/</guid><description>&lt;p>There is a common &lt;a href="https://en.wikipedia.org/wiki/Confidence_interval#Misunderstandings">misunderstanding&lt;/a>
that a 95% confidence interval is an interval that covers the true parameter value with 95% probability.
Meanwhile, the correct definition assumes that
the true parameter value will be covered by 95% of 95% confidence intervals &lt;em>in the long run&lt;/em>.
These two statements sound similar, but there is a huge difference between them.
95% in this context is not a property of a single confidence interval.
Once you get a calculated interval, it may cover the true value (100% probability) or
it may don&amp;rsquo;t cover it (0% probability).
In fact, 95% is a &lt;em>prediction&lt;/em> about the percentage of &lt;em>future&lt;/em> confidence intervals
that cover the true value &lt;em>in the long run&lt;/em>.&lt;/p>
&lt;p>However, even if you know the correct definition, you still may experience some troubles.
The first thing people usually forgot is the &amp;ldquo;long run&amp;rdquo; part.
For example, if we collected 100 samples and calculated a 95% confidence interval of a parameter for each of them,
we shouldn&amp;rsquo;t expect that 95 of these intervals cover the true parameter value.
In fact, we can observe a situation when none of these intervals covers the true value.
Of course, this is an unlikely event, but if you automatically perform thousands of different experiments,
you will definitely get some extreme situations.&lt;/p>
&lt;p>The second thing that may create trouble is the &amp;ldquo;prediction&amp;rdquo; part.
If weather forecasters predicted that it will rain tomorrow, this does not mean that it will rain tomorrow.
The same works for statistical predictions.
The actual prediction reliability may depend on many factors.
If you estimate confidence intervals around the mean for the normal distribution, you are most likely safe.
However, if you estimate confidence intervals around quantiles for non-parametric distributions,
you should care about the following things:&lt;/p>
&lt;ul>
&lt;li>The used approach to estimate confidence intervals&lt;/li>
&lt;li>The underlying distribution&lt;/li>
&lt;li>The sample size&lt;/li>
&lt;li>The position of the target quantile&lt;/li>
&lt;/ul>
&lt;p>I &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">have already showed&lt;/a> how to estimate
the confidence interval around the given quantile using the Maritz-Jarrett method.
It&amp;rsquo;s time to verify the reliability of this approach.
In this post, I&amp;rsquo;m going to show some Monte-Carlo simulations that evaluate the coverage percentage in different situations.&lt;/p></description></item><item><title>Quantile confidence intervals for weighted samples</title><link>https://aakinshin.net/posts/weighted-quantiles-ci/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles-ci/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>When you work with non-parametric distributions,
quantile estimations are essential to get the main distribution properties.
Once you get the estimation values, you may be interested in measuring the accuracy of these estimations.
Without it, it&amp;rsquo;s hard to understand how trustable the obtained values are.
One of the most popular ways to evaluate accuracy is confidence interval estimation.&lt;/p>
&lt;p>Now imagine that you collect some measurements every day.
Each day you get a small sample of values that is not enough to get the accurate daily quantile estimations.
However, the full time-series over the last several weeks has a decent size.
You suspect that past measurements should be similar to today measurements,
but you are not 100% sure about it.
You feel a temptation to extend the up-to-date sample by the previously collected values,
but it may spoil the estimation (e.g., in the case of recent change points or positive/negative trends).&lt;/p>
&lt;p>One of the possible approaches in this situation is to use &lt;em>weighted samples&lt;/em>.
This assumes that we add past measurements to the &amp;ldquo;today sample,&amp;rdquo;
but these values should have smaller weight.
The older measurement we take, the smaller weight it gets.
If you have consistent values across the last several days,
this approach works like a charm.
If you have any recent changes, you can detect such situations by huge confidence intervals
due to the sample inconsistency.&lt;/p>
&lt;p>So, how do we estimate confidence intervals around quantiles for the weighted samples?
In one of the previous posts, I have already shown how to &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">estimate quantiles on weighted samples&lt;/a>.
In this post, I will show how to estimate quantile confidence intervals for weighted samples.&lt;/p></description></item></channel></rss>