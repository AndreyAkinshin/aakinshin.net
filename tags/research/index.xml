<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research on Andrey Akinshin</title><link>https://aakinshin.net/tags/research/</link><description>Recent content in Research on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 27 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/research/index.xml" rel="self" type="application/rss+xml"/><item><title>The Effect Existence, Its Magnitude, and the Goals</title><link>https://aakinshin.net/posts/effect-magnitude-goals/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/effect-magnitude-goals/</guid><description>&lt;p>If you are curious if something impacts something else, the answer is probably &amp;ldquo;yes.&amp;rdquo;
Does that indicator depend on those factors?
Yes, it does.
If we change this thing, would it affect &amp;hellip;?
Yes, it would.
If a person takes this pill, could it cause a non-exactly-zero change in the body?
Yes, the presence of the pill is already a change that can always be detected with the right amount of effort.&lt;/p>
&lt;p>One may argue that in some cases (assuming the list of specific cases is presented), zero effect does exist.
For a moment, let us pretend that it is true.
Now, let us imagine a parallel universe, which is the same as ours but with the presence of the effect.
Unfortunately, the effect is so small that our tools are not sophisticated enough to detect it.
Imagine being put into one of these worlds, but you don&amp;rsquo;t know which one.
How do you determine the existence of the effect?
Of course, you can improve the resolution of the measurement tools via new scientific discoveries,
but with the current state of technology, the absence of the effect cannot be checked.
Therefore, it is always safer to assume that the effect exists, keeping in mind that it can be negligible.
Let us accept this assumption and continue if it is absolute truth.&lt;/p></description></item><item><title>Case study: a city social survey</title><link>https://aakinshin.net/posts/cs-social-survey/</link><pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cs-social-survey/</guid><description>&lt;p>Imagine a city mayor considering a project offering to build parks in several neighborhoods.
It can be a good budget investment since it can potentially increase the happiness level of the citizens.
However, it is just a hypothesis: if parks do not impact happiness,
it is worth considering other city renovation projects.
It makes sense to perform a pilot experiment before spending the budget on all the parks.
The mayor is thinking about the following plan:
pick a random neighborhood,
survey the citizens to measure their happiness,
build a park,
survey the citizens again,
compare the survey results,
make a decision about the further parks in other neighborhoods.
Someone is needed to design the survey and draw the conclusion.&lt;/p>
&lt;p>Let us explore possible approaches to perform such a study.
These artificial examples are not guidelines
but rather simplified illustrations of possible mindsets presented as lists of thoughts.
In this demonstration, we mainly focus on the attitude to the research process rather than on the technical details.
All the examples are based on real stories.&lt;/p></description></item><item><title>Simplifying adjustments of confidence levels and practical significance thresholds</title><link>https://aakinshin.net/posts/adjust-confidence-levels-and-practical-significance/</link><pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/adjust-confidence-levels-and-practical-significance/</guid><description>&lt;p>Translation of the buisness goals to the actual parameters of the statistical procedure is a non-trivial task.
The degree of non-triviality increases if we should adjust several parameters at the same time.
In this post, we consider a problem of simultaneous choice
of the confidence level and the practical significance threshold.
We discuss possible pitfalls and how to simplify the adjusting procedure to avoid them.&lt;/p></description></item><item><title>Degrees of practical significance</title><link>https://aakinshin.net/posts/practical-significance-degrees/</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/practical-significance-degrees/</guid><description>&lt;p>Let&amp;rsquo;s say we have two data samples, and we want to check if there is a difference between them.
If we are talking about any kind of difference, the answer is most probably yes.
It&amp;rsquo;s highly unlikely that two random samples are identical.
Even if they are, there are still chances that we observe such a situation by accident,
and there is a difference in the underlying distributions.
Therefore, the discussion about the existence of any kind of difference is not meaningful.&lt;/p>
&lt;p>To make more meaningful insights, researchers often talk about statistical significance.
The approach can also be misleading.
If the sample size is large enough, we are almost always able to detect even a neglectable difference
and obtain a statistically significant result for any pair of distributions.
On the other hand, a huge difference can be declared insignificant if the sample size is small.
While the concept is interesting and well-researched, it rarely matches the actual research goal.
I strongly believe that we should &lt;em>not&lt;/em> test for the nil hypothesis (checking if the true difference is &lt;em>exactly&lt;/em> zero).&lt;/p>
&lt;p>Here, we can switch from statistical significance to practical significance.
We are supposed to define a threshold (e.g., in terms of minimum effect size)
for the difference that is meaningful for the research.
This approach has more chances to be aligned with the research goals.
However, it is also not always satisfying enough.
We should keep in mind that hypothesis testing often arises in the context of decision-making problems.
In some cases, we can do exploration research in which we just want to have a better understanding of the world.
However, in most cases, we do not perform calculations just because we are curious;
we often want to make a decision based on the results.
And this is the most crucial moment.
It should always be the starting point in any research project.
First of all, we should clearly describe the possible decisions and their preconditions.
When we start doing that, we can discover that not all the practically significant outcomes are equally significant.
If different practically significant results may lead to different decisions,
we should define the proper classification in advance during the research design stage.
The dichotomy of &amp;ldquo;practically significant&amp;rdquo; vs. &amp;ldquo;not practically significant&amp;rdquo;
may conceal important problem aspects and lead to a wrong decision.&lt;/p>
&lt;p>In this post, I would like to discuss the degrees of practical significance and
show an example of how important it is for some problems.&lt;/p></description></item><item><title>Weighted Mann-Whitney U test, Part 3</title><link>https://aakinshin.net/posts/wmw3/</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wmw3/</guid><description>&lt;p>I continue building a weighted version of the Mann–Whitney $U$ test.
While &lt;a href="https://aakinshin.net/posts/wmw1/">previously suggested approach&lt;/a> feel promising,
I don&amp;rsquo;t like the usage of Bootstrap to obtain the $p$-value.
It is always better to have a deterministic and exact approach where it&amp;rsquo;s possible.
I still don&amp;rsquo;t know how to solve it in general case,
but it seems that I&amp;rsquo;ve obtained a reasonable solution for some specific cases.
The current version of the approach still has issues and
requires additional correction factors in some cases and additional improvements.
However, it passes my minimal requirements, so it is worth trying to continue developing this idea.
In this post, I share the description of the weighted approach and provide numerical examples.&lt;/p></description></item><item><title>Andreas Löffler's implementation of the exact p-values calculations for the Mann-Whitney U test</title><link>https://aakinshin.net/posts/mw-loeffler/</link><pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mw-loeffler/</guid><description>&lt;p>Mann-Whitney is one of the most popular non-parametric statistical tests.
Unfortunately, most test implementations in statistical packages are far from perfect.
The exact p-value calculation is time-consuming and can be impractical for large samples.
Therefore, most implementations automatically switch to the asymptotic approximation, which can be quite inaccurate.
Indeed, the classic normal approximation could produce
&lt;a href="https://aakinshin.net/posts/r-mann-whitney-incorrect-p-value/">enormous&lt;/a>
&lt;a href="https://aakinshin.net/posts/python-mann-whitney-incorrect-p-value/">errors&lt;/a>.
Thanks to the &lt;a href="https://aakinshin.net/posts/mw-edgeworth2/">Edgeworth expansion&lt;/a>, the accuracy can be improved,
but it is still not always satisfactory enough.
I prefer using the exact p-value calculation whenever possible.&lt;/p>
&lt;p>The computational complexity of the exact p-value calculation using the classic recurrent equation
suggested by Mann and Whitney is $\mathcal{O}(n^2 m^2)$ in terms of time and memory.
It&amp;rsquo;s not a problem for small samples, but for medium-size samples,
it is slow, and it has an extremely huge memory footprint.
This gives us an unpleasant dilemma:
either we use the exact p-value calculation (which is extremely time and memory-consuming),
or we use the asymptotic approximation (which gives poor accuracy).&lt;/p>
&lt;p>Last week, I got acquainted with a brilliant algorithm for the exact p-value calculation
suggested by Andreas Löffler in 1982.
It&amp;rsquo;s much faster than the classic approach, and it requires only $\mathcal{O}(n+m)$ memory.&lt;/p></description></item><item><title>Eclectic statistics</title><link>https://aakinshin.net/posts/eclectic-statistics/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/eclectic-statistics/</guid><description>&lt;p>In the world of mathematical statistics, there is a constant confrontation between adepts of different paradigms.
This is a constant source of confusion for many researchers who struggle to pick out the proper approach to follow.
For example, how to choose between the frequentist and Bayesian approaches?
Since these paradigms may produce inconsistent results
(e.g., see &lt;a href="https://en.wikipedia.org/wiki/Lindley%27s_paradox">Lindley&amp;rsquo;s paradox&lt;/a>),
some choice has to be made.
The easiest way to conduct research is to pick a single paradigm and stick to it.
The right way to conduct research is to carefully think.&lt;/p></description></item><item><title>Change Point Detection and Recent Changes</title><link>https://aakinshin.net/posts/cpd-recent-changes/</link><pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cpd-recent-changes/</guid><description>&lt;p>Change point detection (CPD) in time series analysis
is an essential tool for identifying significant shifts in data patterns.
These shifts, or &amp;ldquo;change points,&amp;rdquo; can signal critical transitions in various contexts.
While most CPD algorithms are adept at discovering historical change points,
their sensitivity in detecting recent changes can be limited,
often due to a key parameter: the minimum distance between sequential change points.
In this post, I share some speculations on how we can improve cpd analysis by combining two change point detectors.&lt;/p></description></item><item><title>Merging extended P² quantile estimators, Part 1</title><link>https://aakinshin.net/posts/merging-exp2-1/</link><pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/merging-exp2-1/</guid><description>&lt;p>&lt;a href="https://aakinshin.net/tags/research-p2qe/">P² quantile estimator&lt;/a> is a streaming quantile estimator
with $\mathcal{O}(1)$ memory footprint and an extremely fast update procedure.
Several days ago, I learned that it was &lt;a href="https://twitter.com/rickbrewPDN/status/1740233421673349544">adopted&lt;/a> for
the new Paint.NET GPU-based Median Sketch effect
(the description is &lt;a href="https://forums.getpaint.net/topic/124261-median-sketch-gpu/">here&lt;/a>).
While P² meets the basic problem requirement (streaming median approximation without storing all the values),
the algorithm performance is still not acceptable without additional adjustments.
A significant performance improvement &lt;a href="https://twitter.com/rickbrewPDN/status/1740422610545234153">can be obtained&lt;/a>
if we split the input stream, process each part separately with a separate P², and merge the results.
Unfortunately, the merging procedure is a tricky thing to implement.
I enjoy such challenges, so I decided to attempt to build such a merging approach.
In this post, I describe my first attempt.&lt;/p></description></item><item><title>Hodges-Lehmann ratio estimator vs. Bhattacharyya's scale ratio estimator</title><link>https://aakinshin.net/posts/hl-ratio-vs-bhattacharyya/</link><pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-ratio-vs-bhattacharyya/</guid><description>&lt;p>Previously, I &lt;a href="https://aakinshin.net/posts/hl-ratio/">discussed&lt;/a> an idea of a ratio estimator based on the Hodges-Lehmann estimator.
This idea looks so simple and natural that I was sure that it must have already been proposed and studied.
However, when I started to search for it, it turned out that it was not as easy as I expected.
Moreover, some papers attribute this idea to Bhattacharyya, which is not accurate.
In this post, we discuss the difference between these two approaches.&lt;/p></description></item><item><title>Finite-sample Gaussian efficiency: Shamos vs. Rousseeuw-Croux Qn scale estimators</title><link>https://aakinshin.net/posts/shamos-vs-qn/</link><pubDate>Tue, 19 Dec 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/shamos-vs-qn/</guid><description>&lt;p>&lt;a href="https://aakinshin.net/posts/qad-vs-rc/">Previously&lt;/a>, we compared the finite-sample Gaussian efficiency of
the &lt;a href="https://aakinshin.net/library/papers/rousseeuw1993/">Rousseeuw-Croux scale estimators&lt;/a> and the QAD estimator.
In this post, we compare the finite-sample Gaussian efficiency of the Shamos scale estimator
and the Rousseeuw-Croux $Q_n$ scale estimator.
This is a particularly interesting comparison.
In the famous &amp;ldquo;Alternatives to the Median Absolute Deviation&amp;rdquo; (1993) paper by Peter J. Rousseeuw and Christophe Croux,
the authors presented $Q_n$ as an improved version of the Shamos estimator.
Both estimators are based on the set of pairwise absolute differences between the elements of the sample.
The Shamos estimator takes the median of this set and, therefore,
has the asymptotic breakdown point of $\approx 29\%$
and the asymptotic Gaussian efficiency of $\approx 86\%$.
$Q_n$ takes the first quartile of this set and, therefore,
has the asymptotic breakdown point of $\approx 50\%$ (like the median)
and the asymptotic Gaussian efficiency of $\approx 82\%$.
It sounds like a good deal: we trade $4\%$ of the asymptotic Gaussian efficiency
for $21\%$ of the asymptotic breakdown point.
What could possibly stop us from using $Q_n$ everywhere instead of the Shamos estimator?&lt;/p>
&lt;p>Well, here is a trick.
The breakdown point of $29\%$ is actually a practically reasonable value.
If more than $29\%$ of the sample are outliers, we should probably consider them not as outliers but as a separate mode.
Such a situation should be handled by a multimodality detector and lead us to a different approach.
The usage of dispersion estimators in the case of multimodal distributions is potentially misleading.
When such a multimodality diagnostic scheme is used, there is no practical need for a higher breakdown point.&lt;/p>
&lt;p>Thus, the breakdown point of $50\%$ is not so impressive property of $Q_n$.
Meanwhile, the drop in Gaussian efficiency is not so enjoyable.
$4\%$ may sound like a negligible difference, but it is only the asymptotic value.
In real life, we typically tend to work with finite samples.
Let us explore the actual finite-sample Gaussian efficiency values of these estimators.&lt;/p></description></item><item><title>Two-pass change point detection for temporary interval condensation</title><link>https://aakinshin.net/posts/two-pass-cpd/</link><pubDate>Tue, 12 Dec 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/two-pass-cpd/</guid><description>&lt;p>When we choose a change point detection algorithm, the most important thing is to clearly understand
why we want to detect the change points.
The knowledge of the final business goals is essential.
In this post, I show a simple example of how a business requirement can be translated into algorithm adjustments.&lt;/p></description></item><item><title>Inconsistent violin plots</title><link>https://aakinshin.net/posts/inconsistent-violin-plots/</link><pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/inconsistent-violin-plots/</guid><description>&lt;p>The usefulness and meaningfulness of the &lt;a href="https://en.wikipedia.org/wiki/Violin_plot">violin plots&lt;/a> are dubious
(e.g., see &lt;a href="https://youtu.be/_0QMKFzW9fw">this video&lt;/a> and the
&lt;a href="https://www.reddit.com/16z7stw">corresponding discussion&lt;/a>).
While this type of plot inherits issues of density plots (e.g., &lt;a href="https://aakinshin.net/posts/kde-bw/">the bandwidth selection problem&lt;/a>)
and box plots, it also introduces new problems.
One such problem is data inconsistency: default density plots and box plots are often incompatible with each other.
In this post, I show an example of this inconsistency.&lt;/p></description></item><item><title>Sporadic noise problem in change point detection</title><link>https://aakinshin.net/posts/sporadic-noise-problem/</link><pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sporadic-noise-problem/</guid><description>&lt;p>We consider a problem of change point detection at the end of a time series.
Let us say that we systematically monitor readings of an indicator,
and we want to react to noticeable changes in the measured values as fast as possible.
When there are no changes in the underlying distribution,
any alerts about detected change points should be considered false positives.
Typically, in such problems,
we consider the &lt;a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d.&lt;/a>
assumption that claims that in the absence of change points,
all the measurements are independent and identically distributed.
Such an assumption significantly simplifies the mathematical model,
but unfortunately, it is rarely fully satisfied in real life.
If we want to build a reliable change point detection system,
it is important to be aware of possible real-life artifacts that introduce deviations from the declared model.
In this problem, I discuss the problem of the sporadic noise.&lt;/p></description></item><item><title>Resistance to the low-density regions: the Hodges-Lehmann location estimator based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/rldr-hlhd/</link><pubDate>Tue, 21 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-hlhd/</guid><description>&lt;p>Previously, I have discussed the topic of the &lt;a href="https://aakinshin.net/tags/research-rldr/">resistance to the low-density regions&lt;/a>
of various estimators including &lt;a href="https://aakinshin.net/posts/rldr-hl/">the Hodges-Lehmann location estimator&lt;/a> ($\operatorname{HL}$).
In general, $\operatorname{HL}$ is a great estimator with great statistical efficiency and a decent breakdown point.
Unfortunately, it has low resistance to the low-density regions around
$29^\textrm{th}$ and $71^\textrm{th}$ percentiles, which may cause troubles in the case of multimodal distributions.
I am trying to find a modification of $\operatorname{HL}$
that performs almost the same as the original $\operatorname{HL}$, but has increased resistance.
One of the ideas I had was using the Harrell-Davis quantile estimator
instead of the sample median to evaluate $\operatorname{HL}$.
Regrettably, this idea did not turn out to be successful:
such an estimator has a resistance function similar to the original $\operatorname{HL}$.
I believe that it is important to share negative results, and therefore this post contains a bunch of plots,
which illustrate results of relevant numerical simulations.&lt;/p></description></item><item><title>Median vs. Hodges-Lehmann: compare efficiency under heavy-tailedness</title><link>https://aakinshin.net/posts/robust-eff-median-hl/</link><pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/robust-eff-median-hl/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/thoughts-robust-efficiency/">previous post&lt;/a>,
I shared some thoughts on how to evaluate the statistical efficiency of estimators under heavy-tailed distributions.
In this post, I apply described ideas to actually compare efficiency values of
the Mean, the Sample Median, and the Hodges-Lehmann location estimator
under various distributions.&lt;/p></description></item><item><title>Thoughts about robustness and efficiency</title><link>https://aakinshin.net/posts/thoughts-robust-efficiency/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thoughts-robust-efficiency/</guid><description>Statistical efficiency is an essential characteristic, which has to be taken into account when we choose between different estimators. When the underlying distribution is a normal one or at least light-tailed, evaluation of the statistical efficiency typically is not so hard. However, when the underlying distribution is a heavy-tailed one, problems appear. The statistical efficiency is usually expressed via the mean squared error or via variance, which are not robust. Therefore, heavy-tailedness may lead to distorted or even infinite efficiency, which is quite impractical.
So, how do we compare the efficiency of estimators under a heavy-tailed distribution?
Let&amp;rsquo;s say we want to compare the efficiency of the mean and the median distribution. Under the normal distribution (so-called Gaussian efficiency), this task is trivial: we build the sampling mean distribution and the sampling median distribution, estimate the variance for each of them, and then get the ratio of these variances. However, if we are interested in the median, we are probably expecting some outliers. Most of the significant real-life outliers come from the heavy-tailed distributions. Therefore, Gaussian efficiency is not the most interesting metric. It makes sense to evaluate the efficiency of the considered estimators under various heavy-tailed distributions. Unfortunately, the variance is not a robust measure and is too sensitive to tails: if the sampling distribution is also not normal or even heavy-tailed, the meaningfulness of the true variance value decreases. It seems reasonable to consider alternative robust measures of dispersion. Which one should we choose? Maybe Median Absolute Deviation (MAD)? Well, the asymptotic Gaussian efficiency of MAD is only ~37%. And here we have the same problem: should we trust the Gaussian efficiency under heavy-tailedness? Therefore, we should first evaluate the efficiency of dispersion estimators. But we can&amp;rsquo;t do it without a previously chosen dispersion estimator! And could we truly express the actual relative efficiency between two estimators under tricky asymmetric multimodal heavy-tailed distributions using a single number?</description></item><item><title>Finite-sample Gaussian efficiency: Quantile absolute deviation vs. Rousseeuw-Croux scale estimators</title><link>https://aakinshin.net/posts/qad-vs-rc/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-vs-rc/</guid><description>&lt;p>In this post, we discuss the finite-sample Gaussian efficiency of various robust dispersion estimators.
The classic standard deviation has the highest possible Gaussian efficiency of $100\%$,
but it is not robust: a single outlier can completely destroy the estimation.
A typical robust alternative to the standard deviation is the Median Absolute Deviation ($\operatorname{MAD}$).
While the $\operatorname{MAD}$ is highly robust (the breakdown point is $50\%$), it is not efficient:
its asymptotic Gaussian efficiency is only $37\%$.
Common alternative to the $\operatorname{MAD}$ is the Rousseeuw-Croux $S_n$ and $Q_n$ scale estimators
that provide higher efficiency, keeping the breakdown point of $50\%$.
In &lt;a href="https://aakinshin.net/posts/preprint-qad/">one of my recent preprints&lt;/a>,
I introduced the concept of the Quantile Absolute Deviation ($\operatorname{QAD}$)
and its specific cases:
the Standard Quantile Absolute Deviation ($\operatorname{SQAD}$) and
the Optimal Quantile Absolute Deviation ($\operatorname{OQAD}$).
Let us review the finite-sample and asymptotic values of the Gaussian efficiency for these estimators.&lt;/p></description></item><item><title>Mann-Whitney U test and heteroscedasticity</title><link>https://aakinshin.net/posts/mw-heteroscedasticity/</link><pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mw-heteroscedasticity/</guid><description>&lt;p>Mann-Whitney U test is a good nonparametric test, which mostly targets changes in locations.
However, it doesn&amp;rsquo;t properly support all types of differences between the two distributions.
Specifically, it poorly handles changes in variance.
In this post, I briefly discuss its behavior in reaction to scaling a distribution without introducing location changes.&lt;/p></description></item><item><title>Exploring the power curve of the Ansari-Bradley test</title><link>https://aakinshin.net/posts/ansari-power/</link><pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/ansari-power/</guid><description>&lt;p>The Ansari-Bradley test is a popular rank-based nonparametric test for a difference in scale/dispersion parameters.
In this post, we explore its power curve in a numerical simulation.&lt;/p></description></item><item><title>Exploring the power curve of the Lepage test</title><link>https://aakinshin.net/posts/lepage-power/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/lepage-power/</guid><description>&lt;p>Previously, I already &lt;a href="https://aakinshin.net/posts/cucconi-power/">discussed the Cucconi test&lt;/a>.
In this post, I continue the topic of nonparametric tests and
check out the &lt;a href="https://en.wikipedia.org/wiki/Lepage_test">Lepage test&lt;/a>.&lt;/p></description></item><item><title>Weighted Hodges-Lehmann location estimator and mixture distributions</title><link>https://aakinshin.net/posts/whl-mixture/</link><pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/whl-mixture/</guid><description>&lt;p>The classic non-weighted Hodges-Lehmann location estimator of a sample $\mathbf{x} = (x_1, x_2, \ldots, x_n)$
is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right),
$$
&lt;p>where $\operatorname{median}$ is the sample median.
&lt;a href="https://aakinshin.net/posts/whl/">Previously&lt;/a>, we have defined a weighted version of the Hodges-Lehmann location estimator as follows:&lt;/p>
$$
\operatorname{WHL}(\mathbf{x}, \mathbf{w}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{wmedian}} \left(\frac{x_i + x_j}{2},\; w_i \cdot w_j \right),
$$
&lt;p>where $\mathbf{w} = (w_1, w_2, \ldots, w_n)$ is the vector of weights,
$\operatorname{wmedian}$ is the &lt;a href="https://aakinshin.net/posts/preprint-wqe/">weighted median&lt;/a>.
For simplicity, in the scope of the current post,
Hyndman-Fan Type 7 quantile estimator is used as the base for the weighted median.&lt;/p>
&lt;p>In this post, we consider a numerical simulation in which we compare sampling distribution of
$\operatorname{HL}$ and $\operatorname{WHL}$ in a case of mixture distribution.&lt;/p></description></item><item><title>Carling’s Modification of the Tukey's fences</title><link>https://aakinshin.net/posts/carling-outlier-detector/</link><pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/carling-outlier-detector/</guid><description>&lt;p>Let us consider the classic problem of outlier detection in one-dimensional sample.
One of the most popular approaches is Tukey&amp;rsquo;s fences, that defines the following range:&lt;/p>
$$
[Q_1 - k(Q_3 - Q_1);\; Q_3 + k(Q_3 - Q_1)],
$$
&lt;p>where $Q_1$ and $Q_3$ are the first and the third quartiles of the given sample.&lt;/p>
&lt;p>All the values outside the given range are classified as outliers.
The typical values of $k$ are $1.5$ for &amp;ldquo;usual outliers&amp;rdquo; and $3.0$ for &amp;ldquo;far out&amp;rdquo; outliers.
In the classic Tukey&amp;rsquo;s fences approach, $k$ is often a predefined constant.
However, there are alternative approaches that define $k$ dynamically based on the given sample.
One of the possible variations of Tukey&amp;rsquo;s fences is Carling&amp;rsquo;s modification that defines $k$ as follows:&lt;/p>
$$
k = \frac{17.63n - 23.64}{7.74n - 3.71},
$$
&lt;p>where $n$ is the sample size.&lt;/p>
&lt;p>In this post, we compare the classic Tukey&amp;rsquo;s fences with $k=1.5$ and $k=3.0$ against Carling&amp;rsquo;s modification.&lt;/p></description></item><item><title>Central limit theorem and log-normal distribution</title><link>https://aakinshin.net/posts/clt-lnorm/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/clt-lnorm/</guid><description>&lt;p>It is inconvenient to work with samples from a distribution of unknown form.
Therefore, researchers often switch to considering the sample mean value and
hope that thanks to the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem&lt;/a>,
the distribution of the sample means should be approximately normal.
They say that if we consider samples of size $n \geq 30$, we can expect practically acceptable convergence to normality
thanks to &lt;a href="https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem">Berry–Esseen theorem&lt;/a>.
Indeed, this statement is almost valid for many real data sets.
However, we can actually expect the applicability of this approach only for light-tailed distributions.
In the case of heavy-tailed distributions, converging to normality is so slow,
that we cannot imply the normality assumption for the distribution of the sample means.
In this post, I provide an illustration of this effect using the log-normal distribution.&lt;/p></description></item><item><title>Hodges-Lehmann Gaussian efficiency: location shift vs. shift of locations</title><link>https://aakinshin.net/posts/hl-loc-shift-vs-shift-loc/</link><pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-loc-shift-vs-shift-loc/</guid><description>&lt;p>Let us consider two samples $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_m)$.
The one-sample Hodges-Lehman location estimator is defined as the median of the Walsh (pairwise) averages:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right),
\quad
\operatorname{HL}(\mathbf{y}) =
 \underset{1 \leq i \leq j \leq m}{\operatorname{median}} \left(\frac{y_i + y_j}{2} \right).
$$
&lt;p>For these two samples, we can also define the shift between these two estimations:&lt;/p>
$$
\Delta_{\operatorname{HL}}(\mathbf{x}, \mathbf{y}) = \operatorname{HL}(\mathbf{x}) - \operatorname{HL}(\mathbf{y}).
$$
&lt;p>The two-sample Hodges-Lehmann location shift estimator is defined as the median of pairwise differences:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>Previously, I already compared the location shift estimator with the difference of median estimators
(&lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median1/">1&lt;/a>, &lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median2/">2&lt;/a>).
In this post, I compare the difference between two location estimations and the shift estimations
in terms of Gaussian efficiency.
Before I started this study, I expected that $\operatorname{HL}$ should be more efficient
than $\Delta_{\operatorname{HL}}$.
Let us find out if my intuition is correct or not!&lt;/p></description></item><item><title>Thoughts on automatic statistical methods and broken assumptions</title><link>https://aakinshin.net/posts/thoughts-on-broken-assumptions/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thoughts-on-broken-assumptions/</guid><description>In the old times of applied statistics existence, all statistical experiments used to be performed by hand. In manual investigations, an investigator is responsible not only for interpreting the research results but also for the applicability validation of the used statistical approaches. Nowadays, more and more data processing is performed automatically on enormously huge data sets. Due to the extraordinary number of data samples, it is often almost impossible to verify each output individually using human eyes. Unfortunately, since we typically have no full control over the input data, we cannot guarantee certain assumptions that are required by classic statistical methods. These assumptions can be violated not only due to real-life phenomena we were not aware of during the experiment design stage, but also due to data corruption. In such corner cases, we may get misleading results, wrong automatic decisions, unacceptably high Type I/II error rates, or even a program crash because of a division by zero or another invalid operation. If we want to make an automatic analysis system reliable and trustworthy, the underlying mathematical procedures should correctly process malformed data.
The normality assumption is probably the most popular one. There are well-known methods of robust statistics that focus only on slight deviations from normality and the appearance of extreme outliers. However, it is only a violation of one specific consequence from the normality assumption: light-tailedness. In practice, this sub-assumption is often interpreted as &amp;ldquo;the probability of observing extremely large outliers is negligible.&amp;rdquo; Meanwhile, there are other implicit derived sub-assumptions: continuity (we do not expect tied values in the input samples), symmetry (we do not expect highly-skewed distributions), unimodality (we do not expect multiple modes), nondegeneracy (we do not expect all sample values to be equal), sample size sufficiency (we do not expect extremely small samples like single-element samples), and others.</description></item><item><title>Ratio estimator based on the Hodges-Lehmann approach</title><link>https://aakinshin.net/posts/hl-ratio/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-ratio/</guid><description>&lt;p>For two samples $\mathbf{x} = ( x_1, x_2, \ldots, x_n )$ and $\mathbf{y} = ( y_1, y_2, \ldots, y_m )$,
the Hodges-Lehmann location shift estimator is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>Now, let us consider the problem of estimating the ratio of the location measures instead of the shift between them.
While there are multiple approaches to providing such an estimation,
one of the options that can be considered is based on the Hodges-Lehmann ideas.&lt;/p></description></item><item><title>Weighted Mann-Whitney U test, Part 2</title><link>https://aakinshin.net/posts/wmw2/</link><pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wmw2/</guid><description>&lt;p>Previously, I &lt;a href="https://aakinshin.net/posts/wmw1/">suggested&lt;/a> a weighted version of the Mann–Whitney $U$ test.
The distribution of the weighted normalized $U_\circ^\star$ can be obtained via bootstrap.
However, it is always nice if we can come up with an exact solution for the statistic distribution or
at least provide reasonable approximations.
In this post, we start exploring this distribution.&lt;/p></description></item><item><title>Exploring the power curve of the Cucconi test</title><link>https://aakinshin.net/posts/cucconi-power/</link><pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cucconi-power/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Cucconi_test">Cucconi test&lt;/a>
is a nonparametric two-sample test that compares both location and scale.
It is a classic example of the family of tests that
perform such a comparison simultaneously
instead of combining the results of a location test and a scale test.
Intuitively, such an approach should fit well unimodal distributions.
Moreover, it has the potential to outperform more generic nonparametric tests
that do not rely on the unimodality assumption.&lt;/p>
&lt;p>In this post, we briefly show the equations behind the Cucconi test and
present a power curve that compares it with the Student&amp;rsquo;s t-test and the Mann-Whitney U test under normality.&lt;/p></description></item><item><title>Parametric, Nonparametric, Robust, and Defensive statistics</title><link>https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/</link><pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/</guid><description>&lt;p>Recently, I started writing about &lt;a href="https://aakinshin.net/tags/ds/">defensive statistics&lt;/a>.
The methodology allows having parametric assumptions,
but it adjusts statistical methods so that they continue working even in the case
of huge deviations from the declared assumptions.
This idea sounds quite similar to nonparametric and robust statistics.
In this post, I briefly explain the difference between different statistical methodologies.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/img/compare-light.png" target="_blank" alt="compare">
 &lt;img
 src="https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/img/compare-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/img/compare-dark.png" target="_blank" alt="compare">
 &lt;img
 src="https://aakinshin.net/posts/parametric-nonparametric-robust-defensive/img/compare-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Insidious implicit statistical assumptions</title><link>https://aakinshin.net/posts/insidious-assumptions/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/insidious-assumptions/</guid><description>&lt;p>Recently, I was rereading &lt;a href="https://aakinshin.net/library/papers/hampel1986/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#paper">&lt;/use>&lt;/svg>hampel1986&lt;/a>
and I found this quote about the difference between robust and nonparametric statistics (page 9):&lt;/p>
&lt;blockquote>
&lt;p>Robust statistics considers the effects of only approximate fulfillment of assumptions,
while nonparametric statistics makes rather weak but nevertheless strict assumptions
(such as continuity of distribution or independence).&lt;/p>
&lt;/blockquote>
&lt;p>This statement may sound obvious.
Unfortunately, facts that are presumably obvious in general are not always so obvious at the moment.
When a researcher works with specific types of distributions for a long time,
the properties of these distributions may be transformed into implicit assumptions.
This implicitness can be pretty dangerous.
If an assumption is explicitly declared,
it can become a starting point for a discussion on how to handle violations of this assumption.
The implicit assumptions are hidden and
therefore conceal potential issues in cases when the collected data do not meet our expectations.&lt;/p>
&lt;p>A switch from parametric to nonparametric methods is sometimes perceived as a rejection of all assumptions.
Such a perception can be hazardous.
While the original parametric assumption is actually neglected,
many researchers continue to act like the implicit consequences of this assumption are still valid.&lt;/p>
&lt;p>Since normality is the most popular parametric assumption,
I would like to briefly discuss connected implicit assumptions
that are often perceived not as non-validated hypotheses, but as essential properties of the collected data.&lt;/p></description></item><item><title>Multimodal distributions and effect size</title><link>https://aakinshin.net/posts/multimodal-es/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/multimodal-es/</guid><description>&lt;p>When we want to express the difference between two samples or distributions,
a popular measure family is the effect sizes based on differences between means (difference family).
When the normality assumption is satisfied, this approach works well thanks to classic measures of effect size
like Cohen&amp;rsquo;s d, Glass&amp;rsquo; Δ, or Hedges&amp;rsquo; g.
With slight deviations from normality, &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size2/">robust alternatives&lt;/a> may be considered.
To build such a measure, it&amp;rsquo;s enough to upgrade classic measures by
replacing the sample mean with a robust measure of central tendency and
replacing the standard deviation with a robust measure of dispersion.
However, it might not be enough in the case of large deviations from normality.
In this post, I briefly discuss the problem of effect size evaluation in the context of multimodal distributions.&lt;/p></description></item><item><title>Unobvious limitations of R *signrank Wilcoxon Signed Rank functions</title><link>https://aakinshin.net/posts/signrank-limitations/</link><pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/signrank-limitations/</guid><description>&lt;p>In R, we have functions to calculate the density, distribution function, and quantile function
of the Wilcoxon Signed Rank statistic distribution: &lt;code>dsignrank&lt;/code>, &lt;code>psignrank&lt;/code>, and &lt;code>qsignrank&lt;/code>.
All the functions use exact calculations of the target functions
(the R 4.3.1 implementation can be found &lt;a href="https://svn.r-project.org/R/tags/R-4-3-1/src/nmath/signrank.c">here&lt;/a>).
The exact approach works excellently for small sample sizes.
Unfortunately, for large sample sizes, it fails to provide the expected function values.
Out of the box, there are no alternative approximation solutions that could allow us to get reasonable results.
In this post, we investigate the limitations of these functions and
provide sample size thresholds after which we might get invalid results.&lt;/p></description></item><item><title>Weighted Mann-Whitney U test, Part 1</title><link>https://aakinshin.net/posts/wmw1/</link><pubDate>Tue, 04 Jul 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wmw1/</guid><description>&lt;p>Previously, I have discussed how to build weighted versions of various statistical methods.
I have already covered weighted versions of
&lt;a href="https://aakinshin.net/posts/preprint-wqe/">various quantile estimators&lt;/a> and
&lt;a href="https://aakinshin.net/posts/whl/">the Hodges-Lehmann location estimator&lt;/a>.
Such methods can be useful in various tasks like the support of weighted mixture distributions or exponential smoothing.
In this post, I suggest a way to build a weighted version of the Mann-Whitney U test.&lt;/p></description></item><item><title>Joining modes of multimodal distributions</title><link>https://aakinshin.net/posts/joining-modes/</link><pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/joining-modes/</guid><description>&lt;p>Multimodality of distributions is a severe issue in statistical analysis.
Comparing two multimodal distributions is a tricky challenge.
The degree of this challenge depends on the number of existing modes.
Switching from unimodal models to multimodal ones
can be a controversial decision, potentially causing more problems than solutions.
Hence, if we dare to increase the complexity of the considering models,
we should be sure that this is an essential necessity.
Even when we confidently detect a truly multimodal distribution,
a unimodal model could be an acceptable approximation if it is sufficiently close to the true distribution.
The simplicity of a unimodal model may make it preferable, even if it is less accurate.
Of course, the research goals should always be taken into account when the particular model choice is being made.&lt;/p></description></item><item><title>Understanding the pitfalls of preferring the median over the mean</title><link>https://aakinshin.net/posts/median-vs-mean/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-vs-mean/</guid><description>&lt;p>A common task in mathematical statistics is to aggregate a set of numbers $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$
to a single &amp;ldquo;average&amp;rdquo; value.
Such a value is usually called &lt;em>central tendency&lt;/em>.
There are multiple measures of central tendency.
The most popular one is the &lt;em>arithmetic average&lt;/em> or the &lt;em>mean&lt;/em>:&lt;/p>
$$
\overline{\mathbf{x}} = \left( x_1 + x_2 + \ldots + x_n \right) / n.
$$
&lt;p>The mean is so popular not only thanks to its simplicity but also
because it provides the best way to estimate the center of the perfect normal distribution.
Unfortunately, the mean is not a robust measure.
This means that a single extreme value $x_i$ may distort the mean estimation and
lead to a non-reproducible value that has nothing in common with the &amp;ldquo;expected&amp;rdquo; central tendency.
The actual real-life distributions are never normal.
They can be pretty close to the normal distribution, but only to a certain extent.
Even small deviations from normality may produce occasional extreme outliers,
which makes the mean an unreliable measure in the general case.&lt;/p>
&lt;p>When people discover the danger of the mean, they start looking for a more robust measure of the central tendency.
And the first obvious alternative is the sample median $\tilde{\mathbf{x}}$.
The classic sample median is easy to calculate.
First, you have to sort the sample.
If the sample size $n$ is odd, the median is the middle element in the sorted sample.
If $n$ is even, the median is the arithmetic average of the two middle elements in the sorted sample.
The median is extremely robust: it provides a reasonable estimate
even if almost half of the sample elements are corrupted.&lt;/p>
&lt;p>For symmetric distributions (including the normal one), the true values of the mean and the median are the same.
Once we discover the high robustness of the median, it may be tempting to always use the median instead of the mean.
The median is often perceived as &amp;ldquo;something like the mean but with high resistance to outliers.&amp;rdquo;
Indeed, what is the point of using the unreliable mean, if the median always provides a safer choice?
Should we make the median our default option for the central tendency?&lt;/p>
&lt;p>The answer is no.
You should beware of any default options in mathematical statistics.
All the measures are just tools, and each tool has its limitations and areas of applicability.
A mindless transition from the mean to the median, regardless of the underlying distribution, is not a smart move.
When we are picking a measure of central tendency to use,
the first step should be reviewing the research goals:
why do we need a measure of central tendency, and what are we going to do with the result?
It&amp;rsquo;s impossible to make a rational decision on the statistical methods used without a clear understanding of the goals.
Next, we should match the goals to the properties of available measures.&lt;/p>
&lt;p>There are multiple practical issues with the median,
but the most noticeable problem in practice is about its &lt;em>statistical efficiency&lt;/em>.
Understanding this problem reveals the price of advanced robustness of the median.
In this post, we discuss the concept of statistical efficiency,
estimate the statistical efficiency of the mean and the median under different distributions,
and consider the Hodges-Lehman estimator as a measure of central tendency
that provides a better trade-off between robustness and efficiency.&lt;/p></description></item><item><title>Introducing the defensive statistics</title><link>https://aakinshin.net/posts/defensive-statistics-intro/</link><pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/defensive-statistics-intro/</guid><description>&lt;blockquote>
&lt;p>Normal or approximately normal subjects are less useful objects of research than their pathological counterparts.&lt;/p>
&lt;p>&amp;mdash; Sigmund Freud, &amp;ldquo;The Psychopathology of Everyday Life&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>In the realm of software development, reliability is crucial.
This is especially true when creating systems
that automatically analyze performance measurements to maintain optimal application performance.
To achieve the desired level of reliability, we need a set of statistical approaches
that provide accurate and trustworthy results.
These approaches must work even when faced with varying input data sets and multiple violated assumptions,
including malformed and corrupted values.
In this blog post, I introduce &amp;ldquo;Defensive Statistics&amp;rdquo; as an appropriate methodology for tackling this challenge.&lt;/p></description></item><item><title>Edgeworth expansion for the Mann-Whitney U test, Part 2: increased accuracy</title><link>https://aakinshin.net/posts/mw-edgeworth2/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mw-edgeworth2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/mw-edgeworth/">previous post&lt;/a>,
we showed how the Edgeworth expansion can improve the accuracy of obtained p-values in the Mann-Whitney U test.
However, we considered only the Edgeworth expansion to terms of order $1/m$.
In this post, we explore how to improve the accuracyk of this approach using
the Edgeworth expansion to terms of order $1/m^2$.&lt;/p></description></item><item><title>Edgeworth expansion for the Mann-Whitney U test</title><link>https://aakinshin.net/posts/mw-edgeworth/</link><pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mw-edgeworth/</guid><description>&lt;p>In &lt;a href="https://aakinshin.net/posts/r-mann-whitney-incorrect-p-value/">previous posts&lt;/a>,
I have shown a severe drawback of the classic Normal approximation for the Mann-Whitney U test:
under certain conditions, can lead to quite substantial p-value errors,
distorting the significance level of the test.&lt;/p>
&lt;p>In this post, we will explore the potential of the Edgeworth expansion
as a more accurate alternative for approximating the distribution of the Mann-Whitney U statistic.&lt;/p></description></item><item><title>Confusing tie correction in the classic Mann-Whitney U test implementation</title><link>https://aakinshin.net/posts/mw-confusing-tie-correction/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mw-confusing-tie-correction/</guid><description>&lt;p>In this post, we discuss the classic implementation of the Mann-Whitney U test for cases
in which the considered samples contain tied values.
This approach is used the same way in all the popular statistical packages.&lt;/p>
&lt;p>Unfortunately, in some situations, this approach produces confusing p-values, which may be surprising for researchers
who do not have a deep understanding of ties correction.
Moreover, some statistical textbooks argue against the validity of the default tie correction.
The controversialness and counterintuitiveness of this approach may become a severe issue which may lead
to incorrect experiment design and flawed result interpretation.
In order to prevent such problems, it is essential to clearly understand
the actual impact of tied observations on the true p-value and
the impact of tie correction on the approximated p-value estimation.
In this post, we discuss the tie correction for the Mann-Whitney U test
and review examples that illustrate potential problems.
We also provide examples of the Mann-Whitney U test implementations from popular statistical packages:
&lt;code>wilcox.test&lt;/code> from &lt;code>stats&lt;/code> (R),
&lt;code>mannwhitneyu&lt;/code> from &lt;code>SciPy&lt;/code> (Python), and
&lt;code>MannWhitneyUTest&lt;/code> from &lt;code>HypothesisTests&lt;/code> (Julia).
At the end of the post, we discuss how to avoid possible problems related to the tie correction.&lt;/p></description></item><item><title>Efficiency of the central tendency measures under the uniform distribution</title><link>https://aakinshin.net/posts/central-tendency-efficiency-uniform/</link><pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/central-tendency-efficiency-uniform/</guid><description>&lt;p>Statistical efficiency is one of the primary ways to compare various estimators.
Since the normality assumption is often used, Gaussian efficiency (efficiency under the normality distribution)
is typically considered.
For example, the asymptotic Gaussian efficiency values of
the median and the Hodges-Lehmann location estimator (the pseudo-median)
are $\approx 64\%$ and $\approx 96\%$ respectively (assuming the baseline is the mean).&lt;/p>
&lt;p>But what if the underlying distribution is not normal, but uniform?
What would happen to the relative statistical efficiency values in this case?
Let&amp;rsquo;s find out!
In this post, we calculate
the relative efficiency of the median, the Hodges-Lehmann location estimator, and the midrange
to the mean under the uniform distribution (or under uniformity).&lt;/p></description></item><item><title>Unobvious problems of using the R's implementation of the Hodges-Lehmann estimator</title><link>https://aakinshin.net/posts/r-hodges-lehmann-problems/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/r-hodges-lehmann-problems/</guid><description>&lt;p>The Hodges-Lehmann location estimator (also known as pseudo-median) is a robust, non-parametric statistic
used as a measure of the central tendency.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$, it is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right).
$$
&lt;p>Essentially, it&amp;rsquo;s the median of the Walsh (pairwise) averages.&lt;/p>
&lt;p>For two samples $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$ and $\mathbf{y} = \{ y_1, y_2, \ldots, y_m \}$,
we can also consider the Hodges-Lehmann location shift estimator:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>In R, both estimators are available via
the &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html">wilcox.test&lt;/a> function.
Here is a usage example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">set.seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1729&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># A sample of size 2000 from the normal distribution N(5, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># A sample of size 2000 from the normal distribution N(2, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># (pseudo)median&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 5.000984&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># (pseudo)median&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1.969096&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># difference in location&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 3.031782&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In most cases, this function works fine.
However, there is an unobvious corner case, in which it returns wrong values.
In this post, we discuss the underlying problem and provide a correct implementation for the Hodges-Lehmann estimators.&lt;/p></description></item><item><title>When Python's Mann-Whitney U test returns extremely distorted p-values</title><link>https://aakinshin.net/posts/python-mann-whitney-incorrect-p-value/</link><pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/python-mann-whitney-incorrect-p-value/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/r-mann-whitney-incorrect-p-value/">previous post&lt;/a>,
I have discussed a huge difference between p-values evaluated via the R implementation of
the &lt;a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann-Whitney U test&lt;/a>
between the exact and asymptotic implementations.
This issue is not unique only to R, it is relevant for other statistical packages in other languages as well.
In this post, we review this problem in the Python package &lt;a href="https://scipy.org/">SciPy&lt;/a>.&lt;/p></description></item><item><title>When R's Mann-Whitney U test returns extremely distorted p-values</title><link>https://aakinshin.net/posts/r-mann-whitney-incorrect-p-value/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/r-mann-whitney-incorrect-p-value/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann–Whitney U test&lt;/a>
(also known as the Wilcoxon rank-sum test)
is one of the most popular nonparametric statistical tests.
In R, it can be accessed using
the &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html">wilcox.test&lt;/a> function,
which has been &lt;a href="https://github.com/wch/r-source/blob/tags/R-1-0/src/library/ctest/R/wilcox.test.R">available&lt;/a>
since R 1.0.0 (February 2000).
With its extensive adoption and long-standing presence in R,
the &lt;code>wilcox.test&lt;/code> function has become a trusted tool for many researchers.
But is it truly reliable, and to what extent can we rely on its accuracy by default?&lt;/p>
&lt;p>In my work,
I often encounter the task of comparing a large sample (e.g., of size 50+) with a small sample (e.g., of size 5).
In some cases, the ranges of these samples do not overlap with each other,
which is the extreme case of the Mann–Whitney U test: it gives the minimum possible p-value.
In &lt;a href="https://aakinshin.net/posts/mann-whitney-min-stat-level/">one of the previous posts&lt;/a>,
I presented the exact equation for such a p-value.
If we compare two samples of sizes $n$ and $m$,
the minimum p-value we can observe with the one-tailed Mann–Whitney U test is $1/C_{n+m}^n$.
For example, if $n=50$ and $m=5$, we get $1/C_{55}^5 \approx 0.0000002874587$.
Let&amp;rsquo;s check these calculations using R:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">101&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">105&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">50&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">alternative&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;greater&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">p.value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">0.0001337028&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The obtained p-value is $\approx 0.0001337028$, which is $\approx 465$ times larger than we expected!
Have we discovered a critical bug in &lt;code>wilcox.test&lt;/code>?
Can we now trust this function?
Let&amp;rsquo;s find out!&lt;/p></description></item><item><title>Preprint announcement: 'Weighted quantile estimators'</title><link>https://aakinshin.net/posts/preprint-wqe/</link><pubDate>Tue, 18 Apr 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/preprint-wqe/</guid><description>&lt;p>I have just published a preprint of a paper &amp;lsquo;Weighted quantile estimators&amp;rsquo;.
It&amp;rsquo;s based on a series of my &lt;a href="https://aakinshin.net/tags/research-wqe/">research notes&lt;/a> that I have been writing since September 2020.&lt;/p>
&lt;p>The paper preprint is available on arXiv:
&lt;a href="https://arxiv.org/abs/2304.07265">arXiv:2304.07265 [stat.ME]&lt;/a>.
The paper source code is available on GitHub:
&lt;a href="https://github.com/AndreyAkinshin/paper-wqe">AndreyAkinshin/paper-wqe&lt;/a>.
You can cite it as follows:&lt;/p>
&lt;ul>
&lt;li>Andrey Akinshin (2023)
&amp;ldquo;Weighted quantile estimators&amp;rdquo;
&lt;a href="https://arxiv.org/abs/2304.07265">arXiv:2304.07265&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Abstract:&lt;/p>
&lt;blockquote>
&lt;p>In this paper, we consider a generic scheme that allows building weighted versions of various quantile estimators,
such as traditional quantile estimators based on linear interpolation of two order statistics,
the Harrell-Davis quantile estimator and its trimmed modification.
The obtained weighted quantile estimators are especially useful
in the problem of estimating a distribution at the tail of a time series using quantile exponential smoothing.
The presented approach can also be applied to other problems,
such as quantile estimation of weighted mixture distributions.&lt;/p>
&lt;/blockquote></description></item><item><title>Rethinking Type I/II error rates with power curves</title><link>https://aakinshin.net/posts/rethinking-type-i-ii-errors/</link><pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rethinking-type-i-ii-errors/</guid><description>&lt;p>When it comes to the analysis of a statistical significance test design,
many people tend to overfocus purely on the Type I error rate.
Those who are aware of the importance of power analysis
often stop at expressing the Type II error rate as a single number.
It is better than nothing, but such an approach always confuses me.&lt;/p>
&lt;p>Let us say that the declared Type II error rate is 20% (or the declared statistical power is 80%).
What does it actually mean?
If the sample size and the significance level (or any other significance criteria) are given,
the Type II error rate is a function of the effect size.
When we express the Type II error rate as a single number,
we always (implicitly or explicitly) assume the target effect size.
In most cases, it is an arbitrary number
that is somehow chosen to reflect our expectations of the &amp;ldquo;reasonable&amp;rdquo; effect size.
However, the actual Type II error rate and the corresponding statistical power
depend on the actual effect size that we do not know.
Some researchers estimate the Type II error rate / statistical power using the measured effect size,
but it does not make a lot of sense since
it does not provide new information in addition to the measured effect size or p-value.
In reality, we have high statistical power (low Type II error rate) for large effect sizes
and low statistical power (high Type II error rate) for small effect sizes.
Without the knowledge of the actual effect size (which we do not have),
the Type II error rate expressed as a single number mostly describes this arbitrarily chosen expected effect size,
rather than the actual properties of our statistical test.&lt;/p></description></item><item><title>Adaptation of continuous scale measures to discrete distributions</title><link>https://aakinshin.net/posts/scale-measure-for-discrete-case/</link><pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/scale-measure-for-discrete-case/</guid><description>&lt;p>In statistics, it is often important to have a reliable measure of scale
since it is required for estimating many types of the effect size and for statistical tests.
If we work with continuous distributions,
there are plenty of available scale measures with various levels of statistical efficiency and robustness.
However, when distribution becomes discrete (e.g. because of the limited resolution of the measure tools),
classic measures of scale can collapse to zero due to tied values in collected samples.
This can be a severe problem in the analysis
since the scale measures are often used as denominators in various equations.
To make the calculations more reliable,
it is important to handle such situations somehow and ensure that the target scale measure never becomes zero.
In this post,
I discuss a simple approach to work around this problem and adapt any given measure of scale to the discrete case.&lt;/p></description></item><item><title>Weighted modification of the Hodges-Lehmann location estimator</title><link>https://aakinshin.net/posts/whl/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/whl/</guid><description>&lt;p>The classic Hodges-Lehmann location estimator is a robust, non-parametric statistic
used as a measure of the central tendency.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$, it is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) = \underset{1 \leq i &lt; j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right).
$$
&lt;p>This estimator works great for non-weighted samples
(its asymptotic Gaussian efficiency is $\approx 96\%$, and its asymptotic breakdown point is $\approx 29\%$).
However, in real-world applications, data points may have varying importance or relevance.
For example, in finance, different stocks may have different market capitalizations,
which can impact the overall performance of an index.
In social science research, survey responses may be weighted
based on demographic representation to ensure that the final results are more generalizable.
In software performance measurements, the observations may be collected from different source code revisions,
some of which may be obsolete.
In these cases, the classic $\operatorname{HL}$-measure is not suitable, as it treats each data point equally.&lt;/p>
&lt;p>We can overcome this problem using weighted samples to obtain more accurate and meaningful central tendency estimates.
Unfortunately, there is no well-established definition of the weighted Hodges-Lehmann location estimator.
In this blog post, we introduce such a definition so that we can apply this estimator to weighted samples
keeping it compatible with the original version.&lt;/p></description></item><item><title>p-value distribution of the Brunner–Munzel test in the finite case</title><link>https://aakinshin.net/posts/brunner-munzel-pvalue-distribution/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/brunner-munzel-pvalue-distribution/</guid><description>&lt;p>In our of the previous post, I explored the
&lt;a href="https://aakinshin.net/posts/mann-whitney-pvalue-distribution/">distribution of observed p-values for the Mann–Whitney U test&lt;/a>
in the finite case when the null hypothesis is true.
It is time to repeat the experiment for the Brunner–Munzel test.&lt;/p></description></item><item><title>Comparing statistical power of the Mann-Whitney U test and the Brunner-Munzel test</title><link>https://aakinshin.net/posts/power-mw-bm/</link><pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/power-mw-bm/</guid><description>&lt;p>In this post, we perform a short numerical simulation to compare the statistical power
of the Mann-Whitney U test and the Brunner-Munzel test under normality
for various sample sizes and significance levels.&lt;/p></description></item><item><title>p-value distribution of the Mann–Whitney U test in the finite case</title><link>https://aakinshin.net/posts/mann-whitney-pvalue-distribution/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mann-whitney-pvalue-distribution/</guid><description>&lt;p>When we work with null hypothesis significance testing and the null hypothesis is true,
the distribution of observed p-value is asymptotically uniform.
However, the distribution shape is not always uniform in the finite case.
For example, when we work with rank-based tests like the Mann–Whitney U test,
the distribution of the p-values is discrete with a limited set of possible values.
This should be taken into account when we design a testing procedure for small samples
and choose the significance level.&lt;/p>
&lt;p>Previously, we already discussed the &lt;a href="https://aakinshin.net/posts/mann-whitney-min-stat-level/">minimum reasonable significance level&lt;/a>
of the Mann-Whitney U test for small samples.
In this post, we explore the full distribution of the p-values for this case.&lt;/p></description></item><item><title>Types of finite-sample consistency with the standard deviation</title><link>https://aakinshin.net/posts/sd-consistency-types/</link><pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sd-consistency-types/</guid><description>&lt;p>Let us say we have a robust dispersion estimator $\operatorname{T}(X)$.
If it is asymptotically consistent with the standard deviation,
we can use such an estimator as a robust replacement for the standard deviation under normality.
Thanks to asymptotical consistency, we can use the estimator &amp;ldquo;as is&amp;rdquo; for large samples.
However, if the number of sample elements is small,
we typically need finite-sample bias-correction factors to make the estimator unbiased.
Here we should clearly understand what kind of consistency we need.&lt;/p>
&lt;p>There are various ways to estimate the standard deviation.
Let us consider a sample of random variables $X = \{ X_1, X_2, \ldots, X_n \}$.
The most popular equation of the standard deviation is given by&lt;/p>
$$
s(X) = \sqrt{\frac{1}{n - 1} \sum_{i=1}^n (X_i - \overline{X})^2}.
$$
&lt;p>Using this definition, we can get an unbiased estimator for the population variance: $\mathbb{E}[s^2(X)] = 1$.
However, it is a biased estimator for the population standard deviation: $\mathbb{E}[s(X)] \neq 1$.
To obtain to corresponding unbiased estimator, we should use $s(\mathbf{x}) \cdot c_4(n)$,
where $c_4(n)$ is a correction factor defined as follows:&lt;/p>
$$
c_4(n) = \sqrt{\frac{2}{n-1}} \cdot \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}.
$$
&lt;p>When we define finite-sample bias-correction factors for a robust standard deviation replacement,
we should choose which kind of consistency we need.
In this post, I briefly explore available options.&lt;/p></description></item><item><title>Nonparametric effect size: Cohen's d vs. Glass's delta</title><link>https://aakinshin.net/posts/gamma-es-cohen-glass/</link><pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gamma-es-cohen-glass/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/tags/research-gamma-es/">previous posts&lt;/a>,
I discussed the idea of nonparametric effect size measures
consistent with Cohen&amp;rsquo;s d under normality.
However, Cohen&amp;rsquo;s d is not always the best effect size measure, even in the normal case.&lt;/p>
&lt;p>In this post, we briefly discuss a case study in which a nonparametric version of Glass&amp;rsquo;s delta is preferable
than the &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">previously suggested&lt;/a> Cohen&amp;rsquo;s d-consistent measure.&lt;/p></description></item><item><title>Trinal statistical thresholds</title><link>https://aakinshin.net/posts/trinal-thresholds/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/trinal-thresholds/</guid><description>&lt;p>When we design a test for practical significance, which compares two samples,
we should somehow express the threshold.
The most popular options are the shift, the ratio, and the effect size.
Unfortunately, if we have little information about the underlying distributions,
it&amp;rsquo;s hard to get a reliable test based only on a single threshold.
And it&amp;rsquo;s almost impossible to define a generic threshold that fits all situations.
After struggling with a lot of different thresholding approaches,
I came up with the idea of setting a trinal threshold
that includes three individual thresholds for the shift, the ratio, and the effect size.&lt;/p>
&lt;p>In this post, I show some examples in which a single threshold is not enough.&lt;/p></description></item><item><title>Trimmed Hodges-Lehmann location estimator, Part 2: Gaussian efficiency</title><link>https://aakinshin.net/posts/thl-ge/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thl-ge/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/thl-bp/">previous post&lt;/a>, we introduced
the trimmed Hodges-Lehman location estimator.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$,
it is defined as follows:&lt;/p>
$$
\operatorname{THL}(\mathbf{x}, k) = \underset{k &lt; i &lt; j \leq n - k}{\operatorname{median}}\biggl(\frac{x_{(i)} + x_{(j)}}{2}\biggr).
$$
&lt;p>We also derived the exact expression for its asymptotic and finite-sample breakdown point values.
In this post, we explore its Gaussian efficiency.&lt;/p></description></item><item><title>Trimmed Hodges-Lehmann location estimator, Part 1: breakdown point</title><link>https://aakinshin.net/posts/thl-bp/</link><pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thl-bp/</guid><description>&lt;p>For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$,
the Hodges-Lehmann location estimator is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{i &lt; j}{\operatorname{median}}\biggl(\frac{x_i + x_j}{2}\biggr).
$$
&lt;p>Its asymptotic Gaussian efficiency is $\approx 96\%$,
while its asymptotic breakdown point is $\approx 29\%$.
This makes the Hodges-Lehmann location estimator a decent robust alternative to the mean.&lt;/p>
&lt;p>While the Gaussian efficiency is quite impressive (almost as efficient as the mean),
the breakdown point is not as great as in the case of the median (which has a breakdown point of $50\%$).
Could we change this trade-off a little bit and make this estimator more robust,
sacrificing a small portion of efficiency?
Yes, we can!&lt;/p>
&lt;p>In this post, I want to present the idea of the trimmed Hodges-Lehmann location estimator
and provide the exact equation for its breakdown point.&lt;/p></description></item><item><title>Median of the shifts vs. shift of the medians, Part 2: Gaussian efficiency</title><link>https://aakinshin.net/posts/median-shift-vs-shift-median2/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-shift-vs-shift-median2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median1/">previous post&lt;/a>,
we discussed the difference between shifts of the medians
and the Hodges-Lehmann location shift estimator.
In this post, we conduct a simple numerical simulation
to evaluate the Gaussian efficiency of these two estimators.&lt;/p></description></item><item><title>Median of the shifts vs. shift of the medians, Part 1</title><link>https://aakinshin.net/posts/median-shift-vs-shift-median1/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-shift-vs-shift-median1/</guid><description>&lt;p>Let us say that we have two samples
$x = \{ x_1, x_2, \ldots, x_n \}$,
$y = \{ y_1, y_2, \ldots, y_m \}$,
and we want to estimate the shift of locations between them.
In the case of the normal distribution, this task is quite simple
and has a lot of straightforward solutions.
However, in the nonparametric case, the location shift is an ambiguous metric
which heavily depends on the chosen estimator.
In the context of this post, we consider two approaches that may look similar.
The first one is the &lt;strong>s&lt;/strong>hift of the &lt;strong>m&lt;/strong>edians:&lt;/p>
$$
\newcommand{\DSM}{\Delta_{\operatorname{SM}}}
\DSM = \operatorname{median}(y) - \operatorname{median}(x).
$$
&lt;p>The second one of the median of all pairwise shifts,
also known as the &lt;strong>H&lt;/strong>odges-&lt;strong>L&lt;/strong>ehmann location shift estimator:&lt;/p>
$$
\newcommand{\DHL}{\Delta_{\operatorname{HL}}}
\DHL = \operatorname{median}(y_j - x_i).
$$
&lt;p>In the case of the normal distributions, these estimators are consistent.
However, this post will show an example of multimodal distributions
that lead to opposite signs of $\DSM$ and $\DHL$.&lt;/p></description></item><item><title>Resistance to the low-density regions: the Hodges-Lehmann location estimator</title><link>https://aakinshin.net/posts/rldr-hl/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-hl/</guid><description>&lt;p>In the previous posts, I discussed the concept of a resistance function
that shows the sensitivity of the given estimator to the low-density regions.
I already showed how this function behaves for &lt;a href="https://aakinshin.net/posts/rldr-mean-median/">the mean, the sample median&lt;/a>,
and &lt;a href="https://aakinshin.net/posts/rldr-hdmedian/">the Harrell-Davis median&lt;/a>.
In this post, I explore this function for the Hodges-Lehmann location estimator.&lt;/p></description></item><item><title>Kernel density estimation boundary correction: reflection (ggplot2 v3.4.0)</title><link>https://aakinshin.net/posts/kde-bc-reflection/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kde-bc-reflection/</guid><description>&lt;p>Kernel density estimation (KDE) is a popular way to approximate a distribution based on the given data.
However, it has several flaws.
One of the most significant flaws is that it extends the support of the distribution.
It is pretty unfortunate: even if we know the actual range of supported values,
KDE provides non-zero density values for the regions where no values exist.
It is obviously an inaccurate estimation.
The procedure of adjusting the KDE values according to the given boundaries is known as &lt;em>boundary correction&lt;/em>.
As usual, there are plenty of available boundary correction strategies.&lt;/p>
&lt;p>One such strategy was implemented in the
&lt;a href="https://www.tidyverse.org/blog/2022/11/ggplot2-3-4-0/#bounded-density-estimation">v3.4.0 update&lt;/a> of
&lt;a href="https://ggplot2.tidyverse.org/">ggplot2&lt;/a> (a popular R package for plotting)
thanks to &lt;a href="https://github.com/tidyverse/ggplot2/pull/4013/">pull request #4013&lt;/a>.
At the present moment, it supports a single boundary correction strategy called &lt;em>reflection&lt;/em>.
In this post, we discuss this approach and see how it works in practice.&lt;/p></description></item><item><title>Sheather &amp; Jones vs. unbiased cross-validation</title><link>https://aakinshin.net/posts/sj-vs-ucv/</link><pubDate>Tue, 29 Nov 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sj-vs-ucv/</guid><description>&lt;p>In the post about &lt;a href="https://aakinshin.net/posts/kde-bw/">the importance of kernel density estimation bandwidth&lt;/a>,
I reviewed several bandwidth selectors and showed their impact on the KDE.
The classic selectors like Scott&amp;rsquo;s rule of thumb or Silverman&amp;rsquo;s rule of thumb are designed for the normal distribution
and perform purely in non-parametric cases.
One of the most significant caveats is that they can mask multimodality.
The same problem is also relevant to the biased cross-validation method.
Among all the bandwidth selectors
&lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/bandwidth.html">available in R&lt;/a>,
only Sheather &amp;amp; Jones and unbiased cross-validation provide reliable results in the multimodal case.
However, I always advocate using the Sheather &amp;amp; Jones method rather than the unbiased cross-validation approach.&lt;/p>
&lt;p>In this post, I will show the drawbacks of the unbiased cross-validation method
and what kind of problems we can get if we use it as a KDE bandwidth selector.&lt;/p></description></item><item><title>Resistance to the low-density regions: the Harrell-Davis median</title><link>https://aakinshin.net/posts/rldr-hdmedian/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-hdmedian/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/rldr-mean-median/">previous post&lt;/a>,
we defined the resistance function that show sensitivity of the given estimator
to the low-density regions.
We also showed the resistance function plots for the mean and the sample median.
In this post, we explore corresponding plots for the Harrell-Davis median.&lt;/p></description></item><item><title>Resistance to the low-density regions: the mean and the median</title><link>https://aakinshin.net/posts/rldr-mean-median/</link><pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-mean-median/</guid><description>&lt;p>When we discuss resistant statistics, we typically assume resistance to extreme values.
However, extreme values are not the only problem source
that can violate usual assumptions about expected metric distribution.
The low-density regions which often arise in multimodal distributions
can also corrupt the results of the statistical analysis.
In this post, I discuss this problem and introduce a measure of resistance to low-density regions.&lt;/p></description></item><item><title>Finite-sample Gaussian efficiency of the trimmed Harrell-Davis median estimator</title><link>https://aakinshin.net/posts/thd-ge/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thd-ge/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/hd-ge/">previous post&lt;/a>,
we obtained the finite-sample Gaussian efficiency values
of the sample median and the Harrell-Davis median.
In this post, we extended these results
and get the finite-sample Gaussian efficiency values
of the &lt;a href="https://aakinshin.net/posts/pub-thdqe/">trimmed Harrell-Davis median estimator based on the highest density interval of the width $1/\sqrt{n}$&lt;/a>.&lt;/p></description></item><item><title>Finite-sample Gaussian efficiency of the Harrell-Davis median estimator</title><link>https://aakinshin.net/posts/hd-ge/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-ge/</guid><description>&lt;p>In this post, we explore finite-sample and asymptotic Gaussian efficiency values
of the sample median and the Harrell-Davis median.&lt;/p></description></item><item><title>Weighted quantile estimation for a weighted mixture distribution</title><link>https://aakinshin.net/posts/wqe-mixture/</link><pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wqe-mixture/</guid><description>&lt;p>Let $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$ be a sample of size $n$.
We assign non-negative weight coefficients $w_i$ with a positive sum for all sample elements:&lt;/p>
$$
\mathbf{w} = \{ w_1, w_2, \ldots, w_n \}, \quad w_i \geq 0, \quad \sum_{i=1}^{n} w_i > 0.
$$
&lt;p>For simplification, we also consider normalized (standardized) weights $\overline{\mathbf{w}}$:&lt;/p>
$$
\overline{\mathbf{w}} = \{ \overline{w}_1, \overline{w}_2, \ldots, \overline{w}_n \}, \quad
 \overline{w}_i = \frac{w_i}{\sum_{i=1}^{n} w_i}.
$$
&lt;p>In the non-weighted case, we can consider a quantile estimator $\operatorname{Q}(\mathbf{x}, p)$
that estimates the $p^\textrm{th}$ quantile of the underlying distribution.
We want to build a weighted quantile estimator $\operatorname{Q}(\mathbf{x}, \mathbf{w}, p)$
so that we can estimate the quantiles of a weighed sample.&lt;/p>
&lt;p>In this post, we consider a specific problem of estimating quantiles of a weighted mixture distribution.&lt;/p></description></item><item><title>Sensitivity curve of the Harrell-Davis quantile estimator, Part 3</title><link>https://aakinshin.net/posts/hd-sc3/</link><pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sc3/</guid><description>&lt;p>In the previous posts (&lt;a href="https://aakinshin.net/posts/hd-sc1/">1&lt;/a>, &lt;a href="https://aakinshin.net/posts/hd-sc2/">2&lt;/a>), I have explored the sensitivity curves of
the Harrell-Davis quantile estimator on
the normal distribution, the exponential distribution, and the Cauchy distribution.
In this post, I build these sensitivity curves for some additional distributions.&lt;/p></description></item><item><title>Sensitivity curve of the Harrell-Davis quantile estimator, Part 2</title><link>https://aakinshin.net/posts/hd-sc2/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sc2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/hd-sc1/">previous post&lt;/a>, I have explored the sensitivity curves of
the Harrell-Davis quantile estimator on the normal distribution.
In this post, I continue the same investigation on the exponential and Cauchy distributions.&lt;/p></description></item><item><title>Sensitivity curve of the Harrell-Davis quantile estimator, Part 1</title><link>https://aakinshin.net/posts/hd-sc1/</link><pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sc1/</guid><description>&lt;p>The Harrell-Davis quantile estimator is an efficient replacement for the traditional quantile estimator,
especially in the case of light-tailed distributions.
Unfortunately, it is not robust: its breakdown point is zero.
However, the breakdown point is not the only descriptor of robustness.
While the breakdown point describes the portion of the distribution that should be replaced by
arbitrary large values to corrupt the estimation,
it does not describe the actual impact of finite outliers.
The arithmetic mean also has the breakdown point of zero,
but the practical robustness of the mean and the Harrell-Davis quantile estimator are not the same.
The Harrell-Davis quantile estimator is an L-estimator
that assigns extremely low weights to sample elements near the tails
(especially, for reasonably large sample sizes).
Therefore, the actual impact of potential outliers is not so noticeable.
In this post, we use the standardized sensitivity curve to evaluate this impact.&lt;/p></description></item><item><title>Weighted quantile estimators for exponential smoothing and mixture distributions</title><link>https://aakinshin.net/posts/wqe-smoothing-mixture/</link><pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wqe-smoothing-mixture/</guid><description>&lt;p>There are various ways to estimate quantiles of weighted samples.
The proper choice of the most appropriate weighted quantile estimator depends not only on the own estimator properties
but also on the goal.&lt;/p>
&lt;p>Let us consider two problems:&lt;/p>
&lt;ol>
&lt;li>&lt;em>Estimating quantiles of a weighted mixture distribution.&lt;/em>&lt;br />
In this problem, we have a weighted mixture distribution given by $F = \sum_{i=1}^m w_i F_i$.
We collect samples $\mathbf{x_1}, \mathbf{x_2}, \ldots, \mathbf{x_m}$ from $F_1, F_2, \ldots F_m$,
and want to estimate quantile function $F^{-1}$ of the mixture distribution based on the given samples.&lt;/li>
&lt;li>&lt;em>Quantile exponential smoothing.&lt;/em>&lt;br />
In this problem, we have a time series $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$.
We want to describe the distribution &amp;ldquo;at the end&amp;rdquo; of this time series.
The latest series element $x_n$ is the most &amp;ldquo;actual&amp;rdquo; one, but we cannot build a distribution based on a single element.
Therefore, we have to consider more elements at the end of $\mathbf{x}$.
However, if we take too many elements, we may corrupt the estimations due to obsolete measurements.
To resolve this problem, we can assign weights to all elements according to the exponential law
and estimate weighted quantiles.&lt;/li>
&lt;/ol>
&lt;p>In both problems, the usage of weighted quantile estimators looks like a reasonable solution.
However, in each problem, we have different expectations of the estimator behavior.
In this post, we provide an example that illustrates the difference in these expectations.&lt;/p></description></item><item><title>The Huggins-Roy family of effective sample sizes</title><link>https://aakinshin.net/posts/huggins-roy-ess/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/huggins-roy-ess/</guid><description>&lt;p>When we work with weighted samples, it&amp;rsquo;s essential to introduce adjustments for the sample size.
Indeed, let&amp;rsquo;s consider two following weighted samples:&lt;/p>
$$
\mathbf{x}_1 = \{ x_1, x_2, \ldots, x_n \}, \quad \mathbf{w}_1 = \{ w_1, w_2, \ldots, w_n \},
$$
$$
\mathbf{x}_2 = \{ x_1, x_2, \ldots, x_n, x_{n+1} \}, \quad \mathbf{w}_2 = \{ w_1, w_2, \ldots, w_n, 0 \}.
$$
&lt;p>Since the weight of $x_{n+1}$ in the second sample is zero,
it&amp;rsquo;s natural to expect that both samples have the same set of properties.
However, there is a major difference between $\mathbf{x}_1$ and $\mathbf{x}_2$: their sample sizes which are
$n$ and $n+1$.
In order to eliminate this difference, we typically introduce the &lt;em>effective sample size&lt;/em> (ESS)
which is estimated based on the list of weights.&lt;/p>
&lt;p>There are various ways to estimate the ESS.
In this post, we briefly discuss the Huggins-Roy&amp;rsquo;s family of ESS.&lt;/p></description></item><item><title>Finite-sample bias correction factors for Rousseeuw-Croux scale estimators</title><link>https://aakinshin.net/posts/rousseeuw-croux-finite-factors/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rousseeuw-croux-finite-factors/</guid><description>&lt;p>The Rousseeuw-Croux scale estimators $S_n$ and $Q_n$
are &lt;a href="https://aakinshin.net/posts/rousseeuw-croux-finite-efficiency/">efficient&lt;/a> alternatives to the median absolute deviation ($\operatorname{MAD}_n$).
While all three estimators have the same breakdown point of $50\%$,
$S_n$ and $Q_n$ have higher statistical efficiency than $\operatorname{MAD}_n$.
The asymptotic Gaussian efficiency values of $\operatorname{MAD}_n$, $S_n$, and $Q_n$
are $37\%$, $58\%$, and $82\%$ respectively.&lt;/p>
&lt;p>Using scale constants, we can make $S_n$ and $Q_n$ consistent estimators for the standard deviation under normality.
The asymptotic values of these constants are well-known.
However, for finite-samples, only approximated scale constants are known.
In this post, we provide refined values of these constants with higher accuracy.&lt;/p></description></item><item><title>Standard trimmed Harrell-Davis median estimator</title><link>https://aakinshin.net/posts/sthdme/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sthdme/</guid><description>&lt;p>In &lt;a href="https://aakinshin.net/posts/sqad/">one of the previous posts&lt;/a>, I suggested a new measure of dispersion called
&lt;em>the standard quantile absolute deviation around the median&lt;/em> ($\operatorname{SQAD}$) which can be used as an alternative
to the median absolute deviation ($\operatorname{MAD}$) as a consistent estimator for the standard deviation under normality.
The Gaussian efficiency of $\operatorname{SQAD}$ is $54\%$ (comparing to $37\%$ for MAD),
and its breakdown point is $32\%$ (comparing to $50\%$ for MAD).
$\operatorname{SQAD}$ is a symmetric dispersion measure around the median:
the interval $[\operatorname{Median} - \operatorname{SQAD}; \operatorname{Median} + \operatorname{SQAD}]$
covers $68\%$ of the distribution.
In the case of the normal distribution, this corresponds to the interval $[\mu - \sigma; \mu + \sigma]$.&lt;/p>
&lt;p>If we use $\operatorname{SQAD}$, we accept the breakdown point of $32\%$.
This makes the sample median a non-optimal choice for the median estimator.
Indeed, the sample median has high robustness (the breakdown point is $50\%$),
but relatively poor Gaussian efficiency.
If we use $\operatorname{SQAD}$, it doesn&amp;rsquo;t make sense to require a breakdown point of more than $32\%$.
Therefore, we could trade the median robustness for efficiency
and come up with a complementary measure of the median for $\operatorname{SQAD}$.&lt;/p>
&lt;p>In this post, we introduce the standard trimmed Harrell-Davis median estimator which shares
the breakdown point with $\operatorname{SQAD}$ and provides better finite-sample efficiency comparing
to the sample median.&lt;/p></description></item><item><title>Optimal quantile absolute deviation</title><link>https://aakinshin.net/posts/oqad/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/oqad/</guid><description>&lt;p>We consider the quantile absolute deviation around the median defined as follows:&lt;/p>
$$
\newcommand{\E}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\Q}{\operatorname{Q}}
\newcommand{\OQAD}{\operatorname{OQAD}}
\newcommand{\QAD}{\operatorname{QAD}}
\newcommand{\median}{\operatorname{median}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\V}{\mathbb{V}}
\QAD(X, p) = K_p \Q(|X - \median(X)|, p),
$$
&lt;p>where $\Q$ is a quantile estimator,
and $K_p$ is a scale constant which we use to make $\QAD(X, p)$ an asymptotically consistent estimator
for the standard deviation under the normal distribution.&lt;/p>
&lt;p>In this post, we get the exact values of the $K_p$ values,
derive the corresponding equation for the asymptotic Gaussian efficiency of $\QAD(X, p)$,
and find the point in which $\QAD(X, p)$ achieves the highest Gaussian efficiency.&lt;/p></description></item><item><title>Quantile absolute deviation of the Pareto distribution</title><link>https://aakinshin.net/posts/qad-pareto/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-pareto/</guid><description>&lt;p>In this post,
we derive the exact equation for the quantile absolute deviation around the median of the Pareto(1,1) distribution.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qad-pareto/img/qad-light.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-pareto/img/qad-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qad-pareto/img/qad-dark.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-pareto/img/qad-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Quantile absolute deviation of the Exponential distribution</title><link>https://aakinshin.net/posts/qad-exp/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-exp/</guid><description>&lt;p>In this post,
we derive the exact equation for the quantile absolute deviation around the median of the Exponential distribution.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qad-exp/img/qad-light.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-exp/img/qad-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qad-exp/img/qad-dark.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-exp/img/qad-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Quantile absolute deviation of the Uniform distribution</title><link>https://aakinshin.net/posts/qad-uniform/</link><pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-uniform/</guid><description>&lt;p>In this post,
we derive the exact equation for the quantile absolute deviation around the median of the Uniform distribution.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qad-uniform/img/qad-light.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-uniform/img/qad-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qad-uniform/img/qad-dark.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-uniform/img/qad-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Quantile absolute deviation of the Normal distribution</title><link>https://aakinshin.net/posts/qad-normal/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-normal/</guid><description>&lt;p>In this post,
we derive the exact equation for the quantile absolute deviation around the median of the Normal distribution.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qad-normal/img/qad-light.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-normal/img/qad-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qad-normal/img/qad-dark.png" target="_blank" alt="qad">
 &lt;img
 src="https://aakinshin.net/posts/qad-normal/img/qad-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Standard quantile absolute deviation</title><link>https://aakinshin.net/posts/sqad/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sqad/</guid><description>&lt;p>The median absolute deviation (MAD) is a popular robust replacement of the standard deviation (StdDev).
It&amp;rsquo;s truly robust: its breakdown point is $50\%$.
However, it&amp;rsquo;s not so efficient when we use it as a consistent estimator for the standard deviation under normality:
the asymptotic relative efficiency against StdDev (we call it the &lt;em>Gaussian efficiency&lt;/em>) is only about $\approx 37\%$.&lt;/p>
&lt;p>In practice, such robustness is not always essential, while we typically want to have the highest possible efficiency.
I already described the concept of the &lt;a href="https://aakinshin.net/posts/qad/">&lt;em>quantile absolute deviation&lt;/em>&lt;/a> which aims
to provide a customizable trade-off between robustness and efficiency.
In this post, I would like to suggest a new default option for this measure of dispersion
called the &lt;em>standard quantile absolute deviation&lt;/em>.
Its Gaussian efficiency is $\approx 54\%$ while the breakdown point is $\approx 32\%$&lt;/p></description></item><item><title>Asymptotic Gaussian efficiency of the quantile absolute deviation</title><link>https://aakinshin.net/posts/qad-are/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad-are/</guid><description>&lt;p>I have already discussed the concept of the &lt;a href="https://aakinshin.net/posts/qad/">quantile absolute deviation&lt;/a>
in &lt;a href="https://aakinshin.net/tags/research-qad/">several previous posts&lt;/a>.
In this post, we derive the equation for the relative statistical efficiency of the quantile absolute deviation
against the standard deviation under the normal distribution (so call Gaussian efficiency).&lt;/p></description></item><item><title>Finite-sample efficiency of the Rousseeuw-Croux estimators</title><link>https://aakinshin.net/posts/rousseeuw-croux-finite-efficiency/</link><pubDate>Tue, 09 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rousseeuw-croux-finite-efficiency/</guid><description>&lt;p>The Rousseeuw-Croux $S_n$ and $Q_n$ estimators are robust and efficient measures of scale.
Their breakdown points are equal to $0.5$ which is also the breakdown point of the median absolute deviation (MAD).
However, their statistical efficiency values are much better than the efficiency of MAD.
To be specific, the MAD asymptotic relative Gaussian efficiency against the standard deviation is about $37\%$,
whereas the corresponding values for $S_n$ and $Q_n$ are $58\%$ and $82\%$ respectively.
Although these numbers are quite impressive, they are only &lt;em>asymptotic&lt;/em> values.
In practice, we work with finite samples.
And the &lt;em>finite-sample efficiency&lt;/em> could be much lower than the asymptotic one.
In this post, we perform a simulation study in order to obtain the actual finite-sample efficiency values
for these two estimators.&lt;/p></description></item><item><title>Caveats of using the median absolute deviation</title><link>https://aakinshin.net/posts/mad-caveats/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mad-caveats/</guid><description>&lt;p>The median absolute deviation is a measure of dispersion
which can be used as a robust alternative to the standard deviation.
It works great for slight deviations from normality
(e.g., for contaminated normal distributions or slightly skewed unimodal distributions).
Unfortunately, if we apply it to distributions with huge deviations from normality,
we may experience a lot of troubles.
In this post, I discuss some of the most important caveats which we should keep in mind
if we use the median absolute deviation.&lt;/p></description></item><item><title>Dynamical System Case Study 2 (Piecewise linear LLL-system)</title><link>https://aakinshin.net/posts/dscs2/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dscs2/</guid><description>&lt;p>We consider the following dynamical system:&lt;/p>
$$
\begin{cases}
 \dot{x}_1 = L(a_1, k_1, x_3) - k_1 x_1,\\
 \dot{x}_2 = L(a_2, k_2, x_1) - k_2 x_2,\\
 \dot{x}_3 = L(a_3, k_3, x_2) - k_3 x_3,
\end{cases}
$$
&lt;p>where $L$ is a piecewise linear function:&lt;/p>
$$
L(a, k, x) = \begin{cases}
ak &amp; \quad \textrm{for}\; 0 \leq x \leq 1,\\
0 &amp; \quad \textrm{for}\; 1 &lt; x.
\end{cases}
$$
&lt;p>In this case study, we build a &lt;a href="https://shiny.rstudio.com/">Shiny&lt;/a> application that draws 3D phase portraits of this system for various sets of input parameters.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/dscs2/img/screen-light.png" target="_blank" alt="screen">
 &lt;img
 src="https://aakinshin.net/posts/dscs2/img/screen-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/dscs2/img/screen-dark.png" target="_blank" alt="screen">
 &lt;img
 src="https://aakinshin.net/posts/dscs2/img/screen-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Degenerate point of dispersion estimators</title><link>https://aakinshin.net/posts/degenerate-point/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/degenerate-point/</guid><description>&lt;p>Recently, I have been working on searching for a robust statistical dispersion estimator
that doesn&amp;rsquo;t become zero on samples with a huge number of tied values.
I have already created a few of such estimators like
the &lt;em>middle non-zero quantile absolute deviation&lt;/em> (&lt;a href="https://aakinshin.net/posts/mnzqad/">part 1&lt;/a>, &lt;a href="https://aakinshin.net/posts/mnzqad2/">part 2&lt;/a>) and
the &lt;em>&lt;a href="https://aakinshin.net/posts/uqad/">untied quantile absolute deviation&lt;/a>&lt;/em>.
Having several options to compare, we need a proper metric that allows us to perform such a comparison.
Similar to the breakdown point (that is used to describe estimator robustness),
we could introduce the &lt;em>degenerate point&lt;/em> that describes the resistance of a dispersion estimator to the tied values.
In this post, I will briefly describe this concept.&lt;/p></description></item><item><title>Untied quantile absolute deviation</title><link>https://aakinshin.net/posts/uqad/</link><pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/uqad/</guid><description>&lt;p>In the previous posts, I tried to adapt the concept of the &lt;a href="https://aakinshin.net/posts/qad/">quantile absolute deviation&lt;/a>
to samples with tied values so that this measure of dispersion never becomes zero for nondegenerate ranges.
My previous attempt was the &lt;em>middle non-zero quantile absolute deviation&lt;/em>
(&lt;a href="https://aakinshin.net/posts/mnzqad/">modification 1&lt;/a>, &lt;a href="https://aakinshin.net/posts/mnzqad2/">modification 2&lt;/a>).
However, I&amp;rsquo;m not completely satisfied with the behavior of this metric.
In this post, I want to consider another way to work around the problem with tied values.&lt;/p></description></item><item><title>Middle non-zero quantile absolute deviation, Part 2</title><link>https://aakinshin.net/posts/mnzqad2/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mnzqad2/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/mnzqad/">described&lt;/a> the idea of the
middle non-zero quantile absolute deviation.
It&amp;rsquo;s defined as follows:&lt;/p>
$$
\operatorname{MNZQAD}(x, p) = \operatorname{QAD}(x, p, q_m),
$$
$$
q_m = \frac{q_0 + 1}{2}, \quad
q_0 = \frac{\max(k - 1, 0)}{n - 1}, \quad
k = \sum_{i=1}^n \mathbf{1}_{Q(x, p)}(x_i),
$$
&lt;p>where $\mathbf{1}$ is the indicator function&lt;/p>
$$
\mathbf{1}_U(u) = \begin{cases}
1 &amp; \textrm{if}\quad u = U,\\
0 &amp; \textrm{if}\quad u \neq U,
\end{cases}
$$
&lt;p>and $\operatorname{QAD}$ is the &lt;a href="https://aakinshin.net/posts/qad/">quantile absolute deviation&lt;/a>&lt;/p>
$$
\operatorname{QAD}(x, p, q) = Q(|x - Q(x, p)|, q).
$$
&lt;p>The $\operatorname{MNZQAD}$ approach tries to work around a problem with tied values.
While it works well in the generic case, there are some corner cases
where the suggested metric behaves poorly.
In this post, we discuss this problem and how to solve it.&lt;/p></description></item><item><title>Folded medians</title><link>https://aakinshin.net/posts/folded-medians/</link><pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/folded-medians/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/gastwirth/">previous post&lt;/a>, we discussed the Gastwirth&amp;rsquo;s location estimator.
In this post, we continue playing with different location estimators.
To be more specific, we consider an approach called &lt;em>folded medians&lt;/em>.
Let $x = \{ x_1, x_2, \ldots, x_n \}$ be a random sample with order statistics
$\{ x_{(1)}, x_{(2)}, \ldots, x_{(n)} \}$.
We build a folded sample using the following form:&lt;/p>
$$
\Bigg\{ \frac{x_{(1)}+x_{(n)}}{2}, \frac{x_{(2)}+x_{(n-1)}}{2}, \ldots, \Bigg\}.
$$
&lt;p>If $n$ is odd, the middle sample element is folded with itself.
The folding operation could be applied several times.
Once folding is conducted, the median of the final folded sample is the folded median.
A single folding operation gives us the Bickel-Hodges estimator.&lt;/p>
&lt;p>In this post, we briefly check how this metric behaves in the case of the Normal and Cauchy distributions.&lt;/p></description></item><item><title>Gastwirth's location estimator</title><link>https://aakinshin.net/posts/gastwirth/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gastwirth/</guid><description>&lt;p>Let $x = \{ x_1, x_2, \ldots, x_n \}$ be a random sample.
The Gastwirth&amp;rsquo;s location estimator is defined as follows:&lt;/p>
$$
0.3 \cdot Q_{⅓}(x) + 0.4 \cdot Q_{½}(x) + 0.3 \cdot Q_{⅔}(x),
$$
&lt;p>where $Q_p$ is an estimation of the $p^{\textrm{th}}$ quantile (using classic sample quantiles).&lt;/p>
&lt;p>This estimator could be quite interesting from a practical point of view.
On the one hand, it&amp;rsquo;s robust (the breakdown point ⅓)
and it has better statistical efficiency than the classic sample median.
On the other hand, it has better computational efficiency
than other robust and statistical efficient measures of location
like the Harrell-Davis median estimator or
the &lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency2/">Hodges-Lehmann median estimator&lt;/a>.&lt;/p>
&lt;p>In this post, we conduct a short simulation study that shows its behavior for the standard Normal distribution
and the Cauchy distribution.&lt;/p></description></item><item><title>Hodges-Lehmann-Sen shift and shift confidence interval estimators</title><link>https://aakinshin.net/posts/hodges-lehmann-sen-shift-ci/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-sen-shift-ci/</guid><description>&lt;p>In the previous two posts
(&lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency1/">1&lt;/a>, &lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency2/">2&lt;/a>),
I discussed the Hodges-Lehmann median estimator.
The suggested idea of getting median estimations based on a cartesian product
could be adopted to estimate the shift between two samples.
In this post, we discuss how to build Hodges-Lehmann-Sen shift estimator
and how to get confidence intervals for the obtained estimations.
Also, we perform a simulation study that checks the actual coverage percentage of these intervals.&lt;/p></description></item><item><title>Statistical efficiency of the Hodges-Lehmann median estimator, Part 2</title><link>https://aakinshin.net/posts/hodges-lehmann-efficiency2/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-efficiency2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency1/">previous post&lt;/a>,
we evaluated the relative statistical efficiency of the Hodges-Lehmann median estimator
against the sample median under the normal distribution.
In this post, we extended this experiment to a set of various light-tailed and heavy-tailed distributions.&lt;/p></description></item><item><title>Statistical efficiency of the Hodges-Lehmann median estimator, Part 1</title><link>https://aakinshin.net/posts/hodges-lehmann-efficiency1/</link><pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-efficiency1/</guid><description>&lt;p>In this post, we evaluate the relative statistical efficiency of the Hodges-Lehmann median estimator
against the sample median under the normal distribution.
We also compare it with the efficiency of the Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Unbiased median absolute deviation for n=2</title><link>https://aakinshin.net/posts/unbiased-mad-n2/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad-n2/</guid><description>&lt;p>I already covered the topic of the unbiased median deviation based on
&lt;a href="https://aakinshin.net/posts/unbiased-mad/">the traditional sample median&lt;/a>,
&lt;a href="https://aakinshin.net/posts/unbiased-mad-hd/">the Harrell-Davis quantile estimator&lt;/a>, and
&lt;a href="https://aakinshin.net/posts/unbiased-mad-thd/">the trimmed Harrell-Davis quantile estimator&lt;/a>.
In all the posts, the values of bias-correction factors were evaluated using the Monte-Carlo simulation.
In this post, we calculate the exact value of the bias-correction factor for two-element samples.&lt;/p></description></item><item><title>Weighted trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/wthdqe/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wthdqe/</guid><description>&lt;p>In this post, I combine ideas from two of my previous posts:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aakinshin.net/posts/pub-thdqe/">Trimmed Harrell-Davis quantile estimator&lt;/a>:
quantile estimator that provides an optimal trade-off between statistical efficiency and robustness&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/weighted-quantiles/">Weighted quantile estimators&lt;/a>:
a general scheme that allows building weighted quantile estimators.
Could be used for &lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">quantile exponential smoothing&lt;/a>
and &lt;a href="https://aakinshin.net/posts/dispersion-exponential-smoothing/">dispersion exponential smoothing&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Thus, we are going to build a weighted version of the trimmed Harrell-Davis quantile estimator based on the highest
density interval of the given width.&lt;/p></description></item><item><title>Minimum meaningful statistical level for the Mann–Whitney U test</title><link>https://aakinshin.net/posts/mann-whitney-min-stat-level/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mann-whitney-min-stat-level/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann–Whitney U test&lt;/a> is one of the most popular
nonparametric null hypothesis significance tests.
However, like any statistical test, it has limitations.
We should always carefully match them with our business requirements.
In this post, we discuss how to properly choose the statistical level for the Mann–Whitney U test on small samples.&lt;/p>
&lt;p>Let&amp;rsquo;s say we want to compare two samples $x = \{ x_1, x_2, \ldots, x_n \}$ and $y = \{ y_1, y_2, \ldots, y_m \}$
using the one-sided Mann–Whitney U test.
Sometimes, we don&amp;rsquo;t have an opportunity to gather enough data and we have to work with small samples.
Imagine that the size of both samples is six: $n=m=6$.
We want to set the statistical level $\alpha$ to $0.001$ (because we really don&amp;rsquo;t want to get false-positive results).
Is it a valid requirement?
In fact, the minimum p-value we can observe with $n=m=6$ is $\approx 0.001082$.
Thus, with $\alpha = 0.001$, it&amp;rsquo;s impossible to get a positive result.
Meanwhile, everything is correct from the technical point of view:
since we can&amp;rsquo;t get any positive results, the false positive rate is exactly zero which is less than $0.001$.
However, it&amp;rsquo;s definitely not something that we want: with this setup the test becomes useless because
it always provides negative results regardless of the input data.&lt;/p>
&lt;p>This brings an important question: what is the minimum meaningful statistical level
that we can require for the one-sided Mann–Whitney U test knowing the sample sizes?&lt;/p></description></item><item><title>Fence-based outlier detectors, Part 2</title><link>https://aakinshin.net/posts/fenced-outlier-detectors2/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/fenced-outlier-detectors2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/fenced-outlier-detectors1/">previous post&lt;/a>,
I discussed different fence-based outlier detectors.
In this post, I show some examples of these detectors with different parameters.&lt;/p></description></item><item><title>Fence-based outlier detectors, Part 1</title><link>https://aakinshin.net/posts/fenced-outlier-detectors1/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/fenced-outlier-detectors1/</guid><description>&lt;p>In previous posts, I discussed properties of &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">Tukey&amp;rsquo;s fences&lt;/a>
and asymmetric decile-based outlier detector
(&lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers1/">Part 1&lt;/a>, &lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers2/">Part 2&lt;/a>).
In this post, I discuss the generalization of fence-based outlier detectors.&lt;/p></description></item><item><title>Asymmetric decile-based outlier detector, Part 2</title><link>https://aakinshin.net/posts/asymmetric-decile-outliers2/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/asymmetric-decile-outliers2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers1/">previous post&lt;/a>,
I suggested an asymmetric decile-based outlier detector
as an alternative to &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">Tukey&amp;rsquo;s fences&lt;/a>.
In this post, we run some numerical simulations to check out
the suggested outlier detector in action.&lt;/p></description></item><item><title>Asymmetric decile-based outlier detector, Part 1</title><link>https://aakinshin.net/posts/asymmetric-decile-outliers1/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/asymmetric-decile-outliers1/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">previous post&lt;/a>, I covered some problems with the outlier detector
based on Tukey fences.
Mainly, I discussed the probability of observing outliers using Tukey&amp;rsquo;s fences
with different factors under different distributions.
However, it&amp;rsquo;s not the only problem with this approach.&lt;/p>
&lt;p>Since Tukey&amp;rsquo;s fences are based on quartiles,
under multimodal distributions, we could get a situation
when 50% of all sample elements are marked as outliers.
Also, Tukey&amp;rsquo;s fences are designed for symmetric distributions,
so we could get strange results with asymmetric distributions.&lt;/p>
&lt;p>In this post, I want to suggest an asymmetric outlier detector based on deciles
which mitigates this problem.&lt;/p></description></item><item><title>Probability of observing outliers using Tukey's fences</title><link>https://aakinshin.net/posts/tukey-outlier-probability/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/tukey-outlier-probability/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Outlier#Tukey's_fences">Tukey&amp;rsquo;s fences&lt;/a> is one of the most popular
simple outlier detectors for one-dimensional number arrays.
This approach assumes that for a given sample, we calculate first and third quartiles ($Q_1$ and $Q_3$),
and mark all the sample elements outside the interval&lt;/p>
$$
[Q_1 - k (Q_3 - Q_1),\, Q_3 + k (Q_3 - Q_1)]
$$
&lt;p>as outliers.
Typical recommendation for $k$ is $1.5$ for &amp;ldquo;regular&amp;rdquo; outliers and $3.0$ for &amp;ldquo;far outliers&amp;rdquo;.
Here is a box plot example for a sample taken from the standard normal distributions (sample size is $1000$):&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-light.png" target="_blank" alt="boxplot1">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-dark.png" target="_blank" alt="boxplot1">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>As we can see, 11 elements were marked as outliers (shown as dots).
Is it an expected result or not?
The answer depends on your goals.
There is no single definition of an outlier.
In fact, the chosen outlier detector provides a unique outlier definition.&lt;/p>
&lt;p>In my applications, I typically consider outliers as rare events that should be investigated.
When I detect too many outliers, all such reports become useless noise.
For example, on the above image, I wouldn&amp;rsquo;t treat any of the sample elements as outliers.
However, If we add $10.0$ to this sample, this element is an obvious outlier (which will be the only one):&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-light.png" target="_blank" alt="boxplot2">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-dark.png" target="_blank" alt="boxplot2">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Thus, an important property of an outlier detector is the &amp;ldquo;false positive rate&amp;rdquo;:
the percentage of samples with detected outliers which I wouldn&amp;rsquo;t treat as outliers.
In this post, I perform numerical simulations that show the probability of observing outliers
using Tukey&amp;rsquo;s fences with different $k$ values.&lt;/p></description></item><item><title>Gamma effect size powered by the middle non-zero quantile absolute deviation</title><link>https://aakinshin.net/posts/gamma-es-mnzqad/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gamma-es-mnzqad/</guid><description>&lt;p>In &lt;a href="https://aakinshin.net/tags/research-gamma-es/">previous posts&lt;/a>, I covered the concept of the gamma effect size.
It&amp;rsquo;s a nonparametric effect size which is consistent with Cohen&amp;rsquo;s d under the normal distribution.
However, the original definition has drawbacks: this statistic becomes zero
if half of the sample elements are equal to each other.
Last time, I &lt;a href="https://aakinshin.net/posts/zero-mad-gamma-es/">suggested&lt;/a>) a workaround for this problem:
we can replace the median absolute deviation by the &lt;a href="https://aakinshin.net/posts/qad/">quantile absolute deviation&lt;/a>.
Unfortunately, this trick requires parameter tuning:
we should choose a proper quantile position to make this approach work.
Today I want to suggest a strategy that provides a way to make a generic choice:
we can use the &lt;a href="https://aakinshin.net/posts/mnzqad/">middle non-zero quantile absolute deviation&lt;/a>.&lt;/p></description></item><item><title>Middle non-zero quantile absolute deviation</title><link>https://aakinshin.net/posts/mnzqad/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mnzqad/</guid><description>&lt;p>Median absolute deviation ($\operatorname{MAD}$) around the median is a popular robust measure of statistical dispersion.
Unfortunately, if we &lt;a href="https://aakinshin.net/posts/discrete-performance-distributions/">work&lt;/a> with discrete distributions,
we could get zero $\operatorname{MAD}$ values.
It could bring some problems if we &lt;a href="https://aakinshin.net/posts/zero-mad-gamma-es/">use&lt;/a> $\operatorname{MAD}$ as a denominator.
Such a problem is also relevant to some other quantile-based measures of dispersion
like interquartile range ($\operatorname{IQR}$).&lt;/p>
&lt;p>This problem could be solved using the &lt;a href="https://aakinshin.net/posts/qad/">quantile absolute deviation&lt;/a> around the median.
However, it&amp;rsquo;s not always clear how to choose the right quantile to estimate.
In this post, I&amp;rsquo;m going to suggest a choosing approach that is consistent with the classic $\operatorname{MAD}$
under continuous distributions (and samples without tied values).&lt;/p></description></item><item><title>Unbiased median absolute deviation based on the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/unbiased-mad-thd/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad-thd/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> ($\operatorname{MAD}$)
is a robust measure of scale.
For a sample $x = \{ x_1, x_2, \ldots, x_n \}$, it&amp;rsquo;s defined as follows:&lt;/p>
$$
\operatorname{MAD}_n = C_n \cdot \operatorname{median}(|x - \operatorname{median}(x)|)
$$
&lt;p>where $\operatorname{median}$ is a median estimator, $C_n$ is a scale factor.
Using the right scale factor, we can use $\operatorname{MAD}$ as a consistent estimator
for the estimation of the standard deviation under the normal distribution.
For huge samples, we can use the asymptotic value of $C_n$ which is&lt;/p>
$$
C_\infty = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
$$
&lt;p>For small samples, we should use adjusted values $C_n$ which depend on the sample size.
However, $C_n$ depends not only on the sample size but also on the median estimator.
I have already covered how to obtain this values for
&lt;a href="https://aakinshin.net/posts/unbiased-mad/">the traditional median estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/unbiased-mad-hd/">the Harrell-Davis median estimator&lt;/a>.
It&amp;rsquo;s time to get the $C_n$ values for
&lt;a href="https://aakinshin.net/posts/preprint-thdqe/">the trimmed Harrell-Davis median estimator&lt;/a>.&lt;/p></description></item><item><title>Median absolute deviation vs. Shamos estimator</title><link>https://aakinshin.net/posts/mad-vs-shamos/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mad-vs-shamos/</guid><description>&lt;p>There are multiple ways to estimate statistical dispersion.
The standard deviation is the most popular one, but it&amp;rsquo;s not robust:
a single outlier could heavily corrupt the results.
Fortunately, we have robust measures of dispersions like the &lt;em>median absolute deviation&lt;/em> and the &lt;em>Shamos estimator&lt;/em>.
In this post, we perform numerical simulations and
compare these two estimators on different distributions and sample sizes.&lt;/p></description></item><item><title>Moving extended P² quantile estimator</title><link>https://aakinshin.net/posts/moving-ex-p2-quantile-estimator/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/moving-ex-p2-quantile-estimator/</guid><description>&lt;p>In the previous posts, I discussed
&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">the P² quantile estimator&lt;/a>
(a sequential estimator which takes $O(1)$ memory and estimates a single predefined quantile),
&lt;a href="https://aakinshin.net/posts/mp2-quantile-estimator/">the moving P² quantile estimator&lt;/a>
(a moving modification of P² which estimates quantiles within the moving window),
and &lt;a href="https://aakinshin.net/posts/ex-p2-quantile-estimator/">the extended P² quantile estimator&lt;/a>
(a sequential estimator which takes $O(m)$ memory and estimates $m$ predefined quantiles).&lt;/p>
&lt;p>Now it&amp;rsquo;s time to build &lt;em>the moving modification of the extended P² quantile estimator&lt;/em>
which estimates $m$ predefined quantiles using $O(m)$ memory within the moving window.&lt;/p></description></item><item><title>Extended P² quantile estimator</title><link>https://aakinshin.net/posts/ex-p2-quantile-estimator/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/ex-p2-quantile-estimator/</guid><description>&lt;p>I already covered &lt;em>the P² quantile estimator&lt;/em> and its possible implementation improvements
in &lt;a href="https://aakinshin.net/tags/research-p2qe/">several blog posts&lt;/a>.
This sequential estimator uses $O(1)$ memory and allows estimating a single predefined quantile.
Now it&amp;rsquo;s time to discuss &lt;em>the extended P² quantile estimator&lt;/em> that allows estimating multiple predefined quantiles.
This extended version was suggested in the paper
&lt;a href="https://doi.org/10.1177/003754978704900405">&amp;ldquo;Simultaneous estimation of several percentiles&amp;rdquo;&lt;/a>.
In this post, we briefly discuss the approach from this paper and how we can improve its implementation.&lt;/p></description></item><item><title>P² quantile estimator marker adjusting order</title><link>https://aakinshin.net/posts/p2-quantile-estimator-adjusting-order/</link><pubDate>Tue, 11 Jan 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/p2-quantile-estimator-adjusting-order/</guid><description>&lt;p>I have already written a few blog posts about the P² quantile estimator
(which is a sequential estimator that uses $O(1)$ memory):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>P² Quantile Estimator&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator-rounding-issue/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>P² Quantile Estimator Rounding Issue&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator-initialization/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>P² Quantile Estimator Initialization Strategy&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In this post, we continue improving the P² implementation
so that it gives better estimations for streams with a small number of elements.&lt;/p></description></item><item><title>P² quantile estimator initialization strategy</title><link>https://aakinshin.net/posts/p2-quantile-estimator-initialization/</link><pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/p2-quantile-estimator-initialization/</guid><description>&lt;p>&lt;strong>Update: the estimator accuracy could be improved using a bunch of &lt;a href="https://aakinshin.net/tags/research-p2qe/">patches&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>The P² quantile estimator is a sequential estimator that uses $O(1)$ memory.
Thus, for the given sequence of numbers, it allows estimating quantiles without storing values.
I have already written a few blog posts about it:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>P² Quantile Estimator&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/p2-quantile-estimator-rounding-issue/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>P² Quantile Estimator Rounding Issue&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I tried this estimator in various contexts, and it shows pretty decent results.
However, recently I stumbled on a corner case:
if we want to estimate extreme quantile ($p &lt; 0.1$ or $p > 0.9$),
this estimator provides inaccurate results on small number streams ($n &lt; 10$).
While it looks like a minor issue, it would be nice to fix it.
In this post, we briefly discuss choosing a better initialization strategy to workaround this problem.&lt;/p></description></item><item><title>Ratio function vs. ratio distribution</title><link>https://aakinshin.net/posts/ratio-function-vs-distribution/</link><pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/ratio-function-vs-distribution/</guid><description>&lt;p>Let&amp;rsquo;s say we have two distributions $X$ and $Y$.
In the &lt;a href="https://aakinshin.net/posts/shift-function-vs-distribution/">previous post&lt;/a>,
we discussed how to express the &amp;ldquo;absolute difference&amp;rdquo; between them
using the shift function and the shift distribution.
Now let&amp;rsquo;s discuss how to express the &amp;ldquo;relative difference&amp;rdquo; between them.
This abstract term also could be expressed in various ways.
My favorite approach is to build the &lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/">ratio function&lt;/a>.
In order to do this, for each quantile $p$, we should calculate $Q_Y(p)/Q_X(p)$ where $Q$ is the quantile function.
However, some people prefer using the &lt;a href="https://en.wikipedia.org/wiki/Ratio_distribution">ratio distribution&lt;/a> $Y/X$.
While both approaches may provide similar results for narrow positive non-overlapping distributions,
they are not equivalent in the general case.
In this post, we briefly consider examples of both approaches.&lt;/p></description></item><item><title>Shift function vs. shift distribution</title><link>https://aakinshin.net/posts/shift-function-vs-distribution/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/shift-function-vs-distribution/</guid><description>&lt;p>Let&amp;rsquo;s say we have two distributions $X$ and $Y$,
and we want to express the &amp;ldquo;absolute difference&amp;rdquo; between them.
This abstract term could be expressed in various ways.
My favorite approach is to build the &lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/">Doksum&amp;rsquo;s shift function&lt;/a>.
In order to do this, for each quantile $p$, we should calculate $Q_Y(p)-Q_X(p)$ where $Q$ is the quantile function.
However, some people prefer using the shift distribution $Y-X$.
While both approaches may provide similar results for narrow non-overlapping distributions,
they are not equivalent in the general case.
In this post, we briefly consider examples of both approaches.&lt;/p></description></item><item><title>P² quantile estimator rounding issue</title><link>https://aakinshin.net/posts/p2-quantile-estimator-rounding-issue/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/p2-quantile-estimator-rounding-issue/</guid><description>&lt;p>&lt;strong>Update: the estimator accuracy could be improved using a bunch of &lt;a href="https://aakinshin.net/tags/research-p2qe/">patches&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>The P² quantile estimator is a sequential estimator that uses $O(1)$ memory.
Thus, for the given sequence of numbers, it allows estimating quantiles without storing values.
I already wrote &lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">a blog post&lt;/a> about this approach and
&lt;a href="https://github.com/AndreyAkinshin/perfolizer/commit/9e9ff80a4d097fe4c0814ca51c7fbe942763e308">added&lt;/a>
its implementation in &lt;a href="https://github.com/AndreyAkinshin/perfolizer">perfolizer&lt;/a>.
Recently, I got a &lt;a href="https://github.com/AndreyAkinshin/perfolizer/issues/8">bug report&lt;/a>
that revealed a flaw of the &lt;a href="https://doi.org/10.1145/4372.4378">original paper&lt;/a>.
In this post, I&amp;rsquo;m going to briefly discuss this issue and the corresponding fix.&lt;/p></description></item><item><title>Trimmed Harrell-Davis quantile estimator based on the highest density interval of the given width</title><link>https://aakinshin.net/posts/thdqe-hdi/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-hdi/</guid><description>&lt;p>Traditional quantile estimators that are based on one or two order statistics are a common way to estimate
distribution quantiles based on the given samples.
These estimators are robust, but their statistical efficiency is not always good enough.
A more efficient alternative is the Harrell-Davis quantile estimator which uses
a weighted sum of all order statistics.
Whereas this approach provides more accurate estimations for the light-tailed distributions, it&amp;rsquo;s not robust.
To be able to customize the trade-off between statistical efficiency and robustness,
we could consider &lt;em>a trimmed modification of the Harrell-Davis quantile estimator&lt;/em>.
In this approach, we discard order statistics with low weights according to
the highest density interval of the beta distribution.&lt;/p></description></item><item><title>Optimal window of the trimmed Harrell-Davis quantile estimator, Part 2: Trying Planck-taper window</title><link>https://aakinshin.net/posts/thdqe-ow2/</link><pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-ow2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/thdqe-ow1/">previous post&lt;/a>,
I discussed the problem of non-smooth quantile-respectful density estimation (QRDE)
which is generated by the trimmed Harrell-Davis quantile estimator
based on the highest density interval of the given width.
I assumed that non-smoothness was caused by a non-smooth rectangular window
which was used to build the truncated beta distribution.
In this post, we are going to try another option: the Planck-taper window.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-ow2/img/qrde-light.png" target="_blank" alt="qrde">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-ow2/img/qrde-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-ow2/img/qrde-dark.png" target="_blank" alt="qrde">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-ow2/img/qrde-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Optimal window of the trimmed Harrell-Davis quantile estimator, Part 1: Problems with the rectangular window</title><link>https://aakinshin.net/posts/thdqe-ow1/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-ow1/</guid><description>&lt;p>In the previous post, we have obtained a nice version of the trimmed Harrell-Davis quantile estimator
which provides an opportunity to get a nice trade-off between robustness and statistical efficiency
of quantile estimations.
Unfortunately, it has a severe drawback.
If we build a &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimation&lt;/a> based on the suggested estimator,
we won&amp;rsquo;t get a smooth density function as in the case of the classic Harrell-Davis quantile estimator:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-ow1/img/qrde-light.png" target="_blank" alt="qrde">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-ow1/img/qrde-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-ow1/img/qrde-dark.png" target="_blank" alt="qrde">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-ow1/img/qrde-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>In this blog post series, we are going to find a way to improve the trimmed Harrell-Davis quantile estimator
so that it gives a smooth density function and keeps its advantages in terms of robustness and statistical efficiency.&lt;/p></description></item><item><title>Beta distribution highest density interval of the given width</title><link>https://aakinshin.net/posts/beta-hdi/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/beta-hdi/</guid><description>&lt;p>In one of &lt;a href="https://aakinshin.net/posts/kosqe5/">the previous posts&lt;/a>, I discussed the idea of the trimmed Harrell-Davis quantile estimator
based on the highest density interval of the given width.
Since the Harrell-Davis quantile estimator uses the Beta distribution,
we should be able to find the beta distribution highest density interval of the given width.
In this post, I will show how to do this.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 8: Winsorized Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe8/</link><pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe8/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe7/">previous post&lt;/a>, we have discussed
the trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of size $\sqrt{n}/n$.
This quantile estimator showed a decent level of statistical efficiency.
However, the research wouldn&amp;rsquo;t be complete without comparison with the winsorized modification.
Let&amp;rsquo;s fix it!&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 7: Optimal threshold for the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe7/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe7/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe6/">previous post&lt;/a>, we have obtained a nice quantile estimator.
To be specific, we considered a trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of the given size.
The interval size is a parameter that controls the trade-off between statistical efficiency and robustness.
While it&amp;rsquo;s nice to have the ability to control this trade-off, there is also a need for the default value,
which could be used as a starting point
when we have neither estimator breakdown point requirements nor prior knowledge about distribution properties.&lt;/p>
&lt;p>After a series of unsuccessful attempts, it seems that I have found an acceptable solution.
We should build the new estimator based on $\sqrt{n}/n$ order statistics.
In this post, I&amp;rsquo;m going to briefly explain the idea behind the suggested estimator and
share some numerical simulations that compare the proposed estimator
and the classic Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 6: Continuous trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe6/</link><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe6/</guid><description>&lt;p>In my &lt;a href="https://aakinshin.net/posts/kosqe5/">previous post&lt;/a>,
I tried the idea of using the trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of the given width.
The width was defined so that it covers exactly k order statistics (the width equals $(k-1)/n$).
I was pretty satisfied with the result and decided to continue evolving this approach.
While &amp;ldquo;k order statistics&amp;rdquo; is a good mental model that described the trimmed interval,
it doesn&amp;rsquo;t actually require an integer k.
In fact, we can use any real number as the trimming percentage.&lt;/p>
&lt;p>In this post, we are going to perform numerical simulations that check the statistical efficiency
of the trimmed Harrell-Davis quantile estimator with different trimming percentages.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 5: Improving trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe5/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe5/</guid><description>&lt;p>During the last several months,
I have been experimenting with different variations of the trimmed Harrell-Davis quantile estimator.
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">My original idea&lt;/a>
of using the highest density interval based on the fixed area percentage (e.g., HDI 95% or HDI 99%)
led to a set of problems with &lt;a href="https://aakinshin.net/posts/thdqe-overtrimming/">overtrimming&lt;/a>.
I tried to solve them with &lt;a href="https://aakinshin.net/posts/customized-wthdqe/">manually customized&lt;/a> trimming strategy,
but this approach turned out to be too inconvenient;
it was too hard to come up with &lt;a href="https://aakinshin.net/posts/thdqe-threshold/">optimal thresholds&lt;/a>.
One of the main problems was about the suboptimal number of elements
that we actually aggregate to obtain the quantile estimation.
So, I decided to try an &lt;a href="https://aakinshin.net/posts/kosqe1/">approach that involves exactly k order statistics&lt;/a>.
The idea was so promising,
but numerical simulations &lt;a href="https://aakinshin.net/posts/kosqe4/">haven&amp;rsquo;t shown&lt;/a> the appropriate efficiency level.&lt;/p>
&lt;p>This bothered me the whole week.
It sounded so reasonable to trim the Harrell-Davis quantile estimator using exactly k order statistics.
Why didn&amp;rsquo;t this work as expected?
Finally, I have found a fatal flaw in &lt;a href="https://aakinshin.net/posts/kosqe4/">my previous approach&lt;/a>:
while it was a good idea to fix the size of the trimming window,
I mistakenly chose its location following the equation from the Hyndman-Fan Type 7 quantile estimator!&lt;/p>
&lt;p>In this post, we fix this problem and try another modification of the trimmed Harrell-Davis quantile estimator based on
k order statistics &lt;strong>and&lt;/strong> highest density intervals at the same time.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 4: Adopting trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe4/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe4/</guid><description>&lt;p>In the previous posts, I discussed various aspects of
&lt;a href="https://aakinshin.net/posts/kosqe1/">quantile estimators based on k order statistics&lt;/a>.
I already tried a few weight functions that aggregate the sample values to the quantile estimators
(see posts about &lt;a href="https://aakinshin.net/posts/kosqe2/">an extension of the Hyndman-Fan Type 7 equation&lt;/a> and
about &lt;a href="https://aakinshin.net/posts/kosqe3/">adjusted regularized incomplete beta function&lt;/a>).
In this post, I continue my experiments and try to adopt the
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed modifications of the Harrell-Davis quantile estimator&lt;/a> to this approach.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 3: Playing with the Beta function</title><link>https://aakinshin.net/posts/kosqe3/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe3/</guid><description>&lt;p>In the previous two posts, I discussed the idea of quantile estimators based on k order statistics.
A already covered the &lt;a href="https://aakinshin.net/posts/kosqe1/">motivation behind this idea&lt;/a>
and the statistical efficiency of such estimators using the &lt;a href="https://aakinshin.net/posts/kosqe2/">extended Hyndman-Fan equations&lt;/a>
as a weight function.
Now it&amp;rsquo;s time to experiment with the Beta function as a primary way to aggregate k order statistics
into a single quantile estimation!&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 2: Extending Hyndman-Fan equations</title><link>https://aakinshin.net/posts/kosqe2/</link><pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe1/">previous post&lt;/a>,
I described the idea of using quantile estimators based on k order statistics.
Potentially, such estimators could be more robust than estimators based on all samples elements (like
Harrell-Davis,
&lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis&lt;/a>, or
&lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir&lt;/a>)
and more statistically efficient than traditional quantile estimators (based on 1 or 2 order statistics).
Moreover, we should be able to control this trade-off based on the business requirements
(e.g., setting the desired breakdown point).&lt;/p>
&lt;p>The only challenging thing here is choosing the weight function
that aggregates k order statistics to a single quantile estimation.
We are going to try several options, perform Monte-Carlo simulations for each of them, and compare the results.
A reasonable starting point is an extension of the traditional quantile estimators.
In this post, we are going to extend the Hyndman-Fan Type 7 quantile estimator
(nowadays, it&amp;rsquo;s one of the most popular estimators).
It estimates quantiles as a linear interpolation of two subsequent order statistics.
We are going to make some modifications, so a new version is going to be based on k order statistics.&lt;/p>
&lt;p>&lt;strong>Spoiler: this approach doesn&amp;rsquo;t seem like an optimal one.&lt;/strong>
I&amp;rsquo;m pretty disappointed with its statistical efficiency on samples from light-tailed distributions.
So, what&amp;rsquo;s the point of writing a blog post about an inefficient approach?
Because of the following reasons:&lt;/p>
&lt;ol>
&lt;li>I believe it&amp;rsquo;s crucial to share negative results.
Sometimes, knowledge about approaches that don&amp;rsquo;t work
could be more important than knowledge about more effective techniques.
Negative results give you a broader view of the problem
and protect you from wasting your time on potential promising (but not so useful) ideas.&lt;/li>
&lt;li>Negative results improve research completeness.
When we present an approach, it&amp;rsquo;s essential to not only show why it solves problems well,
but also why it solves problems better than other similar approaches.&lt;/li>
&lt;li>While I wouldn&amp;rsquo;t recommend my extension of the Hyndman-Fan Type 7 quantile estimator to the k order statistics case
as the default quantile estimator, there are some specific cases where it could be useful.
For example, if we estimate the median based on small samples from a symmetric light-tailed distribution,
it could outperform not only the original version but also the Harrell-Davis quantile estimator.
The &amp;ldquo;negativity&amp;rdquo; of the negative results always exists in a specific context.
So, there may be cases when negative results for the general case transform to positive results
for a particular niche problem.&lt;/li>
&lt;li>Finally, it&amp;rsquo;s my personal blog, so I have the freedom to write on any topic I like.
My blog posts are not publications to scientific journals (which typically don&amp;rsquo;t welcome negative results),
but rather research notes about conducted experiments.
It&amp;rsquo;s important for me to keep records of all the experiments I perform regardless of the usefulness of the results.&lt;/li>
&lt;/ol>
&lt;p>So, let&amp;rsquo;s briefly look at the results of this not-so-useful approach.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-light.png" target="_blank" alt="LightAndHeavy__N15_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-dark.png" target="_blank" alt="LightAndHeavy__N15_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Quantile estimators based on k order statistics, Part 1: Motivation</title><link>https://aakinshin.net/posts/kosqe1/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe1/</guid><description>&lt;p>It&amp;rsquo;s not easy to choose a good quantile estimator.
In my previous posts, I considered several groups of quantile estimators:&lt;/p>
&lt;ul>
&lt;li>Quantile estimators based 1 or 2 order statistics (Hyndman-Fan Type1-9)&lt;/li>
&lt;li>Quantile estimators based on all order statistics
(the Harrell-Davis quantile estimator,
the &lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis quantile estimator&lt;/a>, and
the &lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir quantile estimator&lt;/a>)&lt;/li>
&lt;li>Quantile estimators based on a variable number of order statistics
(the &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a> and &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> modifications
of the Harrell-Davis quantile estimator)&lt;/li>
&lt;/ul>
&lt;p>Unfortunately, all of these estimators have significant drawbacks
(e.g., poor statistical efficiency or poor robustness).
In this post, I want to discuss all of the advantages and disadvantages of each approach
and suggest another family of quantile estimators that are based on k order statistics.&lt;/p></description></item><item><title>Avoiding over-trimming with the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-overtrimming/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-overtrimming/</guid><description>&lt;p>Previously, I already discussed the
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed modification of the Harrell-Davis quantile estimator&lt;/a> several times.
I performed several numerical simulations that compare the statistical efficiency of this estimator
with the efficiency of the &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">classic Harrell-Davis quantile estimator&lt;/a> (HDQE)
and its &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized modification&lt;/a>;
I showed how we can improve the efficiency using &lt;a href="https://aakinshin.net/posts/customized-wthdqe/">custom trimming strategies&lt;/a>
and how to choose a &lt;a href="https://aakinshin.net/posts/thdqe-threshold/">good trimming threshold value&lt;/a>.&lt;/p>
&lt;p>In the heavy-tailed cases, the trimmed HDQE provides better estimations than the classic HDQE
because of its higher breakdown point.
However, in the light-tailed cases, we could get efficiency that is worse than
the baseline Hyndman-Fan Type 7 (HF7) quantile estimator.
In many cases, such an effect arises because of the over-trimming effect.
If the trimming percentage is too high or if the evaluated quantile is too far from the median,
the trimming strategy based on the highest-density interval may lead to an estimation
that is based on single order statistics.
In this case, we get an efficiency level similar to the Hyndman-Fan Type 1-3 quantile estimators
(which are also based on single order statistics).
In the light-tailed case, such a result is less preferable than Hyndman-Fan Type 4-9 quantile estimators
(which are based on two subsequent order statistics).&lt;/p>
&lt;p>In order to improve the situation, we could introduce the lower bound for the number of order statistics
that contribute to the final quantile estimations.
In this post, I look at some numerical simulations
that compare trimmed HDQEs with different lower bounds.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png" target="_blank" alt="LightAndHeavy__N05_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-dark.png" target="_blank" alt="LightAndHeavy__N05_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Optimal threshold of the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-threshold/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-threshold/</guid><description>&lt;p>The traditional quantile estimators (which are based on 1 or 2 order statistics) have great robustness.
However, the statistical efficiency of these estimators is not so great.
The Harrell-Davis quantile estimator has much better efficiency (at least in the light-tailed case),
but it&amp;rsquo;s not robust (because it calculates a weighted sum of all sample values).
I already wrote a &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">post about trimmed Harrell-Davis quantile estimator&lt;/a>:
this approach suggest dropping some of the low-weight sample values to improve robustness
(keeping good statistical efficiency).
I also perform a numerical simulations that &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">compare efficiency&lt;/a>
of the original Harrell-Davis quantile estimator against its trimmed and winsorized modifications.
It&amp;rsquo;s time to discuss how to choose the optimal trimming threshold
and how it affects the estimator efficiency.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png" target="_blank" alt="LightAndHeavy__N40_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-dark.png" target="_blank" alt="LightAndHeavy__N40_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Estimating quantile confidence intervals: Maritz-Jarrett vs. jackknife</title><link>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</guid><description>&lt;p>When it comes to estimating quantiles of the given sample,
my estimator of choice is the Harrell-Davis quantile estimator
(to be more specific, its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed version&lt;/a>).
If I need to get a confidence interval for the obtained quantiles,
I use the &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/#the-maritz-jarrett-method">Maritz-Jarrett method&lt;/a>
because it provides a &lt;a href="https://aakinshin.net/posts/quantile-ci-coverage/">decent coverage percentage&lt;/a>.
Both approaches work pretty nicely together.&lt;/p>
&lt;p>However, in the original paper by &lt;a href="https://doi.org/10.2307/2335999">Harrell and Davis (1982)&lt;/a>,
the authors suggest using the jackknife variance estimator in order to get the confidence intervals.
The obvious question here is which approach better: the Maritz-Jarrett method or the jackknife estimator?
In this post, I perform a numerical simulation that compares both techniques using different distributions.&lt;/p></description></item><item><title>Using Kish's effective sample size with weighted quantiles</title><link>https://aakinshin.net/posts/kish-ess-weighted-quantiles/</link><pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kish-ess-weighted-quantiles/</guid><description>&lt;p>In my previous posts, I described how to calculate
&lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted quantiles&lt;/a> and
their &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">confidence intervals&lt;/a>
using the Harrell-Davis quantile estimator.
This powerful technique allows applying
&lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">quantile exponential smoothing&lt;/a> and
&lt;a href="https://aakinshin.net/posts/dispersion-exponential-smoothing/">dispersion exponential smoothing&lt;/a> for
time series in order to get its moving properties.&lt;/p>
&lt;p>When we work with weighted samples, we need a way to calculate the
&lt;a href="https://en.wikipedia.org/wiki/Effective_sample_size">effective samples size&lt;/a>.
Previously, I used the sum of all weights normalized by the maximum weight.
In most cases, it worked OK.&lt;/p>
&lt;p>Recently, &lt;a href="https://www.soz.unibe.ch/about_us/people/prof_dr_jann_ben/index_eng.html">Ben Jann&lt;/a> pointed out
that it would be better to use the Kish&amp;rsquo;s formula to calculate the effective sample size.
In this post, you find the formula and a few numerical simulations that illustrate the actual impact of
the underlying sample size formula.&lt;/p></description></item><item><title>Partial binning compression of performance series</title><link>https://aakinshin.net/posts/partial-binning-compression/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partial-binning-compression/</guid><description>&lt;p>Let&amp;rsquo;s start with a problem from real life.
Imagine we have thousands of application components that should be initialized.
We care about the total initialization time of the whole application,
so we want to automatically track the slowest components using a continuous integration (CI) system.
The easiest way to do it is to measure the initialization time of each component in each CI build
and save all the measurements to a database.
Unfortunately, if the total number of components is huge, the overall artifact size may be quite extensive.
Thus, this approach may introduce an unwanted negative impact on the database size and data processing time.&lt;/p>
&lt;p>However, we don&amp;rsquo;t actually need all the measurements.
We want to track only the slowest components.
Typically, it&amp;rsquo;s possible to introduce a reasonable threshold that defines such components.
For example, we can say that all components that are initialized in less than 1ms are &amp;ldquo;fast enough,&amp;rdquo;
so there is no need to know the exact initialization time for them.
Since these time values are insignificant, we can just omit all the measurements below the given thresholds.
This allows to significantly reduce the data traffic without losing any important information.&lt;/p>
&lt;p>The suggested trick can be named &lt;em>partial binning compression&lt;/em>.
Indeed, we introduce a single bin (perform &lt;em>binning&lt;/em>) and
omit all the values inside this bin (perform &lt;em>compression&lt;/em>).
On the other hand, we don&amp;rsquo;t build an honest histogram since we keep all the raw values outside the given bin
(the binning is &lt;em>partial&lt;/em>).&lt;/p>
&lt;p>Let&amp;rsquo;s discuss a few aspects of using partial binning compression.&lt;/p></description></item><item><title>Calculating gamma effect size for samples with zero median absolute deviation</title><link>https://aakinshin.net/posts/zero-mad-gamma-es/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/zero-mad-gamma-es/</guid><description>&lt;p>In previous posts, I discussed the &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">gamma effect size&lt;/a>
which is a Cohen&amp;rsquo;s d-consistent nonparametric and robust measure of the effect size.
Also, I discussed &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size2/">various ways to customize this metric&lt;/a>
and adjust it to different kinds of business requirements.
In this post, I want to briefly cover one more corner case that requires special adjustments.
We are going to discuss the situation when the median absolute deviation is zero.&lt;/p></description></item><item><title>Discrete performance distributions</title><link>https://aakinshin.net/posts/discrete-performance-distributions/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/discrete-performance-distributions/</guid><description>&lt;p>When we collect software performance measurements, we get a bunch of time intervals.
Typically, we tend to interpret time values as continuous values.
However, the obtained values are actually discrete due to the limited resolution of our measurement tool.
In simple cases, we can treat these discrete values as continuous and get meaningful results.
Unfortunately, discretization may produce strange phenomena like pseudo-multimodality or zero dispersion.
If we want to set up a reliable system that automatically analyzes such distributions,
we should be aware of such problems so we could correctly handle them.&lt;/p>
&lt;p>In this post, I want to share a few of discretization problems in real-life performance data sets
(based on the &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> performance tests).&lt;/p></description></item><item><title>Customization of the nonparametric Cohen's d-consistent effect size</title><link>https://aakinshin.net/posts/nonparametric-effect-size2/</link><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nonparametric-effect-size2/</guid><description>&lt;p>One year ago, I publish a post called &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#note">&lt;/use>&lt;/svg>Nonparametric Cohen&amp;#39;s D-Consistent Effect Size&lt;/a>.
During this year, I got a lot of internal and external feedback from
my own statistical experiments and
&lt;a href="https://twitter.com/ViljamiSairanen/status/1400457118340108293">people&lt;/a>
&lt;a href="https://sherbold.github.io/autorank/autorank/">who&lt;/a>
&lt;a href="https://github.com/Ramon-Diaz/Thesis-Project/blob/85df6b11050c7e05c4394d873585f701a7e3f32e/_util.py#L100">tried&lt;/a>
to use the suggested approach.
It seems that the nonparametric version of Cohen&amp;rsquo;s d works much better with real-life not-so-normal data.
While the classic Cohen&amp;rsquo;s d based on
the non-robust arithmetic mean and
the &lt;a href="https://aakinshin.net/posts/misleading-stddev/">non-robust standard deviation&lt;/a>
can be easily &lt;a href="https://aakinshin.net/posts/cohend-and-outliers/">corrupted by a single outlier&lt;/a>,
my approach is much more resistant to unexpected extreme values.
Also, it allows exploring
&lt;a href="https://aakinshin.net/posts/comparing-distributions-using-gamma-es/">the difference between specific quantiles of considered samples&lt;/a>,
which can be useful in the non-parametric case.&lt;/p>
&lt;p>However, I wasn&amp;rsquo;t satisfied with the results of all of my experiments.
While I still like the basic idea
(replace the mean with the median; replace the standard deviation with the median absolute deviation),
it turned out that the final results heavily depend on the used quantile estimator.
To be more specific, the original Harrell-Davis quantile estimator is not always optimal;
in most cases, it&amp;rsquo;s better to replace it with its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a> modification.
However, the particular choice of the quantile estimators depends on the situation.
Also, the consistency constant for the median absolute deviation
should be adjusted according to the current sample size and the used quantile estimator.
Of course, it also can be replaced by other dispersion estimators
that can be used as consistent estimators of the standard deviation.&lt;/p>
&lt;p>In this post, I want to get a brief overview of possible customizations of the suggested metrics.&lt;/p></description></item><item><title>Robust alternative to statistical efficiency</title><link>https://aakinshin.net/posts/robust-statistical-efficiency/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/robust-statistical-efficiency/</guid><description>&lt;p>Statistical efficiency is a common measure of the quality of an estimator.
Typically, it&amp;rsquo;s expressed via the mean square error ($\operatorname{MSE}$).
For the given estimator $T$ and the true parameter value $\theta$,
the $\operatorname{MSE}$ can be expressed as follows:&lt;/p>
$$
\operatorname{MSE}(T) = \operatorname{E}[(T-\theta)^2]
$$
&lt;p>In numerical simulations, the $\operatorname{MSE}$ can&amp;rsquo;t be used as a robust metric
because its breakdown point is zero
(a corruption of a single measurement leads to a corrupted result).
Typically, it&amp;rsquo;s not a problem for light-tailed distributions.
Unfortunately, in the heavy-tailed case,
the $\operatorname{MSE}$ becomes an unreliable and unreproducible metric
because it can be easily spoiled by a single outlier.&lt;/p>
&lt;p>I suggest an alternative way to compare statistical estimators.
Instead of using non-robust $\operatorname{MSE}$,
we can use robust quantile estimations of the absolute error distribution.
In this post, I want to share numerical simulations
that show a problem of irreproducible $\operatorname{MSE}$ values
and how they can be replaced by reproducible quantile values.&lt;/p></description></item><item><title>Improving the efficiency of the Harrell-Davis quantile estimator for special cases using custom winsorizing and trimming strategies</title><link>https://aakinshin.net/posts/customized-wthdqe/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/customized-wthdqe/</guid><description>&lt;p>Let&amp;rsquo;s say we want to
&lt;strong>estimate the median&lt;/strong>
based on a &lt;strong>small sample&lt;/strong> (3 $\leq n \leq 7$)
from a &lt;strong>right-skewed heavy-tailed distribution&lt;/strong>
with &lt;strong>high statistical efficiency&lt;/strong>.&lt;/p>
&lt;p>The traditional median estimator is the most robust estimator, but it&amp;rsquo;s not the most efficient one.
Typically, the Harrell-Davis quantile estimator provides better efficiency,
but it&amp;rsquo;s not robust (its breakdown point is zero),
so it may have worse efficiency in the given case.
The &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>
modifications of the Harrell-Davis quantile estimator provide a good trade-off
between efficiency and robustness, but they require a proper winsorizing/trimming rule.
A reasonable choice of such a rule for medium-size samples is based on the highest density interval of the Beta function
(as described &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">here&lt;/a>).
Unfortunately, this approach may be suboptimal for small samples.
E.g., if we use the 99% highest density interval to estimate the median,
it starts to trim sample values only for $n \geq 8$.&lt;/p>
&lt;p>In this post, we are going to discuss custom winsorizing/trimming strategies for special cases of the quantile estimation problem.&lt;/p></description></item><item><title>Comparing the efficiency of the Harrell-Davis, Sfakianakis-Verginis, and Navruz-Özdemir quantile estimators</title><link>https://aakinshin.net/posts/hd-sv-no-efficiency/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sv-no-efficiency/</guid><description>&lt;p>In the previous posts, I discussed the statistical efficiency of different quantile estimators
(&lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">Efficiency of the Harrell-Davis quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators&lt;/a>).&lt;/p>
&lt;p>In this post, I continue this research and compare the efficiency of
the Harrell-Davis quantile estimator,
the &lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis quantile estimators&lt;/a>, and
the &lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir quantile estimator&lt;/a>.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Dispersion exponential smoothing</title><link>https://aakinshin.net/posts/dispersion-exponential-smoothing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dispersion-exponential-smoothing/</guid><description>&lt;p>In this &lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">previous post&lt;/a>,
I showed how to apply exponential smoothing to quantiles
using the &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted Harrell-Davis quantile estimator&lt;/a>.
This technique allows getting smooth and stable moving median estimations.
In this post, I&amp;rsquo;m going to discuss how to use the same approach
to estimate moving dispersion.&lt;/p></description></item><item><title>Quantile exponential smoothing</title><link>https://aakinshin.net/posts/quantile-exponential-smoothing/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-exponential-smoothing/</guid><description>&lt;p>One of the popular problems in time series analysis is estimating the moving &amp;ldquo;average&amp;rdquo; value.
Let&amp;rsquo;s define the &amp;ldquo;average&amp;rdquo; as a central tendency metric like the mean or the median.
When we talk about the moving value, we assume that we are interested in
the average value &amp;ldquo;at the end&amp;rdquo; of the time series
instead of the average of all available observations.&lt;/p>
&lt;p>One of the most straightforward approaches to estimate the moving average is the &lt;em>simple moving mean&lt;/em>.
Unfortunately, this approach is not robust: outliers can instantly spoil the evaluated mean value.
As an alternative, we can consider &lt;em>simple moving median&lt;/em>.
I already discussed a few of such methods:
&lt;a href="https://aakinshin.net/posts/mp2-quantile-estimator/">the MP² quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/">a moving quantile estimator based on partitioning heaps&lt;/a>
(a modification of the Hardle-Steiger method).
When we talk about &lt;em>simple moving averages&lt;/em>, we typically assume
that we estimate the average value over the last $k$ observations ($k$ is the &lt;em>window size&lt;/em>).
This approach is also known as &lt;em>unweighted moving averages&lt;/em> because
all target observations have the same weight.&lt;/p>
&lt;p>As an alternative to the simple moving average, we can also consider the &lt;em>weighted moving average&lt;/em>.
In this case, we assign a weight for each observation and aggregate the whole time series according to these weights.
A famous example of such a weight function is &lt;em>exponential smoothing&lt;/em>.
And the simplest form of exponential smoothing is the &lt;em>exponentially weighted moving mean&lt;/em>.
This approach estimates the weighted moving mean using exponentially decreasing weights.
Switching from the simple moving mean to the exponentially weighted moving mean provides some benefits
in terms of smoothness and estimation efficiency.&lt;/p>
&lt;p>Although exponential smoothing has advantages over the simple moving mean,
it still estimates the mean value which is not robust.
We can improve the robustness of this approach if we reuse the same idea for weighted moving quantiles.
It&amp;rsquo;s possible because the quantiles also can be estimated for weighted samples.
In one of my previous posts, I &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">showed&lt;/a> how to adapt
the Hyndman-Fan Type 7 and Harrell-Davis quantile estimators to the weighted samples.
In this post, I&amp;rsquo;m going to show how we can use this technique to estimate
the weighted moving quantiles using exponentially decreasing weights.&lt;/p></description></item><item><title>Improving quantile-respectful density estimation for discrete distributions using jittering</title><link>https://aakinshin.net/posts/qrde-discrete/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qrde-discrete/</guid><description>&lt;p>In my previous posts, I already discussed the &lt;a href="https://aakinshin.net/posts/kde-discrete/">problem&lt;/a> that arise
when we try to build the kernel density estimation (KDE) for samples with ties.
We may get such samples in real life from discrete or mixed discrete/continuous distributions.
Even if the original distribution is continuous,
we may observe artificial sample discretization due to a limited resolution of the measuring tool.
Such discretization may lead to inaccurate density plots due to undersmoothing.
The problem can be resolved using a nice technique called &lt;em>jittering&lt;/em>.
I also discussed &lt;a href="https://aakinshin.net/posts/discrete-sample-jittering/">how to apply&lt;/a> jittering to get a smoother version of KDE.&lt;/p>
&lt;p>However, I&amp;rsquo;m not a huge fan of KDE because of two reasons.
The first one is the &lt;a href="https://aakinshin.net/posts/kde-bw/">problem of choosing a proper bandwidth value&lt;/a>.
With poorly chosen bandwidth, we can easily get oversmoothing or undersmoothing even without the discretization problem.
The second one is an inconsistency between the KDE-based probability density function and evaluated sample quantiles.
It could lead to inconsistent visualizations (e.g., KDE-based violin plots with non-KDE-based quantile values)
or it could introduce problems for algorithms that require density function and quantile values at the same time.
The inconsistency could be resolved using &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimation&lt;/a> (QRDE).
This kind of estimation builds the density function which matches the evaluated sample quantiles.
To get a smooth QRDE, we also need a smooth quantile estimator like the Harrell-Davis quantile estimator.
The robustness and componential efficiency of this approach can be improved using
the &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>
modifications of the Harrell-Davis quantile estimator
(which also have a &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">decent statistical efficiency level&lt;/a>).&lt;/p>
&lt;p>Unfortunately, the straightforward QRDE calculation is not always applicable for samples with ties
because it&amp;rsquo;s impossible to build an &amp;ldquo;honest&amp;rdquo; density function for discrete distributions
without using the Dirac delta function.
This is a severe problem for QRDE-based algorithms like the
&lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/">lowland multimodality detection algorithm&lt;/a>.
In this post, I will show how jittering could help to solve this problem and get a smooth QRDE on samples with ties.&lt;/p></description></item><item><title>How to build a smooth density estimation for a discrete sample using jittering</title><link>https://aakinshin.net/posts/discrete-sample-jittering/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/discrete-sample-jittering/</guid><description>&lt;p>Let&amp;rsquo;s say you have a sample with tied values.
If you draw a kernel density estimation (KDE) for such a sample,
you may get a serrated pattern like this:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-light.png" target="_blank" alt="intro">
 &lt;img
 src="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-dark.png" target="_blank" alt="intro">
 &lt;img
 src="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>KDE requires samples from continuous distributions
while tied values arise in discrete or mixture distributions.
Even if the original distribution is continuous,
you may observe artificial sample discretization due to a limited resolution of the measuring tool.
This effect may lead to distorted density plots like in the above picture.&lt;/p>
&lt;p>The problem could be solved using a nice technique called &lt;em>jittering&lt;/em>.
In the simplest case, jittering just adds random noise to each measurement.
Such a trick removes all ties from the sample and allows building a smooth density estimation.&lt;/p>
&lt;p>However, there are many different ways to apply jittering.
The trickiest question here is how to choose proper noise values.
In this post, I want to share one of my favorite jittering approaches.
It generates a non-randomized noise pattern with a low risk of noticeable sample corruption.&lt;/p></description></item><item><title>Kernel density estimation and discrete values</title><link>https://aakinshin.net/posts/kde-discrete/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kde-discrete/</guid><description>&lt;p>Kernel density estimation (KDE) is a popular technique of data visualization.
Based on the given sample, it allows estimating the probability density function (PDF) of the underlying distribution.
Here is an example of KDE for &lt;code>x = {3.82, 4.61, 4.89, 4.91, 5.31, 5.6, 5.66, 7.00, 7.00, 7.00}&lt;/code>
(normal kernel, Sheather &amp;amp; Jones bandwidth selector):&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/kde-discrete/img/intro-light.png" target="_blank" alt="intro">
 &lt;img
 src="https://aakinshin.net/posts/kde-discrete/img/intro-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/kde-discrete/img/intro-dark.png" target="_blank" alt="intro">
 &lt;img
 src="https://aakinshin.net/posts/kde-discrete/img/intro-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>KDE is a simple and straightforward way to build a PDF, but it&amp;rsquo;s not always the best one.
In addition to my &lt;a href="https://aakinshin.net/posts/kde-bw/">concerns about bandwidth selection&lt;/a>,
continuous use of KDE creates an illusion that all distributions are smooth and continuous.
In practice, it&amp;rsquo;s not always true.&lt;/p>
&lt;p>In the above picture, the distribution looks pretty continuous.
However, the picture hides the fact that we have three &lt;code>7.00&lt;/code> elements in the original sample.
With continuous distributions, the probability of getting tied observations (that have the same value) is almost zero.
If a sample contains ties, we are most likely working with
either a discrete distribution or a mixture of discrete and continuous distributions.
A KDE for such a sample may significantly differ from the actual PDF.
Thus, this technique may mislead us instead of providing insights about the true underlying distribution.&lt;/p>
&lt;p>In this post, we discuss the usage of PDF and PMF with continuous and discrete distributions.
Also, we look at examples of corrupted density estimation plots for distributions with discrete features.&lt;/p></description></item><item><title>Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators</title><link>https://aakinshin.net/posts/wthdqe-efficiency/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wthdqe-efficiency/</guid><description>&lt;p>In previous posts, I suggested two modifications of the Harrell-Davis quantile estimator:
&lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>.
Both modifications have a higher level of robustness in comparison to the original estimator.
Also, I &lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">discussed&lt;/a> the &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>
of the Harrell-Davis quantile estimator.
In this post, I&amp;rsquo;m going to continue numerical simulation and estimate the efficiency of
the winsorized and trimmed modifications.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" target="_blank" alt="LightAndHeavy_N10_Efficiency">
 &lt;img
 src="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>Trimmed modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/trimmed-hdqe/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/trimmed-hdqe/</guid><description>&lt;p>In one of &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">the previous posts&lt;/a>, I discussed winsorized Harrell-Davis quantile estimator.
This estimator is more robust than the classic Harrell-Davis quantile estimator.
In this post, I want to suggest another modification that may be better for some corner cases:
the &lt;em>trimmed&lt;/em> Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Efficiency of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/hdqe-efficiency/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hdqe-efficiency/</guid><description>&lt;p>One of the most essential properties of a quantile estimator is
its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
In simple words, the efficiency describes the estimator accuracy.
The Harrell-Davis quantile estimator is a good option to achieve higher efficiency.
However, this estimator may provide lower efficiency in some special cases.
In this post, we will conduct a set of simulations that show the actual efficiency numbers.
We compare different distributions (symmetric and right-skewed, heavy-tailed and light-tailed),
quantiles, and sample sizes.&lt;/p></description></item><item><title>Navruz-Özdemir quantile estimator</title><link>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</guid><description>&lt;p>The Navruz-Özdemir quantile estimator
suggests the following equation to estimate the $p^\textrm{th}$ quantile of sample $X$:&lt;/p>
$$
\begin{split}
\operatorname{NO}_p =
&amp; \Big( (3p-1)X_{(1)} + (2-3p)X_{(2)} - (1-p)X_{(3)} \Big) B_0 +\\
&amp; +\sum_{i=1}^n \Big((1-p)B_{i-1}+pB_i\Big)X_{(i)} +\\
&amp; +\Big( -pX_{(n-2)} + (3p-1)X_{(n-1)} + (2-3p)X_{(n)} \Big) B_n
\end{split}
$$
&lt;p>where $B_i = B(i; n, p)$ is probability mass function of the binomial distribution $B(n, p)$,
$X_{(i)}$ are order statistics of sample $X$.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1111/bmsp.12198">&amp;ldquo;A new quantile estimator with weights based on a subsampling approach&amp;rdquo;&lt;/a> (2020)
by Gözde Navruz and A. Fırat Özdemir.
Also, I add some additional explanations,
simplify the final equation,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Sfakianakis-Verginis quantile estimator</title><link>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</guid><description>&lt;p>There are dozens of different ways to estimate quantiles.
One of these ways is to use the Sfakianakis-Verginis quantile estimator.
To be more specific, it&amp;rsquo;s a family of three estimators.
If we want to estimate the $p^\textrm{th}$ quantile of sample $X$,
we can use one of the following equations:&lt;/p>
$$
\begin{split}
\operatorname{SV1}_p =&amp;
\frac{B_0}{2} \big( X_{(1)}+X_{(2)}-X_{(3)} \big) +
\sum_{i=1}^{n} \frac{B_i+B_{i-1}}{2} X_{(i)} +
\frac{B_n}{2} \big(- X_{(n-2)}+X_{(n-1)}-X_{(n)} \big),\\
\operatorname{SV2}_p =&amp; \sum_{i=1}^{n} B_{i-1} X_{(i)} + B_n \cdot \big(2X_{(n)} - X_{(n-1)}\big),\\
\operatorname{SV3}_p =&amp; \sum_{i=1}^n B_i X_{(i)} + B_0 \cdot \big(2X_{(1)}-X_{(2)}\big).
\end{split}
$$
&lt;p>where $B_i = B(i; n, p)$ is probability mass function of the binomial distribution $B(n, p)$,
$X_{(i)}$ are order statistics of sample $X$.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1080/03610910701790491">&amp;ldquo;A new family of nonparametric quantile estimators&amp;rdquo;&lt;/a> (2008)
by Michael E. Sfakianakis and Dimitris G. Verginis.
Also, I add some additional explanations,
reconstruct missing steps,
simplify the final equations,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Winsorized modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/winsorized-hdqe/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/winsorized-hdqe/</guid><description>&lt;p>The Harrell-Davis quantile estimator is one of my favorite quantile estimators
because of its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
It has a small mean square error which allows getting accurate estimations.
However, it has a severe drawback: it&amp;rsquo;s not robust.
Indeed, since the estimator includes all sample elements with positive weights,
its &lt;a href="https://en.wikipedia.org/wiki/Robust_statistics#Breakdown_point">breakdown point&lt;/a> is zero.&lt;/p>
&lt;p>In this post, I want to suggest modifications of the Harrell-Davis quantile estimator
which increases its &lt;em>robustness&lt;/em> keeping almost the same level of &lt;em>efficiency&lt;/em>.&lt;/p></description></item><item><title>Misleading standard deviation</title><link>https://aakinshin.net/posts/misleading-stddev/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/misleading-stddev/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation&lt;/a> may be an extremely misleading metric.
Even minor deviations from normality could make it completely unreliable and deceiving.
Let me demonstrate this problem using an example.&lt;/p>
&lt;p>Below you can see three density plots of some distributions.
Could you guess their standard deviations?&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/misleading-stddev/img/density1-light.png" target="_blank" alt="density1">
 &lt;img
 src="https://aakinshin.net/posts/misleading-stddev/img/density1-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/misleading-stddev/img/density1-dark.png" target="_blank" alt="density1">
 &lt;img
 src="https://aakinshin.net/posts/misleading-stddev/img/density1-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>The correct answers are $1.0, 3.0, 11.0$.
And here is a more challenging problem: could you match these values with the corresponding distributions?&lt;/p></description></item><item><title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/unbiased-mad-hd/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad-hd/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> ($\textrm{MAD}$)
is a robust measure of scale.
In the previous post, I &lt;a href="https://aakinshin.net/posts/unbiased-mad/">showed&lt;/a>
how to use the &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased&lt;/a>
version of the $\textrm{MAD}$ estimator
as a robust alternative to the standard deviation.
&amp;ldquo;Unbiasedness&amp;rdquo; means that such estimator&amp;rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the &lt;a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias–variance tradeoff&lt;/a>:
when we remove the bias of the $\textrm{MAD}$ estimator,
we increase its variance and mean squared error ($\textrm{MSE}$).&lt;/p>
&lt;p>In this post, I want to suggest a more &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficient&lt;/a>
unbiased $\textrm{MAD}$ estimator.
It&amp;rsquo;s also a consistent estimator for the standard deviation, but it has smaller $\textrm{MSE}$.
To build this estimator,
we should replace the classic &amp;ldquo;straightforward&amp;rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&amp;rsquo;s discuss this approach in detail.&lt;/p></description></item><item><title>Unbiased median absolute deviation</title><link>https://aakinshin.net/posts/unbiased-mad/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> ($\textrm{MAD}$)
is a robust measure of scale.
For distribution $X$, it can be calculated as follows:&lt;/p>
$$
\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
$$
&lt;p>where $C$ is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the $\textrm{MAD}$ as a &lt;a href="https://en.wikipedia.org/wiki/Consistent_estimator">consistent estimator&lt;/a>
for the standard deviation under the normal distribution,
we should set&lt;/p>
$$
C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
$$
&lt;p>where $\Phi^{-1}$ is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If $X$ is the normal distribution, we get $\textrm{MAD} = \sigma$ where $\sigma$ is the standard deviation.&lt;/p>
&lt;p>Now let&amp;rsquo;s consider a sample $x = \{ x_1, x_2, \ldots x_n \}$.
Let&amp;rsquo;s denote the median absolute deviation for a sample of size $n$ as $\textrm{MAD}_n$.
The corresponding equation looks similar to the definition of $\textrm{MAD}$ for a distribution:&lt;/p>
$$
\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
$$
&lt;p>Let&amp;rsquo;s assume that $\textrm{median}$ is the straightforward definition of the median
(if $n$ is odd, the median is the middle element of the sorted sample,
if $n$ is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use $C_n = C_{\infty}$ for extremely large sample sizes.
However, for small $n$, $\textrm{MAD}_n$ becomes a &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased estimator&lt;/a>.
If we want to get an unbiased version, we should adjust the value of $C_n$.&lt;/p>
&lt;p>In this post, we look at the possible approaches and learn the way to get the exact value of $C_n$
that makes $\textrm{MAD}_n$ unbiased estimator of the median absolute deviation for any $n$.&lt;/p></description></item><item><title>Comparing distribution quantiles using gamma effect size</title><link>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</guid><description>&lt;p>There are several ways to describe the difference between two distributions.
Here are a few examples:&lt;/p>
&lt;ul>
&lt;li>Effect sizes based on differences between means (e.g., Cohen&amp;rsquo;s d, Glass&amp;rsquo; Δ, Hedges&amp;rsquo; g)&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/">The shift and ration functions&lt;/a> that
estimate differences between matched quantiles.&lt;/li>
&lt;/ul>
&lt;p>In one of the previous post, I &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">described&lt;/a>
the gamma effect size which is defined not for the mean but for quantiles.
In this post, I want to share a few case studies that demonstrate
how the suggested metric combines the advantages of the above approaches.&lt;/p></description></item><item><title>A single outlier could completely distort your Cohen's d value</title><link>https://aakinshin.net/posts/cohend-and-outliers/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cohend-and-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&amp;rsquo;s d&lt;/a> is a popular way to estimate
the &lt;a href="https://en.wikipedia.org/wiki/Effect_size">effect size&lt;/a> between two samples.
It works excellent for perfectly normal distributions.
Usually, people think that slight deviations from normality
shouldn&amp;rsquo;t produce a noticeable impact on the result.
Unfortunately, it&amp;rsquo;s not always true.
In fact, a single outlier value can completely distort the result even in large samples.&lt;/p>
&lt;p>In this post, I will present some illustrations for this problem and will show how to fix it.&lt;/p></description></item><item><title>Better moving quantile estimations using the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">have discussed&lt;/a> the Hardle-Steiger method.
This algorithm allows estimating &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>
using $O(L)$ memory and $O(log(L))$ element processing complexity (where $L$ is the window size).
Also, I have shown how to adapt this approach to estimate &lt;em>any&lt;/em> moving quantile.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the &lt;a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics&lt;/a>
which is the $k\textrm{th}$ smallest element from the sample.
It means that the estimated quantile value always equals one of the last $L$ observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for $x = \{4, 5, 6, 7\}$,
some estimators return $5.5$ (which is the arithmetical mean of $5$ and $6$)
instead of $5$ or $6$ (which are order statistics).&lt;/p>
&lt;p>Let&amp;rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.&lt;/p></description></item><item><title>MP² quantile estimator: estimating the moving median without storing values</title><link>https://aakinshin.net/posts/mp2-quantile-estimator/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mp2-quantile-estimator/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">described&lt;/a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.&lt;/p>
&lt;p>After the publication, I got a lot of questions about &lt;em>moving&lt;/em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">wrote&lt;/a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last $L$ numbers
($L$ is known as the window size).
However, it requires $O(L)$ memory, and it takes $O(log(L))$ time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&amp;rsquo;t allow implementing low-overhead telemetry in the case of large $L$.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present a moving modification of the P² quantile estimator.
Let&amp;rsquo;s call it MP² (moving P²).
It requires $O(1)$ memory, it takes $O(1)$ to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.&lt;/p>
&lt;p>Let&amp;rsquo;s discuss MP² in detail!&lt;/p></description></item><item><title>Fast implementation of the moving quantile based on the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</guid><description>&lt;p>Imagine you have a time series.
Let&amp;rsquo;s say, after each new observation, you want to know an &amp;ldquo;average&amp;rdquo; value across the last $L$ observations.
Such a metric is known as &lt;a href="https://en.wikipedia.org/wiki/Moving_average">a moving average&lt;/a>
(or rolling/running average).&lt;/p>
&lt;p>The most popular moving average example is &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average">the moving mean&lt;/a>.
It&amp;rsquo;s easy to efficiently implement this metric.
However, it has a major drawback: it&amp;rsquo;s not robust.
Outliers can easily spoil the moving mean and transform it into a meaningless and untrustable metric.&lt;/p>
&lt;p>Fortunately, we have a good alternative: &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>.
Typically, it generates a stable and smooth series of values.
In the below figure, you can see the difference between the moving mean and the moving median on noisy data.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>The moving median also has a drawback: it&amp;rsquo;s not easy to efficiently implement it.
Today we going to discuss the Hardle-Steiger method to estimate the median
(memory: $O(L)$, element processing complexity: $O(log(L))$, median estimating complexity: $O(1)$).
Also, we will learn how to calculate &lt;em>the moving quantiles&lt;/em> based on this method.&lt;/p>
&lt;p>In this post, you will find the following:&lt;/p>
&lt;ul>
&lt;li>An overview of the Hardle-Steiger method&lt;/li>
&lt;li>A simple way to implement the Hardle-Steiger method&lt;/li>
&lt;li>Moving quantiles inspired by the Hardle-Steiger method&lt;/li>
&lt;li>How to process initial elements&lt;/li>
&lt;li>Reference C# implementation&lt;/li>
&lt;/ul></description></item><item><title>Coverage of quantile confidence intervals</title><link>https://aakinshin.net/posts/quantile-ci-coverage/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-ci-coverage/</guid><description>&lt;p>There is a common &lt;a href="https://en.wikipedia.org/wiki/Confidence_interval#Misunderstandings">misunderstanding&lt;/a>
that a 95% confidence interval is an interval that covers the true parameter value with 95% probability.
Meanwhile, the correct definition assumes that
the true parameter value will be covered by 95% of 95% confidence intervals &lt;em>in the long run&lt;/em>.
These two statements sound similar, but there is a huge difference between them.
95% in this context is not a property of a single confidence interval.
Once you get a calculated interval, it may cover the true value (100% probability) or
it may don&amp;rsquo;t cover it (0% probability).
In fact, 95% is a &lt;em>prediction&lt;/em> about the percentage of &lt;em>future&lt;/em> confidence intervals
that cover the true value &lt;em>in the long run&lt;/em>.&lt;/p>
&lt;p>However, even if you know the correct definition, you still may experience some troubles.
The first thing people usually forgot is the &amp;ldquo;long run&amp;rdquo; part.
For example, if we collected 100 samples and calculated a 95% confidence interval of a parameter for each of them,
we shouldn&amp;rsquo;t expect that 95 of these intervals cover the true parameter value.
In fact, we can observe a situation when none of these intervals covers the true value.
Of course, this is an unlikely event, but if you automatically perform thousands of different experiments,
you will definitely get some extreme situations.&lt;/p>
&lt;p>The second thing that may create trouble is the &amp;ldquo;prediction&amp;rdquo; part.
If weather forecasters predicted that it will rain tomorrow, this does not mean that it will rain tomorrow.
The same works for statistical predictions.
The actual prediction reliability may depend on many factors.
If you estimate confidence intervals around the mean for the normal distribution, you are most likely safe.
However, if you estimate confidence intervals around quantiles for non-parametric distributions,
you should care about the following things:&lt;/p>
&lt;ul>
&lt;li>The used approach to estimate confidence intervals&lt;/li>
&lt;li>The underlying distribution&lt;/li>
&lt;li>The sample size&lt;/li>
&lt;li>The position of the target quantile&lt;/li>
&lt;/ul>
&lt;p>I &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">have already showed&lt;/a> how to estimate
the confidence interval around the given quantile using the Maritz-Jarrett method.
It&amp;rsquo;s time to verify the reliability of this approach.
In this post, I&amp;rsquo;m going to show some Monte-Carlo simulations that evaluate the coverage percentage in different situations.&lt;/p></description></item><item><title>Quantile confidence intervals for weighted samples</title><link>https://aakinshin.net/posts/weighted-quantiles-ci/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles-ci/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>When you work with non-parametric distributions,
quantile estimations are essential to get the main distribution properties.
Once you get the estimation values, you may be interested in measuring the accuracy of these estimations.
Without it, it&amp;rsquo;s hard to understand how trustable the obtained values are.
One of the most popular ways to evaluate accuracy is confidence interval estimation.&lt;/p>
&lt;p>Now imagine that you collect some measurements every day.
Each day you get a small sample of values that is not enough to get the accurate daily quantile estimations.
However, the full time-series over the last several weeks has a decent size.
You suspect that past measurements should be similar to today measurements,
but you are not 100% sure about it.
You feel a temptation to extend the up-to-date sample by the previously collected values,
but it may spoil the estimation (e.g., in the case of recent change points or positive/negative trends).&lt;/p>
&lt;p>One of the possible approaches in this situation is to use &lt;em>weighted samples&lt;/em>.
This assumes that we add past measurements to the &amp;ldquo;today sample,&amp;rdquo;
but these values should have smaller weight.
The older measurement we take, the smaller weight it gets.
If you have consistent values across the last several days,
this approach works like a charm.
If you have any recent changes, you can detect such situations by huge confidence intervals
due to the sample inconsistency.&lt;/p>
&lt;p>So, how do we estimate confidence intervals around quantiles for the weighted samples?
In one of the previous posts, I have already shown how to &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">estimate quantiles on weighted samples&lt;/a>.
In this post, I will show how to estimate quantile confidence intervals for weighted samples.&lt;/p></description></item><item><title>Quantile absolute deviation: estimating statistical dispersion around quantiles</title><link>https://aakinshin.net/posts/qad/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad/</guid><description>&lt;p>There are many different metrics for &lt;a href="https://en.wikipedia.org/wiki/Statistical_dispersion">statistical dispersion&lt;/a>.
The most famous one is the &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation&lt;/a>.
The standard deviation is the most popular way to describe the spread around the mean when
you work with normally distributed data.
However, if you work with non-normal distributions, this metric may be misleading.&lt;/p>
&lt;p>In the world of non-parametric distributions,
the most common measure of &lt;a href="https://en.wikipedia.org/wiki/Central_tendency">central tendency&lt;/a> is the median.
For the median, you can describe dispersion using the
&lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation around the median&lt;/a> (MAD).
It works great if the median is the only &lt;a href="https://en.wikipedia.org/wiki/Summary_statistics">summary statistic&lt;/a> that you care about.
However, if you work with multimodal distributions
(they can be detected using the &lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/">lowland multimodality detector&lt;/a>),
you may be interested in other quantiles as well.
So, it makes sense to learn how to describe dispersion around the given quantile.
Which metric should we choose?&lt;/p>
&lt;p>Recently, I came up with a great solution to this problem.
We can generalize the median absolute deviation into the quantile absolute deviation (QAD) around the given quantile based on the Harrell-Davis quantile estimator.
I will show how to calculate it, how to interpret it, and how to get insights about distribution properties
from images like this one:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qad/img/modal5-light.png" target="_blank" alt="modal5">
 &lt;img
 src="https://aakinshin.net/posts/qad/img/modal5-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qad/img/modal5-dark.png" target="_blank" alt="modal5">
 &lt;img
 src="https://aakinshin.net/posts/qad/img/modal5-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div></description></item><item><title>P² quantile estimator: estimating the median without storing values</title><link>https://aakinshin.net/posts/p2-quantile-estimator/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/p2-quantile-estimator/</guid><description>&lt;p>&lt;strong>Update: the estimator accuracy could be improved using a bunch of &lt;a href="https://aakinshin.net/tags/research-p2qe/">patches&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>Imagine that you are implementing performance telemetry in your application.
There is an operation that is executed millions of times, and you want to get its &amp;ldquo;average&amp;rdquo; duration.
It&amp;rsquo;s not a good idea to use the arithmetic mean because the obtained value can be easily spoiled by outliers.
It&amp;rsquo;s much better to use the median which is one of the most robust ways to describe the average.&lt;/p>
&lt;p>The straightforward median estimation approach requires storing all the values.
In our case, it&amp;rsquo;s a bad idea to keep all the values because it will significantly increase the memory footprint.
Such telemetry is harmful because it may become a new bottleneck instead of monitoring the actual performance.&lt;/p>
&lt;p>Another way to get the median value is to use a sequential quantile estimator
(also known as an online quantile estimator or a streaming quantile estimator).
This is an algorithm that allows calculating the median value (or any other quantile value)
using a fixed amount of memory.
Of course, it provides only an approximation of the real median value,
but it&amp;rsquo;s usually enough for typical telemetry use cases.&lt;/p>
&lt;p>In this post, I will show one of the simplest sequential quantile estimators that is called the P² quantile estimator
(or the Piecewise-Parabolic quantile estimator).&lt;/p></description></item><item><title>Plain-text summary notation for multimodal distributions</title><link>https://aakinshin.net/posts/modality-summary-notation/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/modality-summary-notation/</guid><description>&lt;p>Let&amp;rsquo;s say you collected a lot of data and want to explore the underlying distributions of collected samples.
If you have only a few distributions, the best way to do that is to look at the density plots
(expressed via histograms, kernel density estimations, or &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimations&lt;/a>).
However, it&amp;rsquo;s not always possible.&lt;/p>
&lt;p>Suppose you have to process dozens, hundreds, or even thousands of distributions.
In that case,
it may be extremely time-consuming to manually check visualizations of each distribution.
If you analyze distributions from the command line or send notifications about suspicious samples,
it may be impossible to embed images in the reports.
In these cases, there is a need to present a distribution using plain text.&lt;/p>
&lt;p>One way to do that is plain text histograms.
Unfortunately, this kind of visualization may occupy o lot of space.
In complicated cases, you may need 20 or 30 lines per a single distribution.&lt;/p>
&lt;p>Another way is to present classic &lt;a href="https://en.wikipedia.org/wiki/Summary_statistics">summary statistics&lt;/a>
like mean or median, standard deviation or median absolute deviation, quantiles, skewness, and kurtosis.
There is another problem here:
without experience, it&amp;rsquo;s hard to reconstruct the true distribution shape based on these values.
Even if you are an experienced researcher, the statistical metrics may become misleading in the case of multimodal distributions.
Multimodality is one of the most severe challenges in distribution analysis because it distorts basic summary statistics.
It&amp;rsquo;s important to not only find such distribution but also have a way to present brief information about multimodality effects.&lt;/p>
&lt;p>So, how can we condense the underlying distribution shape of a given sample to a short text line?
I didn&amp;rsquo;t manage to find an approach that works fine in my cases, so I came up with my own notation.
Most of the interpretation problems in my experiments arise from multimodality and outliers,
so I decided to focus on these two things and specifically highlight them.
Let&amp;rsquo;s consider this plot:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg" target="_blank" alt="thumbnail">
 &lt;img
 src="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-dark.svg" target="_blank" alt="thumbnail">
 &lt;img
 src="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>I suggest describing it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="o">{&lt;/span>1.00, 2.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>7.16&lt;span class="p">;&lt;/span> 13.12&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>19.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>27.69&lt;span class="p">;&lt;/span> 32.34&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>37.00..39.00&lt;span class="o">}&lt;/span>_3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let me explain the suggested notation in detail.&lt;/p></description></item><item><title>Intermodal outliers</title><link>https://aakinshin.net/posts/intermodal-outliers/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/intermodal-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Outlier">Outlier&lt;/a> analysis is a typical step in distribution exploration.
Usually, we work with the &amp;ldquo;lower outliers&amp;rdquo; (extremely low values) and the &amp;ldquo;upper outliers&amp;rdquo; (extremely high values).
However, outliers are not always extreme values.
In the general case, an outlier is a value that significantly differs from other values in the same sample.
In the case of multimodal distribution, we can also consider outliers in the middle of the distribution.
Let&amp;rsquo;s call such outliers that we found between modes the &amp;ldquo;&lt;em>intermodal outliers&lt;/em>.&amp;rdquo;&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg" target="_blank" alt="step4">
 &lt;img
 src="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/intermodal-outliers/img/step4-dark.svg" target="_blank" alt="step4">
 &lt;img
 src="https://aakinshin.net/posts/intermodal-outliers/img/step4-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Look at the above density plot.
It&amp;rsquo;s a bimodal distribution that is formed as a combination of two unimodal distributions.
Each of the unimodal distributions may have its own lower and upper outliers.
When we merge them, the upper outliers of the first distribution and the lower outliers of the second distribution
stop being lower or upper outliers.
However, if these values don&amp;rsquo;t belong to the modes, they still are a subject of interest.
In this post, I will show you how to detect such intermodal outliers
and how they can be used to form a better distribution description.&lt;/p></description></item><item><title>Lowland multimodality detection</title><link>https://aakinshin.net/posts/lowland-multimodality-detection/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/lowland-multimodality-detection/</guid><description>&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-light.svg" target="_blank" alt="data5">
 &lt;img
 src="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-dark.svg" target="_blank" alt="data5">
 &lt;img
 src="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Multimodality is an essential feature of a distribution, which may create many troubles during automatic analysis.
One of the best ways to work with such distributions is to detect all the modes in advance based on the given samples.
Unfortunately, this problem is much harder than it looks like.&lt;/p>
&lt;p>I tried many different approaches for multimodality detection, but none of them was good enough.
During the past several years, my approach of choice was the &lt;a href="http://www.brendangregg.com/FrequencyTrails/modes.html">mvalue-based modal test&lt;/a> by Brendan Gregg.
It works nicely in simple cases, but I was constantly stumbling over noisy samples where this algorithm doesn&amp;rsquo;t produce reliable results.
Also, it has some limitations that make it unapplicable to some corner cases.&lt;/p>
&lt;p>So, I needed a better approach.
Here are my main requirements:&lt;/p>
&lt;ul>
&lt;li>It should detect the exact mode locations and ranges&lt;/li>
&lt;li>It should provide reliable results even on noisy samples&lt;/li>
&lt;li>It should be able to detect multimodality even when some modes are extremely close to each other&lt;/li>
&lt;li>It should work out of the box without tricky parameter tuning for each specific distribution&lt;/li>
&lt;/ul>
&lt;p>I failed to find such an algorithm anywhere, so I came up with my own!
The current working title is &amp;ldquo;the lowland multimodality detector.&amp;rdquo;
It takes an estimation of the probability density function (PDF) and tries to find &amp;ldquo;lowlands&amp;rdquo; (areas that are much lower than neighboring peaks).
Next, it splits the plot by these lowlands and detects modes between them.
For the PDF estimation, it uses the &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimation based on the Harrell-Davis quantile estimator&lt;/a> (QRDE-HD).
Let me explain how it works in detail.&lt;/p></description></item><item><title>Quantile-respectful density estimation based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/qrde-hd/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qrde-hd/</guid><description>&lt;p>The idea of this post was born when I was working on a presentation for my recent &lt;a href="https://dotnext.ru/en/">DotNext&lt;/a> &lt;a href="https://www.youtube.com/watch?v=gc3yVybPuaY&amp;list=PL21xssNXOJNGUROqzSTOC8uZL4W2QZpvK&amp;index=1">talk&lt;/a>.
It had a &lt;a href="https://slides.aakinshin.net/dotnext-piter2020/#193">slide&lt;/a> with a density plot like this:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/qrde-hd/img/riddle-light.png" target="_blank" alt="riddle">
 &lt;img
 src="https://aakinshin.net/posts/qrde-hd/img/riddle-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/qrde-hd/img/riddle-dark.png" target="_blank" alt="riddle">
 &lt;img
 src="https://aakinshin.net/posts/qrde-hd/img/riddle-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Here we can see a density plot based on a sample with highlighted &lt;a href="https://en.wikipedia.org/wiki/Decile">decile&lt;/a> locations that split the plot into 10 equal parts.
Before the conference, I have been reviewed by &lt;a href="https://twitter.com/VladimirSitnikv">@VladimirSitnikv&lt;/a>.
He raised a reasonable concern: it doesn&amp;rsquo;t look like all the density plot segments are equal and contain exactly 10% of the whole plot.
And he was right!&lt;/p>
&lt;p>However, I didn&amp;rsquo;t make any miscalculations.
I generated a real sample with 61 elements.
Next, I build a density plot with the kernel density estimation (KDE) using the Sheather &amp;amp; Jones method and the normal kernel.
Next, I calculated decile values using the Harrell-Davis quantile estimator.
Although both the density plot and the decile values are calculated correctly and consistent with the sample,
they are not consistent with each other!
Indeed, such a density plot is just an estimation of the underlying distribution.
It has its own decile values, which are not equal to the sample decile values regardless of the used quantile estimator.
This problem is common for different kinds of visualization that presents density and quantiles at the same time (e.g., &lt;a href="https://towardsdatascience.com/violin-plots-explained-fb1d115e023d">violin plots&lt;/a>)&lt;/p>
&lt;p>It leads us to a question: how should we present the shape of our data together with quantile values without confusing inconsistency in the final image?
Today I will present a good solution: we should use the quantile-respectful density estimation based on the Harrell-Davis quantile estimator!
I know the title is a bit long, but it&amp;rsquo;s not so complicated as it sounds.
In this post, I will show how to build such plots.
Also I will compare them to the classic histograms and kernel density estimations.
As a bonus, I will demonstrate how awesome these plots are for multimodality detection.&lt;/p></description></item><item><title>Misleading histograms</title><link>https://aakinshin.net/posts/misleading-histograms/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/misleading-histograms/</guid><description>&lt;p>Below you see two histograms.
What could you say about them?&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-light.svg" target="_blank" alt="hist-riddle">
 &lt;img
 src="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-dark.svg" target="_blank" alt="hist-riddle">
 &lt;img
 src="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Most likely, you say that the first histogram is based on a uniform distribution,
and the second one is based on a multimodal distribution with four modes.
Although this is not obvious from the plots,
both histograms are based on the same sample:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="line">&lt;span class="cl">&lt;span class="m">20.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.94&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.03&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.15&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.99&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.20&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.99&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.22&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.86&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.06&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">29.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.73&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.12&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.18&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.95&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.93&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.07&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">40.10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.93&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.05&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.91&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.07&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.86&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.91&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.14&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">49.95&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.03&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.15&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.70&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.05&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.13&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Thus, the only difference between histograms is the offset!&lt;/p>
&lt;p>Visualization is a simple way to understand the shape of your data.
Unfortunately, this way may easily become a slippery slope.
In the &lt;a href="https://aakinshin.net/posts/kde-bw/">previous post&lt;/a>, I have shown how density plots may deceive you when the bandwidth is poorly chosen.
Today, we talk about histograms and why you can&amp;rsquo;t trust them in the general case.&lt;/p></description></item><item><title>The importance of kernel density estimation bandwidth</title><link>https://aakinshin.net/posts/kde-bw/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kde-bw/</guid><description>&lt;p>Below see two kernel density estimations.
What could you say about them?&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/kde-bw/img/kde-riddle-light.svg" target="_blank" alt="kde-riddle">
 &lt;img
 src="https://aakinshin.net/posts/kde-bw/img/kde-riddle-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/kde-bw/img/kde-riddle-dark.svg" target="_blank" alt="kde-riddle">
 &lt;img
 src="https://aakinshin.net/posts/kde-bw/img/kde-riddle-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Most likely, you say that the first plot is based on a uniform distribution,
and the second one is based on a multimodal distribution with four modes.
Although this is not obvious from the plots,
both density plots are based on the same sample:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-txt" data-lang="txt">&lt;span class="line">&lt;span class="cl">21.370, 19.435, 20.363, 20.632, 20.404, 19.893, 21.511, 19.905, 22.018, 19.93,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">31.304, 32.286, 28.611, 29.721, 29.866, 30.635, 29.715, 27.343, 27.559, 31.32,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">39.693, 38.218, 39.828, 41.214, 41.895, 39.569, 39.742, 38.236, 40.460, 39.36,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">50.455, 50.704, 51.035, 49.391, 50.504, 48.282, 49.215, 49.149, 47.585, 50.03
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The only difference between plots is in &lt;a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#Bandwidth_selection">bandwidth selection&lt;/a>!&lt;/p>
&lt;p>Bandwidth selection is crucial when you are trying to visualize your distributions.
Unfortunately, most people just call a regular function to build a density plot and don&amp;rsquo;t think about how the bandwidth will be chosen.
As a result, the plot may present data in the wrong way, which may lead to incorrect conclusions.
Let&amp;rsquo;s discuss bandwidth selection in detail and figure out how to improve the correctness of your density plots.
In this post, we will cover the following topics:&lt;/p>
&lt;ul>
&lt;li>Kernel density estimation&lt;/li>
&lt;li>How bandwidth selection affects plot smoothness&lt;/li>
&lt;li>Which bandwidth selectors can we use&lt;/li>
&lt;li>Which bandwidth selectors should we use&lt;/li>
&lt;li>Insidious default bandwidth selectors in statistical packages&lt;/li>
&lt;/ul></description></item><item><title>The median absolute deviation value of the Gumbel distribution</title><link>https://aakinshin.net/posts/gumbel-mad/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gumbel-mad/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution&lt;/a> is not only a useful model in the &lt;a href="https://en.wikipedia.org/wiki/Extreme_value_theory">extreme value theory&lt;/a>,
but it&amp;rsquo;s also a nice example of a slightly right-skewed distribution (skewness $\approx 1.14$).
Here is its density plot:&lt;/p>






&lt;div class="row">
&lt;div class="mx-auto">
 &lt;a href="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg" target="_blank" class="imgldlink" alt="gumbel">
 &lt;picture>
 &lt;source
 theme='dark'
 srcset="https://aakinshin.net/posts/gumbel-mad/img/gumbel-dark.svg"
 media="(prefers-color-scheme: dark)">
 &lt;source
 theme='light'
 srcset="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg"
 media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
 &lt;img
 class="mx-auto d-block img-fluid"
 width='600'
 src="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg">
 &lt;/picture>
 &lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>In some of my statistical experiments, I like to use the Gumbel distribution as a sample generator for hypothesis checking or unit tests.
I also prefer the &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (MAD) over the standard deviation as a measure of dispersion because it&amp;rsquo;s more robust in the case of non-parametric distributions.
Numerical hypothesis verification often requires the exact value of the median absolute deviation of the original distribution.
I didn&amp;rsquo;t find this value in the reference tables, so I decided to do another exercise and derive it myself.
In this post, you will find a short derivation and the result (spoiler: the exact value is &lt;code>0.767049251325708 * β&lt;/code>).
The general approach of the MAD derivation is common for most distributions, so it can be easily reused.&lt;/p></description></item><item><title>Weighted quantile estimators</title><link>https://aakinshin.net/posts/weighted-quantiles/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>In this post, I will show how to calculate weighted quantile estimates and how to use them in practice.&lt;/p>
&lt;p>Let&amp;rsquo;s start with a problem from real life.
Imagine that you measure the total duration of a unit test executed daily on a CI server.
Every day you get a single number that corresponds to the test duration from the latest revision for this day:&lt;/p>






&lt;div class="row">
&lt;div class="mx-auto">
 &lt;a href="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg" target="_blank" class="imgldlink" alt="moving1">
 &lt;picture>
 &lt;source
 theme='dark'
 srcset="https://aakinshin.net/posts/weighted-quantiles/img/moving1-dark.svg"
 media="(prefers-color-scheme: dark)">
 &lt;source
 theme='light'
 srcset="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg"
 media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
 &lt;img
 class="mx-auto d-block img-fluid"
 width='600'
 src="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg">
 &lt;/picture>
 &lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>You collect a history of such measurements for 100 days.
Now you want to describe the &amp;ldquo;actual&amp;rdquo; distribution of the performance measurements.&lt;/p>
&lt;p>However, for the latest &amp;ldquo;actual&amp;rdquo; revision, you have only a single measurement, which is not enough to build a distribution.
Also, you can&amp;rsquo;t build a distribution based on the last N measurements because they can contain change points that will spoil your results.
So, what you really want to do is to use all the measurements, but older values should have a lower impact on the final distribution form.&lt;/p>
&lt;p>Such a problem can be solved using the weighted quantiles!
This powerful approach can be applied to any time series regardless of the domain area.
In this post, we learn how to calculate and apply weighted quantiles.&lt;/p></description></item><item><title>Nonparametric Cohen's d-consistent effect size</title><link>https://aakinshin.net/posts/nonparametric-effect-size/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nonparametric-effect-size/</guid><description>&lt;p>&lt;strong>Update: the second part of this post is available &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size2/">here&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>The effect size is a common way to describe a difference between two distributions.
When these distributions are normal, one of the most popular approaches to express the effect size is &lt;a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&amp;rsquo;s d&lt;/a>.
Unfortunately, it doesn&amp;rsquo;t work great for non-normal distributions.&lt;/p>
&lt;p>In this post, I will show a robust Cohen&amp;rsquo;s d-consistent effect size formula for nonparametric distributions.&lt;/p>





&lt;div class="row">
&lt;div class="mx-auto">
 &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/img/blackboard.png" target="_blank" alt="blackboard">
 &lt;img
 class="mx-auto d-block img-fluid"
 width='800'
 src="https://aakinshin.net/posts/nonparametric-effect-size/img/blackboard.png" />
 &lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>DoubleMAD outlier detector based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</guid><description>&lt;p>Outlier detection is an important step in data processing.
Unfortunately, if the distribution is not normal (e.g., right-skewed and heavy-tailed), it&amp;rsquo;s hard to choose
a robust outlier detection algorithm that will not be affected by tricky distribution properties.
During the last several years, I tried many different approaches, but I was not satisfied with their results.
Finally, I found an algorithm to which I have (almost) no complaints.
It&amp;rsquo;s based on the &lt;em>double median absolute deviation&lt;/em> and the &lt;em>Harrell-Davis quantile estimator&lt;/em>.
In this post, I will show how it works and why it&amp;rsquo;s better than some other approaches.&lt;/p></description></item><item><title>Distribution comparison via the shift and ratio functions</title><link>https://aakinshin.net/posts/shift-and-ratio-functions/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/shift-and-ratio-functions/</guid><description>&lt;p>When we compare two distributions, it&amp;rsquo;s not always enough to detect a statistically significant difference between them.
In many cases, we also want to evaluate the magnitude of this difference.
Let&amp;rsquo;s look at the following image:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-light.png" target="_blank" alt="compare1">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-dark.png" target="_blank" alt="compare1">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>On the left side, we can see a timeline plot with 2000 points
(at the middle of this plot, the distribution was significantly changed).
On the right side, you can see density plots for the left and the right side of
the timeline plot (before and after the change).
It&amp;rsquo;s a pretty simple case, the difference between distributions be expressed via the
difference between mean values.&lt;/p>
&lt;p>Now let&amp;rsquo;s look at a more tricky case:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-light.png" target="_blank" alt="compare2">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-dark.png" target="_blank" alt="compare2">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Here we have a bimodal distribution; after the change, the left mode &amp;ldquo;moved right.&amp;rdquo;
Now it&amp;rsquo;s much harder to evaluate the difference between distributions
because the mean and the median values almost not changed:
the right mode has the biggest impact on these metrics than the left more.&lt;/p>
&lt;p>And here is a much more tricky case:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-light.png" target="_blank" alt="compare3">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-dark.png" target="_blank" alt="compare3">
 &lt;img
 src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Here we also have a bimodal distribution; after the change, both modes moved:
the left mode &amp;ldquo;moved right&amp;rdquo; and the right mode &amp;ldquo;moved left.&amp;rdquo;
How should we describe the difference between these distributions now?&lt;/p></description></item></channel></rss>