<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Moving quantile on Andrey Akinshin</title><link>https://aakinshin.net/tags/moving-quantile/</link><description>Recent content in Moving quantile on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 11 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/moving-quantile/index.xml" rel="self" type="application/rss+xml"/><item><title>Dispersion exponential smoothing</title><link>https://aakinshin.net/posts/dispersion-exponential-smoothing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dispersion-exponential-smoothing/</guid><description>&lt;p>In this &lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">previous post&lt;/a>,
I showed how to apply exponential smoothing to quantiles
using the &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted Harrell-Davis quantile estimator&lt;/a>.
This technique allows getting smooth and stable moving median estimations.
In this post, I&amp;rsquo;m going to discuss how to use the same approach
to estimate moving dispersion.&lt;/p></description></item><item><title>Quantile exponential smoothing</title><link>https://aakinshin.net/posts/quantile-exponential-smoothing/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-exponential-smoothing/</guid><description>&lt;p>One of the popular problems in time series analysis is estimating the moving &amp;ldquo;average&amp;rdquo; value.
Let&amp;rsquo;s define the &amp;ldquo;average&amp;rdquo; as a central tendency metric like the mean or the median.
When we talk about the moving value, we assume that we are interested in
the average value &amp;ldquo;at the end&amp;rdquo; of the time series
instead of the average of all available observations.&lt;/p>
&lt;p>One of the most straightforward approaches to estimate the moving average is the &lt;em>simple moving mean&lt;/em>.
Unfortunately, this approach is not robust: outliers can instantly spoil the evaluated mean value.
As an alternative, we can consider &lt;em>simple moving median&lt;/em>.
I already discussed a few of such methods:
&lt;a href="https://aakinshin.net/posts/mp2-quantile-estimator/">the MP² quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/">a moving quantile estimator based on partitioning heaps&lt;/a>
(a modification of the Hardle-Steiger method).
When we talk about &lt;em>simple moving averages&lt;/em>, we typically assume
that we estimate the average value over the last $k$ observations ($k$ is the &lt;em>window size&lt;/em>).
This approach is also known as &lt;em>unweighted moving averages&lt;/em> because
all target observations have the same weight.&lt;/p>
&lt;p>As an alternative to the simple moving average, we can also consider the &lt;em>weighted moving average&lt;/em>.
In this case, we assign a weight for each observation and aggregate the whole time series according to these weights.
A famous example of such a weight function is &lt;em>exponential smoothing&lt;/em>.
And the simplest form of exponential smoothing is the &lt;em>exponentially weighted moving mean&lt;/em>.
This approach estimates the weighted moving mean using exponentially decreasing weights.
Switching from the simple moving mean to the exponentially weighted moving mean provides some benefits
in terms of smoothness and estimation efficiency.&lt;/p>
&lt;p>Although exponential smoothing has advantages over the simple moving mean,
it still estimates the mean value which is not robust.
We can improve the robustness of this approach if we reuse the same idea for weighted moving quantiles.
It&amp;rsquo;s possible because the quantiles also can be estimated for weighted samples.
In one of my previous posts, I &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">showed&lt;/a> how to adapt
the Hyndman-Fan Type 7 and Harrell-Davis quantile estimators to the weighted samples.
In this post, I&amp;rsquo;m going to show how we can use this technique to estimate
the weighted moving quantiles using exponentially decreasing weights.&lt;/p></description></item><item><title>Better moving quantile estimations using the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">have discussed&lt;/a> the Hardle-Steiger method.
This algorithm allows estimating &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>
using $O(L)$ memory and $O(log(L))$ element processing complexity (where $L$ is the window size).
Also, I have shown how to adapt this approach to estimate &lt;em>any&lt;/em> moving quantile.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the &lt;a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics&lt;/a>
which is the $k\textrm{th}$ smallest element from the sample.
It means that the estimated quantile value always equals one of the last $L$ observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for $x = \{4, 5, 6, 7\}$,
some estimators return $5.5$ (which is the arithmetical mean of $5$ and $6$)
instead of $5$ or $6$ (which are order statistics).&lt;/p>
&lt;p>Let&amp;rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.&lt;/p></description></item><item><title>MP² quantile estimator: estimating the moving median without storing values</title><link>https://aakinshin.net/posts/mp2-quantile-estimator/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mp2-quantile-estimator/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/tags/p2-quantile-estimator/">described&lt;/a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.&lt;/p>
&lt;p>After the publication, I got a lot of questions about &lt;em>moving&lt;/em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">wrote&lt;/a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last $L$ numbers
($L$ is known as the window size).
However, it requires $O(L)$ memory, and it takes $O(log(L))$ time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&amp;rsquo;t allow implementing low-overhead telemetry in the case of large $L$.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present a moving modification of the P² quantile estimator.
Let&amp;rsquo;s call it MP² (moving P²).
It requires $O(1)$ memory, it takes $O(1)$ to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.&lt;/p>
&lt;p>Let&amp;rsquo;s discuss MP² in detail!&lt;/p></description></item><item><title>Fast implementation of the moving quantile based on the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</guid><description>&lt;p>Imagine you have a time series.
Let&amp;rsquo;s say, after each new observation, you want to know an &amp;ldquo;average&amp;rdquo; value across the last $L$ observations.
Such a metric is known as &lt;a href="https://en.wikipedia.org/wiki/Moving_average">a moving average&lt;/a>
(or rolling/running average).&lt;/p>
&lt;p>The most popular moving average example is &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average">the moving mean&lt;/a>.
It&amp;rsquo;s easy to efficiently implement this metric.
However, it has a major drawback: it&amp;rsquo;s not robust.
Outliers can easily spoil the moving mean and transform it into a meaningless and untrustable metric.&lt;/p>
&lt;p>Fortunately, we have a good alternative: &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>.
Typically, it generates a stable and smooth series of values.
In the below figure, you can see the difference between the moving mean and the moving median on noisy data.&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" target="_blank" alt="example">
 &lt;img
 src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>The moving median also has a drawback: it&amp;rsquo;s not easy to efficiently implement it.
Today we going to discuss the Hardle-Steiger method to estimate the median
(memory: $O(L)$, element processing complexity: $O(log(L))$, median estimating complexity: $O(1)$).
Also, we will learn how to calculate &lt;em>the moving quantiles&lt;/em> based on this method.&lt;/p>
&lt;p>In this post, you will find the following:&lt;/p>
&lt;ul>
&lt;li>An overview of the Hardle-Steiger method&lt;/li>
&lt;li>A simple way to implement the Hardle-Steiger method&lt;/li>
&lt;li>Moving quantiles inspired by the Hardle-Steiger method&lt;/li>
&lt;li>How to process initial elements&lt;/li>
&lt;li>Reference C# implementation&lt;/li>
&lt;/ul></description></item></channel></rss>