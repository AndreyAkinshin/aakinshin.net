<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hodges–Lehmann Estimator on Andrey Akinshin</title><link>https://aakinshin.net/tags/hodges-lehmann-estimator/</link><description>Recent content in Hodges–Lehmann Estimator on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 26 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/hodges-lehmann-estimator/index.xml" rel="self" type="application/rss+xml"/><item><title>Hodges-Lehmann ratio estimator vs. Bhattacharyya's scale ratio estimator</title><link>https://aakinshin.net/posts/hl-ratio-vs-bhattacharyya/</link><pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-ratio-vs-bhattacharyya/</guid><description>&lt;p>Previously, I &lt;a href="https://aakinshin.net/posts/hl-ratio/">discussed&lt;/a> an idea of a ratio estimator based on the Hodges-Lehmann estimator.
This idea looks so simple and natural that I was sure that it must have already been proposed and studied.
However, when I started to search for it, it turned out that it was not as easy as I expected.
Moreover, some papers attribute this idea to Bhattacharyya, which is not accurate.
In this post, we discuss the difference between these two approaches.&lt;/p></description></item><item><title>Resistance to the low-density regions: the Hodges-Lehmann location estimator based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/rldr-hlhd/</link><pubDate>Tue, 21 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-hlhd/</guid><description>&lt;p>Previously, I have discussed the topic of the &lt;a href="https://aakinshin.net/tags/research-rldr/">resistance to the low-density regions&lt;/a>
of various estimators including &lt;a href="https://aakinshin.net/posts/rldr-hl/">the Hodges-Lehmann location estimator&lt;/a> ($\operatorname{HL}$).
In general, $\operatorname{HL}$ is a great estimator with great statistical efficiency and a decent breakdown point.
Unfortunately, it has low resistance to the low-density regions around
$29^\textrm{th}$ and $71^\textrm{th}$ percentiles, which may cause troubles in the case of multimodal distributions.
I am trying to find a modification of $\operatorname{HL}$
that performs almost the same as the original $\operatorname{HL}$, but has increased resistance.
One of the ideas I had was using the Harrell-Davis quantile estimator
instead of the sample median to evaluate $\operatorname{HL}$.
Regrettably, this idea did not turn out to be successful:
such an estimator has a resistance function similar to the original $\operatorname{HL}$.
I believe that it is important to share negative results, and therefore this post contains a bunch of plots,
which illustrate results of relevant numerical simulations.&lt;/p></description></item><item><title>Median vs. Hodges-Lehmann: compare efficiency under heavy-tailedness</title><link>https://aakinshin.net/posts/robust-eff-median-hl/</link><pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/robust-eff-median-hl/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/thoughts-robust-efficiency/">previous post&lt;/a>,
I shared some thoughts on how to evaluate the statistical efficiency of estimators under heavy-tailed distributions.
In this post, I apply the described ideas to actually compare efficiency values of
the Mean, the Sample Median, and the Hodges-Lehmann location estimator
under various distributions.&lt;/p></description></item><item><title>Weighted Hodges-Lehmann location estimator and mixture distributions</title><link>https://aakinshin.net/posts/whl-mixture/</link><pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/whl-mixture/</guid><description>&lt;p>The classic non-weighted Hodges-Lehmann location estimator of a sample $\mathbf{x} = (x_1, x_2, \ldots, x_n)$
is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right),
$$
&lt;p>where $\operatorname{median}$ is the sample median.
&lt;a href="https://aakinshin.net/posts/whl/">Previously&lt;/a>, we have defined a weighted version of the Hodges-Lehmann location estimator as follows:&lt;/p>
$$
\operatorname{WHL}(\mathbf{x}, \mathbf{w}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{wmedian}} \left(\frac{x_i + x_j}{2},\; w_i \cdot w_j \right),
$$
&lt;p>where $\mathbf{w} = (w_1, w_2, \ldots, w_n)$ is the vector of weights,
$\operatorname{wmedian}$ is the &lt;a href="https://aakinshin.net/posts/preprint-wqe/">weighted median&lt;/a>.
For simplicity, in the scope of the current post,
Hyndman-Fan Type 7 quantile estimator is used as the base for the weighted median.&lt;/p>
&lt;p>In this post, we consider a numerical simulation in which we compare sampling distribution of
$\operatorname{HL}$ and $\operatorname{WHL}$ in a case of mixture distribution.&lt;/p></description></item><item><title>Hodges-Lehmann Gaussian efficiency: location shift vs. shift of locations</title><link>https://aakinshin.net/posts/hl-loc-shift-vs-shift-loc/</link><pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-loc-shift-vs-shift-loc/</guid><description>&lt;p>Let us consider two samples $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_m)$.
The one-sample Hodges-Lehman location estimator is defined as the median of the Walsh (pairwise) averages:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right),
\quad
\operatorname{HL}(\mathbf{y}) =
 \underset{1 \leq i \leq j \leq m}{\operatorname{median}} \left(\frac{y_i + y_j}{2} \right).
$$
&lt;p>For these two samples, we can also define the shift between these two estimations:&lt;/p>
$$
\Delta_{\operatorname{HL}}(\mathbf{x}, \mathbf{y}) = \operatorname{HL}(\mathbf{x}) - \operatorname{HL}(\mathbf{y}).
$$
&lt;p>The two-sample Hodges-Lehmann location shift estimator is defined as the median of pairwise differences:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>Previously, I already compared the location shift estimator with the difference of median estimators
(&lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median1/">1&lt;/a>, &lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median2/">2&lt;/a>).
In this post, I compare the difference between two location estimations and the shift estimations
in terms of Gaussian efficiency.
Before I started this study, I expected that $\operatorname{HL}$ should be more efficient
than $\Delta_{\operatorname{HL}}$.
Let us find out if my intuition is correct or not!&lt;/p></description></item><item><title>Ratio estimator based on the Hodges-Lehmann approach</title><link>https://aakinshin.net/posts/hl-ratio/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hl-ratio/</guid><description>&lt;p>For two samples $\mathbf{x} = ( x_1, x_2, \ldots, x_n )$ and $\mathbf{y} = ( y_1, y_2, \ldots, y_m )$,
the Hodges-Lehmann location shift estimator is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>Now, let us consider the problem of estimating the ratio of the location measures instead of the shift between them.
While there are multiple approaches to providing such an estimation,
one of the options that can be considered is based on the Hodges-Lehmann ideas.&lt;/p></description></item><item><title>Understanding the pitfalls of preferring the median over the mean</title><link>https://aakinshin.net/posts/median-vs-mean/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-vs-mean/</guid><description>&lt;p>A common task in mathematical statistics is to aggregate a set of numbers $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$
to a single &amp;ldquo;average&amp;rdquo; value.
Such a value is usually called &lt;em>central tendency&lt;/em>.
There are multiple measures of central tendency.
The most popular one is the &lt;em>arithmetic average&lt;/em> or the &lt;em>mean&lt;/em>:&lt;/p>
$$
\overline{\mathbf{x}} = \left( x_1 + x_2 + \ldots + x_n \right) / n.
$$
&lt;p>The mean is so popular not only thanks to its simplicity but also
because it provides the best way to estimate the center of the perfect normal distribution.
Unfortunately, the mean is not a robust measure.
This means that a single extreme value $x_i$ may distort the mean estimation and
lead to a non-reproducible value that has nothing in common with the &amp;ldquo;expected&amp;rdquo; central tendency.
The actual real-life distributions are never normal.
They can be pretty close to the normal distribution, but only to a certain extent.
Even small deviations from normality may produce occasional extreme outliers,
which makes the mean an unreliable measure in the general case.&lt;/p>
&lt;p>When people discover the danger of the mean, they start looking for a more robust measure of the central tendency.
And the first obvious alternative is the sample median $\tilde{\mathbf{x}}$.
The classic sample median is easy to calculate.
First, you have to sort the sample.
If the sample size $n$ is odd, the median is the middle element in the sorted sample.
If $n$ is even, the median is the arithmetic average of the two middle elements in the sorted sample.
The median is extremely robust: it provides a reasonable estimate
even if almost half of the sample elements are corrupted.&lt;/p>
&lt;p>For symmetric distributions (including the normal one), the true values of the mean and the median are the same.
Once we discover the high robustness of the median, it may be tempting to always use the median instead of the mean.
The median is often perceived as &amp;ldquo;something like the mean but with high resistance to outliers.&amp;rdquo;
Indeed, what is the point of using the unreliable mean, if the median always provides a safer choice?
Should we make the median our default option for the central tendency?&lt;/p>
&lt;p>The answer is no.
You should beware of any default options in mathematical statistics.
All the measures are just tools, and each tool has its limitations and areas of applicability.
A mindless transition from the mean to the median, regardless of the underlying distribution, is not a smart move.
When we are picking a measure of central tendency to use,
the first step should be reviewing the research goals:
why do we need a measure of central tendency, and what are we going to do with the result?
It&amp;rsquo;s impossible to make a rational decision on the statistical methods used without a clear understanding of the goals.
Next, we should match the goals to the properties of available measures.&lt;/p>
&lt;p>There are multiple practical issues with the median,
but the most noticeable problem in practice is about its &lt;em>statistical efficiency&lt;/em>.
Understanding this problem reveals the price of advanced robustness of the median.
In this post, we discuss the concept of statistical efficiency,
estimate the statistical efficiency of the mean and the median under different distributions,
and consider the Hodges-Lehman estimator as a measure of central tendency
that provides a better trade-off between robustness and efficiency.&lt;/p></description></item><item><title>Efficiency of the central tendency measures under the uniform distribution</title><link>https://aakinshin.net/posts/central-tendency-efficiency-uniform/</link><pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/central-tendency-efficiency-uniform/</guid><description>&lt;p>Statistical efficiency is one of the primary ways to compare various estimators.
Since the normality assumption is often used, Gaussian efficiency (efficiency under the normality distribution)
is typically considered.
For example, the asymptotic Gaussian efficiency values of
the median and the Hodges-Lehmann location estimator (the pseudo-median)
are $\approx 64\%$ and $\approx 96\%$ respectively (assuming the baseline is the mean).&lt;/p>
&lt;p>But what if the underlying distribution is not normal, but uniform?
What would happen to the relative statistical efficiency values in this case?
Let&amp;rsquo;s find out!
In this post, we calculate
the relative efficiency of the median, the Hodges-Lehmann location estimator, and the midrange
to the mean under the uniform distribution (or under uniformity).&lt;/p></description></item><item><title>Unobvious problems of using the R's implementation of the Hodges-Lehmann estimator</title><link>https://aakinshin.net/posts/r-hodges-lehmann-problems/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/r-hodges-lehmann-problems/</guid><description>&lt;p>The Hodges-Lehmann location estimator (also known as pseudo-median) is a robust, non-parametric statistic
used as a measure of the central tendency.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$, it is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right).
$$
&lt;p>Essentially, it&amp;rsquo;s the median of the Walsh (pairwise) averages.&lt;/p>
&lt;p>For two samples $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$ and $\mathbf{y} = \{ y_1, y_2, \ldots, y_m \}$,
we can also consider the Hodges-Lehmann location shift estimator:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}, \mathbf{y}) =
 \underset{1 \leq i \leq n,\,\, 1 \leq j \leq m}{\operatorname{median}} \left(x_i - y_j \right).
$$
&lt;p>In R, both estimators are available via
the &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html">wilcox.test&lt;/a> function.
Here is a usage example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">set.seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1729&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># A sample of size 2000 from the normal distribution N(5, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">rnorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># A sample of size 2000 from the normal distribution N(2, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># (pseudo)median&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 5.000984&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># (pseudo)median&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1.969096&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">wilcox.test&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">conf.int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">estimate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># difference in location&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 3.031782&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In most cases, this function works fine.
However, there is an unobvious corner case, in which it returns wrong values.
In this post, we discuss the underlying problem and provide a correct implementation for the Hodges-Lehmann estimators.&lt;/p></description></item><item><title>Weighted modification of the Hodges-Lehmann location estimator</title><link>https://aakinshin.net/posts/whl/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/whl/</guid><description>&lt;p>The classic Hodges-Lehmann location estimator is a robust, non-parametric statistic
used as a measure of the central tendency.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$, it is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) = \underset{1 \leq i &lt; j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right).
$$
&lt;p>This estimator works great for non-weighted samples
(its asymptotic Gaussian efficiency is $\approx 96\%$, and its asymptotic breakdown point is $\approx 29\%$).
However, in real-world applications, data points may have varying importance or relevance.
For example, in finance, different stocks may have different market capitalizations,
which can impact the overall performance of an index.
In social science research, survey responses may be weighted
based on demographic representation to ensure that the final results are more generalizable.
In software performance measurements, the observations may be collected from different source code revisions,
some of which may be obsolete.
In these cases, the classic $\operatorname{HL}$-measure is not suitable, as it treats each data point equally.&lt;/p>
&lt;p>We can overcome this problem using weighted samples to obtain more accurate and meaningful central tendency estimates.
Unfortunately, there is no well-established definition of the weighted Hodges-Lehmann location estimator.
In this blog post, we introduce such a definition so that we can apply this estimator to weighted samples
keeping it compatible with the original version.&lt;/p></description></item><item><title>Trimmed Hodges-Lehmann location estimator, Part 2: Gaussian efficiency</title><link>https://aakinshin.net/posts/thl-ge/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thl-ge/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/thl-bp/">previous post&lt;/a>, we introduced
the trimmed Hodges-Lehman location estimator.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$,
it is defined as follows:&lt;/p>
$$
\operatorname{THL}(\mathbf{x}, k) = \underset{k &lt; i &lt; j \leq n - k}{\operatorname{median}}\biggl(\frac{x_{(i)} + x_{(j)}}{2}\biggr).
$$
&lt;p>We also derived the exact expression for its asymptotic and finite-sample breakdown point values.
In this post, we explore its Gaussian efficiency.&lt;/p></description></item><item><title>Trimmed Hodges-Lehmann location estimator, Part 1: breakdown point</title><link>https://aakinshin.net/posts/thl-bp/</link><pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thl-bp/</guid><description>&lt;p>For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$,
the Hodges-Lehmann location estimator is defined as follows:&lt;/p>
$$
\operatorname{HL}(\mathbf{x}) =
 \underset{i &lt; j}{\operatorname{median}}\biggl(\frac{x_i + x_j}{2}\biggr).
$$
&lt;p>Its asymptotic Gaussian efficiency is $\approx 96\%$,
while its asymptotic breakdown point is $\approx 29\%$.
This makes the Hodges-Lehmann location estimator a decent robust alternative to the mean.&lt;/p>
&lt;p>While the Gaussian efficiency is quite impressive (almost as efficient as the mean),
the breakdown point is not as great as in the case of the median (which has a breakdown point of $50\%$).
Could we change this trade-off a little bit and make this estimator more robust,
sacrificing a small portion of efficiency?
Yes, we can!&lt;/p>
&lt;p>In this post, I want to present the idea of the trimmed Hodges-Lehmann location estimator
and provide the exact equation for its breakdown point.&lt;/p></description></item><item><title>Median of the shifts vs. shift of the medians, Part 2: Gaussian efficiency</title><link>https://aakinshin.net/posts/median-shift-vs-shift-median2/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-shift-vs-shift-median2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/median-shift-vs-shift-median1/">previous post&lt;/a>,
we discussed the difference between shifts of the medians
and the Hodges-Lehmann location shift estimator.
In this post, we conduct a simple numerical simulation
to evaluate the Gaussian efficiency of these two estimators.&lt;/p></description></item><item><title>Median of the shifts vs. shift of the medians, Part 1</title><link>https://aakinshin.net/posts/median-shift-vs-shift-median1/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/median-shift-vs-shift-median1/</guid><description>&lt;p>Let us say that we have two samples
$x = \{ x_1, x_2, \ldots, x_n \}$,
$y = \{ y_1, y_2, \ldots, y_m \}$,
and we want to estimate the shift of locations between them.
In the case of the normal distribution, this task is quite simple
and has a lot of straightforward solutions.
However, in the nonparametric case, the location shift is an ambiguous metric
which heavily depends on the chosen estimator.
In the context of this post, we consider two approaches that may look similar.
The first one is the &lt;strong>s&lt;/strong>hift of the &lt;strong>m&lt;/strong>edians:&lt;/p>
$$
\newcommand{\DSM}{\Delta_{\operatorname{SM}}}
\DSM = \operatorname{median}(y) - \operatorname{median}(x).
$$
&lt;p>The second one of the median of all pairwise shifts,
also known as the &lt;strong>H&lt;/strong>odges-&lt;strong>L&lt;/strong>ehmann location shift estimator:&lt;/p>
$$
\newcommand{\DHL}{\Delta_{\operatorname{HL}}}
\DHL = \operatorname{median}(y_j - x_i).
$$
&lt;p>In the case of the normal distributions, these estimators are consistent.
However, this post will show an example of multimodal distributions
that lead to opposite signs of $\DSM$ and $\DHL$.&lt;/p></description></item><item><title>Resistance to the low-density regions: the Hodges-Lehmann location estimator</title><link>https://aakinshin.net/posts/rldr-hl/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rldr-hl/</guid><description>&lt;p>In the previous posts, I discussed the concept of a resistance function
that shows the sensitivity of the given estimator to the low-density regions.
I already showed how this function behaves for &lt;a href="https://aakinshin.net/posts/rldr-mean-median/">the mean, the sample median&lt;/a>,
and &lt;a href="https://aakinshin.net/posts/rldr-hdmedian/">the Harrell-Davis median&lt;/a>.
In this post, I explore this function for the Hodges-Lehmann location estimator.&lt;/p></description></item><item><title>Hodges-Lehmann-Sen shift and shift confidence interval estimators</title><link>https://aakinshin.net/posts/hodges-lehmann-sen-shift-ci/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-sen-shift-ci/</guid><description>&lt;p>In the previous two posts
(&lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency1/">1&lt;/a>, &lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency2/">2&lt;/a>),
I discussed the Hodges-Lehmann median estimator.
The suggested idea of getting median estimations based on a cartesian product
could be adopted to estimate the shift between two samples.
In this post, we discuss how to build Hodges-Lehmann-Sen shift estimator
and how to get confidence intervals for the obtained estimations.
Also, we perform a simulation study that checks the actual coverage percentage of these intervals.&lt;/p></description></item><item><title>Statistical efficiency of the Hodges-Lehmann median estimator, Part 2</title><link>https://aakinshin.net/posts/hodges-lehmann-efficiency2/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-efficiency2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/hodges-lehmann-efficiency1/">previous post&lt;/a>,
we evaluated the relative statistical efficiency of the Hodges-Lehmann median estimator
against the sample median under the normal distribution.
In this post, we extended this experiment to a set of various light-tailed and heavy-tailed distributions.&lt;/p></description></item><item><title>Statistical efficiency of the Hodges-Lehmann median estimator, Part 1</title><link>https://aakinshin.net/posts/hodges-lehmann-efficiency1/</link><pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hodges-lehmann-efficiency1/</guid><description>&lt;p>In this post, we evaluate the relative statistical efficiency of
the &lt;a href="https://aakinshin.net/tags/hodges-lehmann-estimator/">Hodges-Lehmann median estimator&lt;/a>
against the sample median under the normal distribution.
We also compare it with the efficiency of the Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>A Note on the Hodges–Lehmann Estimator</title><link>https://aakinshin.net/library/papers/rosenkranz2010/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/rosenkranz2010/</guid><description>Reference Gerd K Rosenkranz “A note on the Hodges–Lehmann estimator” (2010) // Pharmaceutical Statistics. Vol. 9. No 2. Pp. 162–167. DOI: 10.1002/pst.387
Abstract The Hodges-Lehmann estimator was originally developed as a non-parametric estimator of a shift parameter. As it is widely used in statistical applications, the question is investigated what it is estimating if the shift model does not hold. It is shown that for data whose distributions are symmetric about their median the Hodges–Lehmann estimator based on the Wilcoxon Rank Sum test estimates the difference between the medians of the distributions. This result does generally not hold if the symmetry assumption is violated.
Bib @Article{rosenkranz2010, title = {A note on the Hodges–Lehmann estimator}, volume = {9}, issn = {1539-1604, 1539-1612}, url = {https://onlinelibrary.wiley.com/doi/10.1002/pst.387}, doi = {10.1002/pst.387}, abstract = {The Hodges-Lehmann estimator was originally developed as a non-parametric estimator of a shift parameter. As it is widely used in statistical applications, the question is investigated what it is estimating if the shift model does not hold. It is shown that for data whose distributions are symmetric about their median the Hodges–Lehmann estimator based on the Wilcoxon Rank Sum test estimates the difference between the medians of the distributions. This result does generally not hold if the symmetry assumption is violated.}, language = {en}, number = {2}, urldate = {2023-12-10}, journal = {Pharmaceutical Statistics}, author = {Rosenkranz, Gerd K}, month = {apr}, year = {2010}, pages = {162--167} }</description></item><item><title>Estimates of Location Based on Rank Tests</title><link>https://aakinshin.net/library/papers/hodges1963/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hodges1963/</guid><description>Reference J L Hodges, E L Lehmann “Estimates of Location Based on Rank Tests” (1963) // The Annals of Mathematical Statistics. Vol. 34. No 2. Pp. 598–611. DOI: 10.1214/aoms/1177704172
Bib @Article{hodges1963, title = {Estimates of Location Based on Rank Tests}, volume = {34}, issn = {0003-4851}, url = {http://projecteuclid.org/euclid.aoms/1177704172}, doi = {10.1214/aoms/1177704172}, language = {en}, number = {2}, urldate = {2022-05-31}, journal = {The Annals of Mathematical Statistics}, author = {Hodges, J L and Lehmann, E L}, month = {jun}, year = {1963}, pages = {598--611} }</description></item><item><title>Nonparametric Confidence Intervals for a Shift Parameter</title><link>https://aakinshin.net/library/papers/lehmann2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/lehmann2011/</guid><description>Reference E. L. Lehmann “Nonparametric Confidence Intervals for a Shift Parameter” (2011) // Selected Works of E. L. Lehmann. Publisher: Springer US. ISBN: 9781461414124. Pp. 459–464. DOI: 10.1007/978-1-4614-1412-4_41
Bib @Inbook{lehmann2011, title = {Nonparametric Confidence Intervals for a Shift Parameter}, isbn = {9781461414124}, url = {http://dx.doi.org/10.1007/978-1-4614-1412-4_41}, doi = {10.1007/978-1-4614-1412-4_41}, booktitle = {Selected Works of E. L. Lehmann}, publisher = {Springer US}, author = {Lehmann, E. L.}, year = {2011}, month = {nov}, pages = {459–464} }</description></item></channel></rss>