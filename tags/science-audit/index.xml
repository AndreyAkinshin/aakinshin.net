<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Science Audit on Andrey Akinshin</title><link>https://aakinshin.net/tags/science-audit/</link><description>Recent content in Science Audit on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 31 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/science-audit/index.xml" rel="self" type="application/rss+xml"/><item><title>Debunking the myth about ozone holes, NASA, and outlier removal</title><link>https://aakinshin.net/posts/outliers-ozon-holes/</link><pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/outliers-ozon-holes/</guid><description>&lt;p>Imagine you work with some data and assume that the underlying distribution is approximately normal.
In such cases, the data analysis typically involves non-robust statistics like the mean and the standard deviation.
While these metrics are highly efficient under normality, they make the analysis procedure fragile:
a single extreme value can corrupt all the results.
You may not expect any significant outliers, but you can never be 100% sure.
To avoid surprises and ensure the reliability of the results,
it may be tempting to automatically exclude all outliers from the collected samples.
While this approach is widely adopted, it conceals an essential part of the obtained data
and can lead to fallacious conclusions.&lt;/p>
&lt;p>Let me recite a classic story about ozone holes from &lt;a href="https://aakinshin.net/library/papers/kandel1990/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#paper">&lt;/use>&lt;/svg>kandel1990&lt;/a>,
which is typically used to illustrate the danger of blind outlier removal:&lt;/p></description></item><item><title>A Decade of Reversal: An Analysis of 146 Contradicted Medical Practices</title><link>https://aakinshin.net/library/papers/prasad2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/prasad2013/</guid><description>Reference Vinay Prasad, Andrae Vandross, Caitlin Toomey, Michael Cheung, Jason Rho, Steven Quinn, Satish Jacob Chacko, Durga Borkar, Victor Gall, Senthil Selvaraj, Nancy Ho, Adam Cifu “A Decade of Reversal: An Analysis of 146 Contradicted Medical Practices” (2013) // Mayo Clinic Proceedings. Publisher: Elsevier BV. Vol. 88. No 8. Pp. 790–798. DOI: 10.1016/j.mayocp.2013.05.012
Bib @Article{prasad2013, title = {A Decade of Reversal: An Analysis of 146 Contradicted Medical Practices}, volume = {88}, issn = {0025-6196}, url = {http://dx.doi.org/10.1016/j.mayocp.2013.05.012}, doi = {10.1016/j.mayocp.2013.05.012}, number = {8}, journal = {Mayo Clinic Proceedings}, publisher = {Elsevier BV}, author = {Prasad, Vinay and Vandross, Andrae and Toomey, Caitlin and Cheung, Michael and Rho, Jason and Quinn, Steven and Chacko, Satish Jacob and Borkar, Durga and Gall, Victor and Selvaraj, Senthil and Ho, Nancy and Cifu, Adam}, year = {2013}, month = {aug}, pages = {790–798} }</description></item><item><title>Addressing False Positives in Self-Report Surveys</title><link>https://aakinshin.net/library/quotes/6c23bac3-f06b-4234-8b0d-eb4c29388360/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/6c23bac3-f06b-4234-8b0d-eb4c29388360/</guid><description>Self-report surveys of rare events easily lead to huge overestimates of the true incidence of such events, particularly if the event in question has some potential social desirability. Researchers who claim that such survey incidence data are accurate must show how they have eliminated the enormous problem of false positives.</description></item><item><title>Analysis of Variance in Genetic Associations and Health Care Interventions</title><link>https://aakinshin.net/library/quotes/c84cec6e-c9dd-4ea2-b87f-516d09824fbf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c84cec6e-c9dd-4ea2-b87f-516d09824fbf/</guid><description>The maximal between-study variance was more likely to be recorded early in the 44 eligible meta-analyses of genetic associations than in the 37 meta-analyses of health care interventions (P = .013). At the time of the first heterogeneity assessment, the most favorable-ever result in support of a specific association was more likely to appear than the least favorable-ever result (22 vs. 10, P = .017); the opposite was seen at the second heterogeneity assessment (15 vs. 5, P = .031). Such a sequence of extreme opposite results was not seen in the clinical trials meta-analyses. The estimated between-study variance decreased over time in genetic association studies (P = .010), but not in clinical trials (P = .30).</description></item><item><title>Balancing Realism and Purity</title><link>https://aakinshin.net/library/quotes/c940f25b-3f02-40ea-8f2c-8934efe03226/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c940f25b-3f02-40ea-8f2c-8934efe03226/</guid><description>It is difficult to give an objective assessment of the validity of a statistical analysis. Rather like medicine, statistics is an art, and there is rarely a unique correct approach. Rather judgement is necessary to select one of a number of possible analyses, each with their own advantages and limitations. In my review I encountered many analyses which I would have tackled differently, but where in my judgement the analysis as presented was perfectly acceptable. In writing this article I have attempted to confine my criticisms to points where I believe that the vast majority of statisticians would agree that the approach adopted was not acceptable, and indeed many statisticians would probably take a harder line than I have. My statistical philosophy leans towards being a realist rather than a purist, and my research interests lie in the area of how to obtain the least biased results possible in areas where perhaps for ethical or practical reasons randomization is not possible, or where missing data abound.</description></item><item><title>Black and White Medical Conclusions</title><link>https://aakinshin.net/library/quotes/61d2c118-762a-4f38-8710-231759813a08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/61d2c118-762a-4f38-8710-231759813a08/</guid><description>It appears that medical authors feel the need to make black and white conclusions when their data almost never allows for such dichotomous statements. This is particularly true when comparing results to similar studies with largely overlapping CIs. Virtually all of the conclusion confusion discussed in this paper can be linked to slavish adherence to an arbitrary threshold for statistical significance. Even if the threshold is reasonable, it still cannot be used to make dichotomous conclusions.</description></item><item><title>Consequences of Dichotomization</title><link>https://aakinshin.net/library/papers/fedorov2008/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/fedorov2008/</guid><description>Reference Valerii Fedorov, Frank Mannino, Rongmei Zhang “Consequences of dichotomization” (2008) // Pharmaceutical Statistics. Publisher: Wiley. Vol. 8. No 1. Pp. 50–61. DOI: 10.1002/pst.331
Bib @Article{fedorov2008, title = {Consequences of dichotomization}, volume = {8}, issn = {1539-1612}, url = {http://dx.doi.org/10.1002/pst.331}, doi = {10.1002/pst.331}, number = {1}, journal = {Pharmaceutical Statistics}, publisher = {Wiley}, author = {Fedorov, Valerii and Mannino, Frank and Zhang, Rongmei}, year = {2008}, month = {apr}, pages = {50–61} }</description></item><item><title>Contradicted and Initially Stronger Effects in Highly Cited Clinical Research</title><link>https://aakinshin.net/library/papers/ioannidis2005a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005a/</guid><description>Reference John PA Ioannidis “Contradicted and Initially Stronger Effects in Highly Cited Clinical Research” (2005) // JAMA. Publisher: American Medical Association (AMA). Vol. 294. No 2. Pp. 218. DOI: 10.1001/jama.294.2.218
Bib @Article{ioannidis2005a, title = {Contradicted and Initially Stronger Effects in Highly Cited Clinical Research}, volume = {294}, issn = {0098-7484}, url = {http://dx.doi.org/10.1001/jama.294.2.218}, doi = {10.1001/jama.294.2.218}, number = {2}, journal = {JAMA}, publisher = {American Medical Association (AMA)}, author = {Ioannidis, John PA}, year = {2005}, month = {jul}, pages = {218} }</description></item><item><title>Debunking the Stanford Prison Experiment</title><link>https://aakinshin.net/library/papers/texier2019/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/texier2019/</guid><description>Reference Thibault Le Texier “Debunking the Stanford Prison Experiment” (2019) // American Psychologist. Publisher: American Psychological Association (APA). Vol. 74. No 7. Pp. 823–839. DOI: 10.1037/amp0000401
Bib @Article{texier2019, title = {Debunking the Stanford Prison Experiment}, volume = {74}, issn = {0003-066X}, url = {http://dx.doi.org/10.1037/amp0000401}, doi = {10.1037/amp0000401}, number = {7}, journal = {American Psychologist}, publisher = {American Psychological Association (APA)}, author = {Texier, Thibault Le}, year = {2019}, month = {oct}, pages = {823–839}, custom-url-pdf = {http://letexier.org/IMG/pdf/LeTexier_Debunking-the-SPE_American-Psychologist_2019.pdf} }</description></item><item><title>Dichotomization Should Be Avoided in Most Cases</title><link>https://aakinshin.net/library/quotes/8cb30d74-c4bb-470b-b52f-eab706477a22/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8cb30d74-c4bb-470b-b52f-eab706477a22/</guid><description>The knowledge of losing information from dichotomizing a continuous outcome is nothing new. However, many previous writings report on the optimal choice of cut points, which depends upon the parameters we wish to estimate. If we are lucky, the chosen cut point is near the optimal point, but the consequences of dichotomizing become more dire as we deviate from the optimal point. We focus our study on the evaluation of losses caused by dichotomization given cut points. While the analysis of dichotomized outcomes may be easier, there are no benefits to this approach when the true outcomes can be observed and the ‘working’ model is flexible enough to describe the population at hand. Thus, dichotomization should be avoided in most cases. Only when we wish to estimate a CDF value, our working model poorly approximates reality, and our sample size is large will the biasedness of model-based estimators overpower the improvement in variance. In this case, the dichotomized estimator may lead to better results, but further study-specific consideration is needed. We also want to emphasize that while analysis should be done using actual outcomes, some aspects of this analysis can be reported on a dichotomized scale.</description></item><item><title>Early Extreme Contradictory Estimates May Appear in Published research: The Proteus Phenomenon in Molecular Genetics Research and Randomized Trials</title><link>https://aakinshin.net/library/papers/ioannidis2005b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005b/</guid><description>Reference John PA Ioannidis, Thomas A Trikalinos “Early extreme contradictory estimates may appear in published research: The Proteus phenomenon in molecular genetics research and randomized trials” (2005) // Journal of Clinical Epidemiology. Publisher: Elsevier BV. Vol. 58. No 6. Pp. 543–549. DOI: 10.1016/j.jclinepi.2004.10.019
Bib @Article{ioannidis2005b, title = {Early extreme contradictory estimates may appear in published research: The Proteus phenomenon in molecular genetics research and randomized trials}, volume = {58}, issn = {0895-4356}, url = {http://dx.doi.org/10.1016/j.jclinepi.2004.10.019}, doi = {10.1016/j.jclinepi.2004.10.019}, number = {6}, journal = {Journal of Clinical Epidemiology}, publisher = {Elsevier BV}, author = {Ioannidis, John PA and Trikalinos, Thomas A}, year = {2005}, month = {jun}, pages = {543–549} }</description></item><item><title>Easy to Publish</title><link>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</guid><description>Our job as scientists is to discover truths about the world. We generate hypotheses, collect data, and examine whether or not the data are consistent with those hypotheses. Although we aspire to always be accurate, errors are inevitable. Perhaps the most costly error is a false positive, the incorrect rejection of a null hypothesis. First, once they appear in the literature, false positives are particularly persistent. Because null results have many possible causes, failures to replicate previous findings are never conclusive. Furthermore, because it is uncommon for prestigious journals to publish null findings or exact replications, researchers have little incentive to even attempt them. Second, false positives waste resources: They inspire investment in fruitless research programs and can lead to ineffective policy changes. Finally, a field known for publishing false positives risks losing its credibility. In this article, we show that despite the nominal endorsement of a maximum false-positive rate of 5% (i.e., p ≤ .05), current standards for disclosing details of data collection and analyses make false positives vastly more likely. In fact, it is unacceptably easy to publish “statistically significant” evidence consistent with any hypothesis.
(Emphasis is mine.)</description></item><item><title>Efficiency Loss of Dichotomization</title><link>https://aakinshin.net/library/quotes/834262d6-ba81-46ea-a08a-952fafecc22c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/834262d6-ba81-46ea-a08a-952fafecc22c/</guid><description>Dichotomization is the transformation of a continuous outcome (response) to a binary outcome. This approach, while somewhat common, is harmful from the viewpoint of statistical estimation and hypothesis testing. We show that this leads to loss of information, which can be large. For normally distributed data, this loss in terms of Fisher’s information is at least $1-2/\pi$ (or 36%). In other words, 100 continuous observations are statistically equivalent to 158 dichotomized observations. The amount of information lost depends greatly on the prior choice of cut points, with the optimal cut point depending upon the unknown parameters. The loss of information leads to loss of power or conversely a sample size increase to maintain power. Only in certain cases, for instance, in estimating a value of the cumulative distribution function and when the assumed model is very different from the true model, can the use of dichotomized outcomes be considered a reasonable approach.</description></item><item><title>Erroneous Analyses of Interactions in neuroscience: A Problem of Significance</title><link>https://aakinshin.net/library/papers/nieuwenhuis2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/nieuwenhuis2011/</guid><description>Reference Sander Nieuwenhuis, Birte U Forstmann, Eric-Jan Wagenmakers “Erroneous analyses of interactions in neuroscience: a problem of significance” (2011) // Nature Neuroscience. Publisher: Springer Science and Business Media LLC. Vol. 14. No 9. Pp. 1105–1107. DOI: 10.1038/nn.2886
Bib @Article{nieuwenhuis2011, title = {Erroneous analyses of interactions in neuroscience: a problem of significance}, volume = {14}, issn = {1546-1726}, url = {http://dx.doi.org/10.1038/nn.2886}, doi = {10.1038/nn.2886}, number = {9}, journal = {Nature Neuroscience}, publisher = {Springer Science and Business Media LLC}, author = {Nieuwenhuis, Sander and Forstmann, Birte U and Wagenmakers, Eric-Jan}, year = {2011}, month = {aug}, pages = {1105–1107} }</description></item><item><title>Evaluating the Reliability of Highly Cited Clinical Research Studies</title><link>https://aakinshin.net/library/quotes/3862f143-7b54-472a-b201-83407f62ac6c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3862f143-7b54-472a-b201-83407f62ac6c/</guid><description>Of 49 highly cited original clinical research studies, 45 claimed that the intervention was effective. Of these, 7 (16%) were contradicted by subsequent studies, 7 others (16%) had found effects that were stronger than those of subsequent studies, 20 (44%) were replicated, and 11 (24%) remained largely unchallenged. Five of 6 highlycited nonrandomized studies had been contradicted or had found stronger effects vs 9 of 39 randomized controlled trials (P=.008). Among randomized trials, studies with contradicted or stronger effects were smaller (P=.009) than replicated or unchallenged studies although there was no statistically significant difference in their early or overall citation impact. Matched control studies did not have a significantly different share of refuted results than highly cited studies, but they included more studies with “negative” results.</description></item><item><title>False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant</title><link>https://aakinshin.net/library/papers/simmons2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/simmons2011/</guid><description>Reference Joseph P Simmons, Leif D Nelson, Uri Simonsohn “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant” (2011) // Psychological Science. Publisher: SAGE Publications. Vol. 22. No 11. Pp. 1359–1366. DOI: 10.1177/0956797611417632
Bib @Article{simmons2011, title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}, volume = {22}, issn = {1467-9280}, url = {http://dx.doi.org/10.1177/0956797611417632}, doi = {10.1177/0956797611417632}, number = {11}, journal = {Psychological Science}, publisher = {SAGE Publications}, author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri}, year = {2011}, month = {oct}, pages = {1359–1366} }</description></item><item><title>Guidelines for Reviewers</title><link>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</guid><description>We propose the following four guidelines for reviewers.
Reviewers should ensure that authors follow the requirements. Review teams are the gatekeepers of the scientific community, and they should encourage authors not only to rule out alternative explanations, but also to more convincingly demonstrate that their findings are not due to chance alone. This means prioritizing transparency over tidiness; if a wonderful study is partially marred by a peculiar exclusion or an inconsistent condition, those imperfections should be retained. If reviewers require authors to follow these requirements, they will. Reviewers should be more tolerant of imperfections in results. One reason researchers exploit researcher degrees of freedom is the unreasonable expectation we often impose as reviewers for every data pattern to be (significantly) as predicted. Underpowered studies with perfect results are the ones that should invite extra scrutiny. Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions. Even if authors follow all of our guidelines, they will necessarily still face arbitrary decisions. For example, should they subtract the baseline measure of the dependent variable from the final result or should they use the baseline measure as a covariate? When there is no obviously correct way to answer questions like this, the reviewer should ask for alternatives. For example, reviewer reports might include questions such as, “Do the results also hold if the baseline measure is instead used as a covariate?” Similarly, reviewers should ensure that arbitrary decisions are used consistently across studies (e.g., “Do the results hold for Study 3 if gender is entered as a covariate, as was done in Study 2?”).5 If a result holds only for one arbitrary specification, then everyone involved has learned a great deal about the robustness (or lack thereof) of the effect. If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication.</description></item><item><title>How Confidence Intervals Become Confusion Intervals</title><link>https://aakinshin.net/library/papers/mccormack2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/mccormack2013/</guid><description>Reference James McCormack, Ben Vandermeer, G Michael Allan “How confidence intervals become confusion intervals” (2013) // BMC Medical Research Methodology. Publisher: Springer Science and Business Media LLC. Vol. 13. No 1. DOI: 10.1186/1471-2288-13-134
Abstract In this paper, we review how researchers can look at very similar data yet have completely different conclusions based purely on an over-reliance of statistical significance and an unclear understanding of confidence intervals. The dogmatic adherence to statistical significant thresholds can lead authors to write dichotomized absolute conclusions while ignoring the broader interpretations of very consistent findings. We describe three examples of controversy around the potential benefit of a medication, a comparison between new medications, and a medication with a potential harm. The examples include the highest levels of evidence, both meta-analyses and randomized controlled trials. We will show how in each case the confidence intervals and point estimates were very similar. The only identifiable differences to account for the contrasting conclusions arise from the serendipitous finding of confidence intervals that either marginally cross or just fail to cross the line of statistical significance.
Bib @Article{mccormack2013, title = {How confidence intervals become confusion intervals}, abstract = {In this paper, we review how researchers can look at very similar data yet have completely different conclusions based purely on an over-reliance of statistical significance and an unclear understanding of confidence intervals. The dogmatic adherence to statistical significant thresholds can lead authors to write dichotomized absolute conclusions while ignoring the broader interpretations of very consistent findings. We describe three examples of controversy around the potential benefit of a medication, a comparison between new medications, and a medication with a potential harm. The examples include the highest levels of evidence, both meta-analyses and randomized controlled trials. We will show how in each case the confidence intervals and point estimates were very similar.</description></item><item><title>How to Lie with Statistics</title><link>https://aakinshin.net/library/books/huff-how-to-lie-with-statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/books/huff-how-to-lie-with-statistics/</guid><description/></item><item><title>Incorrect Statistical Procedures in Neuroscience</title><link>https://aakinshin.net/library/quotes/4e6b0a2d-9a6c-4b97-9c15-b7009ef063a5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/4e6b0a2d-9a6c-4b97-9c15-b7009ef063a5/</guid><description>In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P &amp;lt; 0.05) but the other is not (P &amp;gt; 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. An additional analysis suggests that incorrect analyses of interactions are even more common in cellular and molecular neuroscience.</description></item><item><title>Is Molecular Profiling Ready for Use in Clinical Decision Making?</title><link>https://aakinshin.net/library/papers/ioannidis2007a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2007a/</guid><description>Reference John PA Ioannidis “Is Molecular Profiling Ready for Use in Clinical Decision Making?” (2007) // The Oncologist. Publisher: Oxford University Press (OUP). Vol. 12. No 3. Pp. 301–311. DOI: 10.1634/theoncologist.12-3-301
Abstract Molecular profiling, the classification of tissue or other specimens for diagnostic, prognostic, and predictive purposes based on multiple gene expression, is a technology that holds major promise for optimizing the management of patients with cancer. However, the use of these tests for clinical decision making presents many challenges to overcome. Assay development and data analysis in this field have been largely exploratory, and leave numerous possibilities for the introduction of bias. Standardization of profiles remains the exception. Classifier performance is usually overinterpreted by presenting the results as p-values or multiplicative effects (e.g., relative risks), while the absolute sensitivity and specificity of classification remain modest at best, especially when tested in large validation samples. Validation has often been done with suboptimal attention to methodology and protection from bias. The postulated classifier performance may be inflated compared to what these profiles can achieve. With the exception of breast cancer, we have little evidence about the incremental discrimination that molecular profiles can provide versus classic risk factors alone. Clinical trials have started to evaluate the utility of using molecular profiles for breast cancer management. Until we obtain data from these trials, the impact of these tests and the net benefit under real-life settings remain unknown. Optimal incorporation into clinical practice is not straightforward. Finally, cost-effectiveness is difficult to appreciate until these other challenges are addressed. Overall, molecular profiling is a fascinating and promising technology, but its incorporation into clinical decision making requires careful planning and robust evidence.
Bib @Article{ioannidis2007a, title = {Is Molecular Profiling Ready for Use in Clinical Decision Making?}, abstract = {Molecular profiling, the classification of tissue or other specimens for diagnostic, prognostic, and predictive purposes based on multiple gene expression, is a technology that holds major promise for optimizing the management of patients with cancer.</description></item><item><title>Magnitude of Effects in Clinical Trials Published in high-impact General Medical Journals</title><link>https://aakinshin.net/library/papers/siontis2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/siontis2011/</guid><description>Reference Konstantinos CM Siontis, Evangelos Evangelou, John PA Ioannidis “Magnitude of effects in clinical trials published in high-impact general medical journals” (2011) // International Journal of Epidemiology. Publisher: Oxford University Press (OUP). Vol. 40. No 5. Pp. 1280–1291. DOI: 10.1093/ije/dyr095
Bib @Article{siontis2011, title = {Magnitude of effects in clinical trials published in high-impact general medical journals}, volume = {40}, issn = {0300-5771}, url = {http://dx.doi.org/10.1093/ije/dyr095}, doi = {10.1093/ije/dyr095}, number = {5}, journal = {International Journal of Epidemiology}, publisher = {Oxford University Press (OUP)}, author = {Siontis, Konstantinos CM and Evangelou, Evangelos and Ioannidis, John PA}, year = {2011}, month = {sep}, pages = {1280–1291} }</description></item><item><title>Matthew Walker's 'Why We Sleep' Is Riddled with Scientific and Factual Errors</title><link>https://aakinshin.net/library/web/3e48c9f4f48b4a81806edc7d5e21690a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/web/3e48c9f4f48b4a81806edc7d5e21690a/</guid><description>Excellent write-up by Alexey Guzey who scrupulously reviewed part of Why We Sleep and listed the issues.</description></item><item><title>Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling</title><link>https://aakinshin.net/library/papers/john2012/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/john2012/</guid><description>Reference Leslie K John, George Loewenstein, Drazen Prelec “Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling” (2012) // Psychological Science. Publisher: SAGE Publications. Vol. 23. No 5. Pp. 524–532. DOI: 10.1177/0956797611430953
Bib @Article{john2012, title = {Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}, volume = {23}, issn = {1467-9280}, url = {http://dx.doi.org/10.1177/0956797611430953}, doi = {10.1177/0956797611430953}, number = {5}, journal = {Psychological Science}, publisher = {SAGE Publications}, author = {John, Leslie K and Loewenstein, George and Prelec, Drazen}, year = {2012}, month = {apr}, pages = {524–532} }</description></item><item><title>Neural Correlates of Interspecies Perspective Taking in the post-mortem Atlantic Salmon: An Argument for Multiple Comparisons Correction</title><link>https://aakinshin.net/library/papers/bennett2009/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/bennett2009/</guid><description>One of my favorite examples of statistics misuse.
Reference Craig M Bennett, Michael B Miller, George L Wolford “Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction” (2009) // NeuroImage. Publisher: Elsevier BV. Vol. 47. Pp. S125. DOI: 10.1016/s1053-8119(09)71202-9
Bib @Article{bennett2009, title = {Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction}, volume = {47}, issn = {1053-8119}, url = {http://dx.doi.org/10.1016/S1053-8119(09)71202-9}, doi = {10.1016/s1053-8119(09)71202-9}, journal = {NeuroImage}, publisher = {Elsevier BV}, author = {Bennett, Craig M and Miller, Michael B and Wolford, George L}, year = {2009}, month = {jul}, pages = {S125} }</description></item><item><title>Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists</title><link>https://aakinshin.net/library/papers/mitroff1974/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/mitroff1974/</guid><description>Reference Ian I. Mitroff “Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists” (1974) // American Sociological Review. Publisher: SAGE Publications. Vol. 39. No 4. Pp. 579. DOI: 10.2307/2094423
Abstract This paper describes a three and a half year study conducted over the course of the Apollo lunar missions with forty-two of the most prestigiousscientists who studiedthe lunar rocks. Thepapersupportsthe Merton-E.Barberconceptof sociologicalambivalence,that social institutions reflect potentially conflicting sets of norms. The paper offers a set of counter-normsfor science, arguingthat if the norm of universalismis rooted in the impersonal characterof science, an opposing counter-normis rooted in the personalcharacterof science. Thepaperalso arguesthat not only is sociologicalambivalencea characteristicof science,but it seemsnecessaryfor the existenceand ultimaterationalityof science.
Bib @Article{mitroff1974, title = {Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists}, abstract = {This paper describes a three and a half year study conducted over the course of the Apollo lunar missions with forty-two of the most prestigiousscientists who studiedthe lunar rocks. Thepapersupportsthe Merton-E.Barberconceptof sociologicalambivalence,that social institutions reflect potentially conflicting sets of norms. The paper offers a set of counter-normsfor science, arguingthat if the norm of universalismis rooted in the impersonal characterof science, an opposing counter-normis rooted in the personalcharacterof science. Thepaperalso arguesthat not only is sociologicalambivalencea characteristicof science,but it seemsnecessaryfor the existenceand ultimaterationalityof science.}, volume = {39}, issn = {0003-1224}, url = {http://dx.doi.org/10.2307/2094423}, doi = {10.2307/2094423}, number = {4}, journal = {American Sociological Review}, publisher = {SAGE Publications}, author = {Mitroff, Ian I.}, year = {1974}, month = {aug}, pages = {579} }</description></item><item><title>Questionable Practices</title><link>https://aakinshin.net/library/quotes/a40b9984-ae39-4f04-821b-42f8fabf5719/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a40b9984-ae39-4f04-821b-42f8fabf5719/</guid><description>Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.</description></item><item><title>Recommendations on Presenting of Statistical Results in Medical Literature</title><link>https://aakinshin.net/library/quotes/10d26f39-ca23-4d67-8a99-baa4b9c0211f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/10d26f39-ca23-4d67-8a99-baa4b9c0211f/</guid><description>We encourage authors to avoid statements like “X has no effect on mortality” as they are likely to be both untrue and misleading. This is especially true as results get “close” to being statistically significant. Results should speak for themselves. For that to happen, readers (clinicians and science reporters) need to understand the language of statistics and approach authors’ conclusions with a critical eye. We are not trying to say that the reader should not review the abstract but when authors’ conclusions differ from others, readers must examine and compare the actual results. In fact, all but one of the meta-analyses provided point estimates and CIs in the abstracts. This facilitates quick comparisons to other studies reported to be “completely different,” and to determine if the CIs demonstrate clinically important differences. The problem lies in the authors’ conclusions, which often have little to do with their results but rather what they want the results to show. We encourage journal editors to challenge authors’ conclusions, particularly when they argue they have found something unique or different than other researchers but the difference is based solely on tiny variations in CIs or p-value (statistically significant or not).
We are not suggesting the elimination of statistical testing or statistical significance, but rather that all people (authors, publishers, regulators etc.) who write about medical interventions use common sense and good judgment when presenting results that differ from others and not be so beholden to the “magical” statistical significance level of 0.05. We urge them to consider the degree to which the results of the “differing” study overlap with their own, the true difference in the point estimates and range of possible effects, where the preponderance of the effect lies and how clinicians might apply the evidence.
It appears that readers of the papers discussed here would be better served by reviewing the actual results than reading the authors’ conclusions.</description></item><item><title>Requirements for Authors</title><link>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</guid><description>We propose the following six requirements for authors.
Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. Following this requirement may mean reporting the outcome of power calculations or disclosing arbitrary rules, such as “we decided to collect 100 observations” or “we decided to collect as many observations as we could before the end of the semester.” The rule itself is secondary, but it must be determined ex ante and be reported. Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification. This requirement offers extra protection for the first requirement. Samples smaller than 20 per cell are simply not powerful enough to detect most effects, and so there is usually no good reason to decide in advance to collect such a small number of observations. Smaller samples, it follows, are much more likely to reflect interim data analysis and a flexible termination rule. In addition, as Figure 1 shows, larger minimum sample sizes can lessen the impact of violating Requirement 1. Authors must list all variables collected in a study. This requirement prevents researchers from reporting only a convenient subset of the many measures that were collected, allowing readers and reviewers to easily identify possible researcher degrees of freedom. Because authors are required to just list those variables rather than describe them in detail, this requirement increases the length of an article by only a few words per otherwise shrouded variable. We encourage authors to begin the list with “only,” to assure readers that the list is exhaustive (e.g., “participants reported only their age and gender”). Authors must report all experimental conditions, including failed manipulations. This requirement prevents authors from selectively choosing only to report the condition comparisons that yield results that are consistent with their hypothesis.</description></item><item><title>Same Data, Different Results</title><link>https://aakinshin.net/library/quotes/6e3a83e1-2574-4f7f-a436-579ec133c468/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/6e3a83e1-2574-4f7f-a436-579ec133c468/</guid><description>Most published reports of clinical studies begin with an abstract – likely the first and perhaps only thing many clinicians, the media and patients will read. Within that abstract, authors/investigators typically provide a brief summary of the results and a 1–2 sentence conclusion. At times, the conclusion of one study will be different, even diametrically opposed, to another despite the authors looking at similar data. In these cases, readers may assume that these individual authors somehow found dramatically different results. While these reported differences may be true some of the time, radically diverse conclusions and ensuing controversies may simply be due to tiny differences in confidence intervals combined with an over-reliance and misunderstanding of a “statistically significant difference.” Unfortunately, this misunderstanding can lead to therapeutic uncertainty for front-line clinicians when in fact the overall data on a particular issue is remarkably consistent.</description></item><item><title>Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test</title><link>https://aakinshin.net/library/papers/berkson1938/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/berkson1938/</guid><description>Reference Joseph Berkson “Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test” (1938) // Journal of the American Statistical Association. Publisher: JSTOR. Vol. 33. No 203. Pp. 526. DOI: 10.2307/2279690
Bib @Article{berkson1938, title = {Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test}, volume = {33}, issn = {0162-1459}, url = {http://dx.doi.org/10.2307/2279690}, doi = {10.2307/2279690}, number = {203}, journal = {Journal of the American Statistical Association}, publisher = {JSTOR}, author = {Berkson, Joseph}, year = {1938}, month = {sep}, pages = {526} }</description></item><item><title>Statistics Done Wrong: The Woefully Complete Guide</title><link>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</guid><description/></item><item><title>Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates</title><link>https://aakinshin.net/library/papers/hemenway1997/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hemenway1997/</guid><description>Reference David Hemenway “Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates” (1997) // The Journal of Criminal Law and Criminology (1973-). Publisher: JSTOR. Vol. 87. No 4. Pp. 1430. DOI: 10.2307/1144020
Abstract Gary Kleck and Marc Gertz conducted a survey of civilian defensive gun use in 1992. In 1993, Kleck began publicizing the estimate that civilians use guns in self-defense against offenders up to 2.5 million times each year.&amp;rsquo; This figure has been widely used by the National Rifle Association and by gun advocates. It is also often cited in the media and even in Congress. The Kleck and Gertz (K-G) paper has now been published. It is clear, however, that its conclusions cannot be accepted as valid.
Bib @Article{hemenway1997, title = {Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates}, abstract = {Gary Kleck and Marc Gertz conducted a survey of civilian defensive gun use in 1992. In 1993, Kleck began publicizing the estimate that civilians use guns in self-defense against offenders up to 2.5 million times each year.&amp;#39; This figure has been widely used by the National Rifle Association and by gun advocates. It is also often cited in the media and even in Congress. The Kleck and Gertz (K-G) paper has now been published. It is clear, however, that its conclusions cannot be accepted as valid.}, volume = {87}, issn = {0091-4169}, url = {http://dx.doi.org/10.2307/1144020}, doi = {10.2307/1144020}, number = {4}, journal = {The Journal of Criminal Law and Criminology (1973-)}, publisher = {JSTOR}, author = {Hemenway, David}, year = {1997}, pages = {1430} }</description></item><item><title>Testing the Approximate Validity of Statistical Hypotheses</title><link>https://aakinshin.net/library/papers/hodges1954/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hodges1954/</guid><description>Dicussed P-Value Beyond Any Usual Limit of Significance from berkson1938.
Reference J. L. Hodges, E. L. Lehmann “Testing the Approximate Validity of Statistical Hypotheses” (1954) // Journal of the Royal Statistical Society: Series B (Methodological). Publisher: Wiley. Vol. 16. No 2. Pp. 261–268. DOI: 10.1111/j.2517-6161.1954.tb00169.x
Abstract The distinction between statistical significance and material significance in hypotheses testing is discussed. Modifications of the customary tests, in order to test for the absence of material significance, are derived for several parametric problems, for the chi-square test of goodness of fit, and for Student&amp;rsquo;s hypothesis. The latter permits one to test the hypothesis that the means of two normal populations of equal variance, do not differ by more than a stated amount.
Bib @Article{hodges1954, title = {Testing the Approximate Validity of Statistical Hypotheses}, abstract = {The distinction between statistical significance and material significance in hypotheses testing is discussed. Modifications of the customary tests, in order to test for the absence of material significance, are derived for several parametric problems, for the chi-square test of goodness of fit, and for Student&amp;#39;s hypothesis. The latter permits one to test the hypothesis that the means of two normal populations of equal variance, do not differ by more than a stated amount.}, volume = {16}, issn = {2517-6161}, url = {http://dx.doi.org/10.1111/j.2517-6161.1954.tb00169.x}, doi = {10.1111/j.2517-6161.1954.tb00169.x}, number = {2}, journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, publisher = {Wiley}, author = {Hodges, J. L. and Lehmann, E. L.}, year = {1954}, month = {jul}, pages = {261–268} }</description></item><item><title>The Difference Between “Significant” And “Not Significant” Is Not Itself Statistically Significant</title><link>https://aakinshin.net/library/papers/gelman2006/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/gelman2006/</guid><description>Reference Andrew Gelman, Hal Stern “The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant” (2006) // The American Statistician. Publisher: Informa UK Limited. Vol. 60. No 4. Pp. 328–331. DOI: 10.1198/000313006x152649
Abstract It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary—for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities. The error we describe is conceptually different from other oftcited problems—that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between &amp;ldquo;significant&amp;rdquo; and &amp;ldquo;not significant&amp;rdquo; is not itself statistically significant.
Bib @Article{gelman2006, title = {The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant}, abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant.</description></item><item><title>The Harm Done by Tests of Significance</title><link>https://aakinshin.net/library/papers/hauer2004/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hauer2004/</guid><description>Reference Ezra Hauer “The harm done by tests of significance” (2004) // Accident Analysis &amp;amp; Prevention. Publisher: Elsevier BV. Vol. 36. No 3. Pp. 495–500. DOI: 10.1016/s0001-4575(03)00036-8
Abstract Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.
Bib @Article{hauer2004, title = {The harm done by tests of significance}, volume = {36}, issn = {0001-4575}, url = {http://dx.doi.org/10.1016/S0001-4575(03)00036-8}, abstract = {Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.}, doi = {10.1016/s0001-4575(03)00036-8}, number = {3}, journal = {Accident Analysis &amp;amp;amp; Prevention}, publisher = {Elsevier BV}, author = {Hauer, Ezra}, year = {2004}, month = {may}, pages = {495–500} }</description></item><item><title>The Importance of Effect Magnitude Over Statistical Hypotheses</title><link>https://aakinshin.net/library/quotes/c6000a61-924a-47b7-805c-2fc024a131bb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c6000a61-924a-47b7-805c-2fc024a131bb/</guid><description>Instead of asking “how many more crashes?” and the authors chose to ask “are we sufficiently sure that the effect was not zero?” This substitution of questions led to all the subsequent entanglements. These can all be avoided by not testing statistical hypotheses when the research question is about the effect of some treatment; by returning to common sense and the mainstream of science and providing estimates of effect magnitude and its standard error instead.</description></item><item><title>The Influence of Journal Prestige on the Inflation of Effect Sizes in Small Trials</title><link>https://aakinshin.net/library/quotes/79991435-cde7-4963-8f62-2bf812a39fda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/79991435-cde7-4963-8f62-2bf812a39fda/</guid><description>We found that small trials published in NEJM, JAMA and Lancet were more likely to display more favourable results for experimental interventions compared with trials in other publication venues. Inflated effect sizes were seen primarily for early small trials in these prominent journals. Therefore, the results of small trials with spectacular early promises for large treatment effects should be seen with great caution. Conversely, for large trials, effect estimates are likely to be more reliable. Small-study effects have been previously documented in the randomized trials literature. However, the results of our study provide further insight suggesting the possibility of a specific interaction with further exaggerated effects when the limited evidence from small trials appears in the most prestigious journals. Also, the inflation of effects in early randomized trials on particular interventions appears to be quite specific to these most prestigious journals. Some modest heterogeneity was seen in the two tertiles with higher events, but heterogeneity is difficult to determine in the tertile with lower events, because of the wide uncertainty in the ROR for single topics, when there is limited evidence.</description></item><item><title>The Null Field</title><link>https://aakinshin.net/library/quotes/f05e3cd0-fb35-4654-b121-421c58c5f5e4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/f05e3cd0-fb35-4654-b121-421c58c5f5e4/</guid><description>Let us suppose that in a research field there are no true findings at all to be discovered. History of science teaches us that scientific endeavor has often in the past wasted effort in fields with absolutely no yield of true scientific information, at least based on our current understanding. In such a “null field,” one would ideally expect all observed effect sizes to vary by chance around the null in the absence of bias. The extent that observed findings deviate from what is expected by chance alone would be simply a pure measure of the prevailing bias.</description></item><item><title>The P Value Fallacy</title><link>https://aakinshin.net/library/quotes/aeb58f62-484b-49a5-853c-8f72e49c82e3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/aeb58f62-484b-49a5-853c-8f72e49c82e3/</guid><description>An important problem exists in the interpretation of modern medical research data: Biological understanding and previous research play little formal role in the interpretation of quantitative results. This phenomenon is manifest in the discussion sections of research articles and ultimately can affect the reliability of conclusions. The standard statistical approach has created this situation by promoting the illusion that conclusions can be produced with certain “error rates,” without consideration of information from outside the experiment. This statistical approach, the key components of which are P values and hypothesis tests, is widely perceived as a mathematically coherent approach to inference. There is little appreciation in the medical community that the methodology is an amalgam of incompatible elements, whose utility for scientific inference has been the subject of intense debate among statisticians for almost 70 years. This article introduces some of the key elements of that debate and traces the appeal and adverse impact of this methodology to the P value fallacy, the mistaken idea that a single number can capture both the long-run outcomes of an experiment and the evidential meaning of a single result. This argument is made as a prelude to the suggestion that another measure of evidence should be used—the Bayes factor, which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings.</description></item><item><title>The Task of a Statistical Referee</title><link>https://aakinshin.net/library/papers/murray1988/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/murray1988/</guid><description>Reference G D Murray “The task of a statistical referee” (1988) // British Journal of Surgery. Publisher: Oxford University Press (OUP). Vol. 75. No 7. Pp. 664–667. DOI: 10.1002/bjs.1800750714
Bib @Article{murray1988, title = {The task of a statistical referee}, volume = {75}, issn = {1365-2168}, url = {http://dx.doi.org/10.1002/bjs.1800750714}, doi = {10.1002/bjs.1800750714}, number = {7}, journal = {British Journal of Surgery}, publisher = {Oxford University Press (OUP)}, author = {Murray, G D}, year = {1988}, month = {jul}, pages = {664–667} }</description></item><item><title>Three Doctors</title><link>https://aakinshin.net/library/quotes/7cc6c419-8556-40a5-b642-4c7ae9c6a3d9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/7cc6c419-8556-40a5-b642-4c7ae9c6a3d9/</guid><description>Imagine three researchers in separate laboratories around the world who believe that patients in two groups differ in their values of a variable. Each researcher proclaims high clinical and research standards. Dr A is particularly fastidious, taking care to remeasure any initial measurements where they are inconsistent with the clinical picture. Dr B is especially scrupulous about bias in research and tries to prevent even a few patients who have other intercurrent diseases from distorting results. Dr C realises that unaided clinical judgement may be poor at classifying patients and that test results may be better guidance.</description></item><item><title>Toward evidence-based Medical statistics. 1: The P Value Fallacy</title><link>https://aakinshin.net/library/papers/goodman1999/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/goodman1999/</guid><description>Reference Steven N Goodman “Toward evidence-based medical statistics. 1: The P value fallacy” (1999) // Annals of internal medicine. Publisher: American College of Physicians. Vol. 130. No 12. Pp. 995–1004. DOI: 10.7326/0003-4819-130-12-199906150-00008
Bib @Article{goodman1999, title = {Toward evidence-based medical statistics. 1: The P value fallacy}, author = {Goodman, Steven N}, journal = {Annals of internal medicine}, volume = {130}, number = {12}, pages = {995--1004}, year = {1999}, doi = {10.7326/0003-4819-130-12-199906150-00008}, publisher = {American College of Physicians} }</description></item><item><title>US Studies May Overestimate Effect Sizes in Softer Research</title><link>https://aakinshin.net/library/papers/fanelli2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/fanelli2013/</guid><description>Reference Daniele Fanelli, John PA Ioannidis “US studies may overestimate effect sizes in softer research” (2013) // Proceedings of the National Academy of Sciences. Publisher: National Acad Sciences. Vol. 110. No 37. Pp. 15031–15036. DOI: 10.1073/pnas.1302997110
Abstract Many biases affect scientific research, causing a waste of resources, posing a threat to human health, and hampering scientific progress. These problems are hypothesized to be worsened by lack of consensus on theories and methods, by selective publication processes, and by career systems too heavily oriented toward productivity, such as those adopted in the United States (US). Here, we extracted 1,174 primary outcomes appearing in 82 meta-analyses published in health-related biological and behavioral research sampled from the Web of Science categories Genetics &amp;amp; Heredity and Psychiatry and measured how individual results deviated from the overall summary effect size within their respective meta-analysis. We found that primary studies whose outcome included behavioral parameters were generally more likely to report extreme effects, and those with a corresponding author based in the US were more likely to deviate in the direction predicted by their experimental hypotheses, particularly when their outcome did not include additional biological parameters. Nonbehavioral studies showed no such &amp;ldquo;US effect&amp;rdquo; and were subject mainly to sampling variance and small-study effects, which were stronger for non-US countries. Although this latter finding could be interpreted as a publication bias against non-US authors, the US effect observed in behavioral research is unlikely to be generated by editorial biases. Behavioral studies have lower methodological consensus and higher noise, making US researchers potentially more likely to express an underlying propensity to report strong and significant findings.
Bib @Article{fanelli2013, title = {US studies may overestimate effect sizes in softer research}, author = {Fanelli, Daniele and Ioannidis, John PA}, journal = {Proceedings of the National Academy of Sciences}, abstract = {Many biases affect scientific research, causing a waste of resources, posing a threat to human health, and hampering scientific progress.</description></item><item><title>Why Even More Clinical Research Studies May Be False: Effect of Asymmetrical Handling of Clinically Unexpected Values</title><link>https://aakinshin.net/library/papers/shun-shin2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/shun-shin2013/</guid><description>Sequel for ioannidis2005. At the beginning, there is a wonderful questionnaire for clinical readers with subsequent sarcastic discussion of the state of the art.
Reference Matthew James Shun-Shin, Darrel P. Francis, Andrew R. Dalby “Why Even More Clinical Research Studies May Be False: Effect of Asymmetrical Handling of Clinically Unexpected Values” (2013) // PLoS ONE. Publisher: Public Library of Science (PLoS). Vol. 8. No 6. Pp. e65323. DOI: 10.1371/journal.pone.0065323
Bib @Article{shun-shin2013, title = {Why Even More Clinical Research Studies May Be False: Effect of Asymmetrical Handling of Clinically Unexpected Values}, volume = {8}, issn = {1932-6203}, url = {http://dx.doi.org/10.1371/journal.pone.0065323}, doi = {10.1371/journal.pone.0065323}, number = {6}, journal = {PLoS ONE}, publisher = {Public Library of Science (PLoS)}, author = {Shun-Shin, Matthew James and Francis, Darrel P.}, editor = {Dalby, Andrew R.}, year = {2013}, month = {jun}, pages = {e65323} }</description></item><item><title>Why Most Discovered True Associations Are Inflated</title><link>https://aakinshin.net/library/papers/ioannidis2008/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2008/</guid><description>Disclaimer The debate on whether research findings are credible is out of the scope. Assumption: considered research findings are true. First example is based on the previous work of the author: ioannidis2007a. While the original study van2002 is based on 117 patients (98 initial + 19 confirmatory), the author highlights only 19 patients from the second trial (to show the small sample size in the pioneer work). On page 645, left column, third line from the bottom, the number 256 should be 461. The same correction should be made in the legend for Figure 2. Reference John PA Ioannidis “Why Most Discovered True Associations Are Inflated” (2008) // Epidemiology. Publisher: Ovid Technologies (Wolters Kluwer Health). Vol. 19. No 5. Pp. 640–648. DOI: 10.1097/ede.0b013e31818131e7
Abstract Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated-for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.</description></item><item><title>Why Most Published Research Findings Are False</title><link>https://aakinshin.net/library/papers/ioannidis2005/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005/</guid><description>Corollaries:
The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true. How Can We Improve the Situation:
Better powered evidence. The totality of the evidence (including all false discoveries). Instead of chasing statistical significance, we should improve our understanding of the pre-study odds. Reference John PA Ioannidis “Why most published research findings are false” (2005) // PLoS medicine. Publisher: Public Library of Science. Vol. 2. No 8. Pp. e124. DOI: 10.1371/journal.pmed.0020124
Abstract There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance.</description></item></channel></rss>