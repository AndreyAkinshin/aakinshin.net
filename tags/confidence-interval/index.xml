<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Confidence Interval on Andrey Akinshin</title><link>https://aakinshin.net/tags/confidence-interval/</link><description>Recent content in Confidence Interval on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 13 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/confidence-interval/index.xml" rel="self" type="application/rss+xml"/><item><title>Estimating quantile confidence intervals: Maritz-Jarrett vs. jackknife</title><link>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</guid><description>&lt;p>When it comes to estimating quantiles of the given sample,
my estimator of choice is the Harrell-Davis quantile estimator
(to be more specific, its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed version&lt;/a>).
If I need to get a confidence interval for the obtained quantiles,
I use the &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/#the-maritz-jarrett-method">Maritz-Jarrett method&lt;/a>
because it provides a &lt;a href="https://aakinshin.net/posts/quantile-ci-coverage/">decent coverage percentage&lt;/a>.
Both approaches work pretty nicely together.&lt;/p>
&lt;p>However, in the original paper by &lt;a href="https://doi.org/10.2307/2335999">Harrell and Davis (1982)&lt;/a>,
the authors suggest using the jackknife variance estimator in order to get the confidence intervals.
The obvious question here is which approach better: the Maritz-Jarrett method or the jackknife estimator?
In this post, I perform a numerical simulation that compares both techniques using different distributions.&lt;/p></description></item><item><title>Coverage of quantile confidence intervals</title><link>https://aakinshin.net/posts/quantile-ci-coverage/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-ci-coverage/</guid><description>&lt;p>There is a common &lt;a href="https://en.wikipedia.org/wiki/Confidence_interval#Misunderstandings">misunderstanding&lt;/a>
that a 95% confidence interval is an interval that covers the true parameter value with 95% probability.
Meanwhile, the correct definition assumes that
the true parameter value will be covered by 95% of 95% confidence intervals &lt;em>in the long run&lt;/em>.
These two statements sound similar, but there is a huge difference between them.
95% in this context is not a property of a single confidence interval.
Once you get a calculated interval, it may cover the true value (100% probability) or
it may don&amp;rsquo;t cover it (0% probability).
In fact, 95% is a &lt;em>prediction&lt;/em> about the percentage of &lt;em>future&lt;/em> confidence intervals
that cover the true value &lt;em>in the long run&lt;/em>.&lt;/p>
&lt;p>However, even if you know the correct definition, you still may experience some troubles.
The first thing people usually forgot is the &amp;ldquo;long run&amp;rdquo; part.
For example, if we collected 100 samples and calculated a 95% confidence interval of a parameter for each of them,
we shouldn&amp;rsquo;t expect that 95 of these intervals cover the true parameter value.
In fact, we can observe a situation when none of these intervals covers the true value.
Of course, this is an unlikely event, but if you automatically perform thousands of different experiments,
you will definitely get some extreme situations.&lt;/p>
&lt;p>The second thing that may create trouble is the &amp;ldquo;prediction&amp;rdquo; part.
If weather forecasters predicted that it will rain tomorrow, this does not mean that it will rain tomorrow.
The same works for statistical predictions.
The actual prediction reliability may depend on many factors.
If you estimate confidence intervals around the mean for the normal distribution, you are most likely safe.
However, if you estimate confidence intervals around quantiles for non-parametric distributions,
you should care about the following things:&lt;/p>
&lt;ul>
&lt;li>The used approach to estimate confidence intervals&lt;/li>
&lt;li>The underlying distribution&lt;/li>
&lt;li>The sample size&lt;/li>
&lt;li>The position of the target quantile&lt;/li>
&lt;/ul>
&lt;p>I &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">have already showed&lt;/a> how to estimate
the confidence interval around the given quantile using the Maritz-Jarrett method.
It&amp;rsquo;s time to verify the reliability of this approach.
In this post, I&amp;rsquo;m going to show some Monte-Carlo simulations that evaluate the coverage percentage in different situations.&lt;/p></description></item><item><title>Quantile confidence intervals for weighted samples</title><link>https://aakinshin.net/posts/weighted-quantiles-ci/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles-ci/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>When you work with non-parametric distributions,
quantile estimations are essential to get the main distribution properties.
Once you get the estimation values, you may be interested in measuring the accuracy of these estimations.
Without it, it&amp;rsquo;s hard to understand how trustable the obtained values are.
One of the most popular ways to evaluate accuracy is confidence interval estimation.&lt;/p>
&lt;p>Now imagine that you collect some measurements every day.
Each day you get a small sample of values that is not enough to get the accurate daily quantile estimations.
However, the full time-series over the last several weeks has a decent size.
You suspect that past measurements should be similar to today measurements,
but you are not 100% sure about it.
You feel a temptation to extend the up-to-date sample by the previously collected values,
but it may spoil the estimation (e.g., in the case of recent change points or positive/negative trends).&lt;/p>
&lt;p>One of the possible approaches in this situation is to use &lt;em>weighted samples&lt;/em>.
This assumes that we add past measurements to the &amp;ldquo;today sample,&amp;rdquo;
but these values should have smaller weight.
The older measurement we take, the smaller weight it gets.
If you have consistent values across the last several days,
this approach works like a charm.
If you have any recent changes, you can detect such situations by huge confidence intervals
due to the sample inconsistency.&lt;/p>
&lt;p>So, how do we estimate confidence intervals around quantiles for the weighted samples?
In one of the previous posts, I have already shown how to &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">estimate quantiles on weighted samples&lt;/a>.
In this post, I will show how to estimate quantile confidence intervals for weighted samples.&lt;/p></description></item></channel></rss>