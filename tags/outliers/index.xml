<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Outliers on Andrey Akinshin</title><link>https://aakinshin.net/tags/outliers/</link><description>Recent content in Outliers on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 31 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/tags/outliers/index.xml" rel="self" type="application/rss+xml"/><item><title>Debunking the tale about outlier removal and ozone holes</title><link>https://aakinshin.net/posts/outliers-ozon-holes/</link><pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/outliers-ozon-holes/</guid><description>&lt;p>Imagine you work with some data and assume that the underlying distribution is approximately normal.
In such cases, the data analysis typically involves non-robust statistics like the mean and the standard deviation.
While these metrics are highly efficient under normality, they make the analysis procedure fragile:
a single extreme value can corrupt all the results.
You may not expect any significant outliers, but you can never be 100% sure.
To avoid unexpected surprises and ensure the reliability of the results,
it may be tempting to automatically exclude all outliers from the collected samples.
While this approach is widely adopted, it conceals an essential part of the obtained data
and can lead to fallacious conclusions.&lt;/p>
&lt;p>Let me recite a classic story about ozone holes from &lt;a href="https://aakinshin.net/library/papers/kandel1990/">&lt;svg class="rating-icon">&lt;use xlink:href="https://aakinshin.net/img/fa/all.svg#paper">&lt;/use>&lt;/svg>kandel1990&lt;/a>,
which is typically used to illustrate the danger of blind outlier removal:&lt;/p></description></item><item><title>Fence-based outlier detectors, Part 2</title><link>https://aakinshin.net/posts/fenced-outlier-detectors2/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/fenced-outlier-detectors2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/fenced-outlier-detectors1/">previous post&lt;/a>,
I discussed different fence-based outlier detectors.
In this post, I show some examples of these detectors with different parameters.&lt;/p></description></item><item><title>Fence-based outlier detectors, Part 1</title><link>https://aakinshin.net/posts/fenced-outlier-detectors1/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/fenced-outlier-detectors1/</guid><description>&lt;p>In previous posts, I discussed properties of &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">Tukey&amp;rsquo;s fences&lt;/a>
and asymmetric decile-based outlier detector
(&lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers1/">Part 1&lt;/a>, &lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers2/">Part 2&lt;/a>).
In this post, I discuss the generalization of fence-based outlier detectors.&lt;/p></description></item><item><title>Asymmetric decile-based outlier detector, Part 2</title><link>https://aakinshin.net/posts/asymmetric-decile-outliers2/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/asymmetric-decile-outliers2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/asymmetric-decile-outliers1/">previous post&lt;/a>,
I suggested an asymmetric decile-based outlier detector
as an alternative to &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">Tukey&amp;rsquo;s fences&lt;/a>.
In this post, we run some numerical simulations to check out
the suggested outlier detector in action.&lt;/p></description></item><item><title>Asymmetric decile-based outlier detector, Part 1</title><link>https://aakinshin.net/posts/asymmetric-decile-outliers1/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/asymmetric-decile-outliers1/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/tukey-outlier-probability/">previous post&lt;/a>, I covered some problems with the outlier detector
based on Tukey fences.
Mainly, I discussed the probability of observing outliers using Tukey&amp;rsquo;s fences
with different factors under different distributions.
However, it&amp;rsquo;s not the only problem with this approach.&lt;/p>
&lt;p>Since Tukey&amp;rsquo;s fences are based on quartiles,
under multimodal distributions, we could get a situation
when 50% of all sample elements are marked as outliers.
Also, Tukey&amp;rsquo;s fences are designed for symmetric distributions,
so we could get strange results with asymmetric distributions.&lt;/p>
&lt;p>In this post, I want to suggest an asymmetric outlier detector based on deciles
which mitigates this problem.&lt;/p></description></item><item><title>Probability of observing outliers using Tukey's fences</title><link>https://aakinshin.net/posts/tukey-outlier-probability/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/tukey-outlier-probability/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Outlier#Tukey's_fences">Tukey&amp;rsquo;s fences&lt;/a> is one of the most popular
simple outlier detectors for one-dimensional number arrays.
This approach assumes that for a given sample, we calculate first and third quartiles ($Q_1$ and $Q_3$),
and mark all the sample elements outside the interval&lt;/p>
$$
[Q_1 - k (Q_3 - Q_1),\, Q_3 + k (Q_3 - Q_1)]
$$
&lt;p>as outliers.
Typical recommendation for $k$ is $1.5$ for &amp;ldquo;regular&amp;rdquo; outliers and $3.0$ for &amp;ldquo;far outliers&amp;rdquo;.
Here is a box plot example for a sample taken from the standard normal distributions (sample size is $1000$):&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-light.png" target="_blank" alt="boxplot1">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-dark.png" target="_blank" alt="boxplot1">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot1-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>As we can see, 11 elements were marked as outliers (shown as dots).
Is it an expected result or not?
The answer depends on your goals.
There is no single definition of an outlier.
In fact, the chosen outlier detector provides a unique outlier definition.&lt;/p>
&lt;p>In my applications, I typically consider outliers as rare events that should be investigated.
When I detect too many outliers, all such reports become useless noise.
For example, on the above image, I wouldn&amp;rsquo;t treat any of the sample elements as outliers.
However, If we add $10.0$ to this sample, this element is an obvious outlier (which will be the only one):&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-light.png" target="_blank" alt="boxplot2">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-light.png" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-dark.png" target="_blank" alt="boxplot2">
 &lt;img
 src="https://aakinshin.net/posts/tukey-outlier-probability/img/boxplot2-dark.png" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Thus, an important property of an outlier detector is the &amp;ldquo;false positive rate&amp;rdquo;:
the percentage of samples with detected outliers which I wouldn&amp;rsquo;t treat as outliers.
In this post, I perform numerical simulations that show the probability of observing outliers
using Tukey&amp;rsquo;s fences with different $k$ values.&lt;/p></description></item><item><title>Plain-text summary notation for multimodal distributions</title><link>https://aakinshin.net/posts/modality-summary-notation/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/modality-summary-notation/</guid><description>&lt;p>Let&amp;rsquo;s say you collected a lot of data and want to explore the underlying distributions of collected samples.
If you have only a few distributions, the best way to do that is to look at the density plots
(expressed via histograms, kernel density estimations, or &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimations&lt;/a>).
However, it&amp;rsquo;s not always possible.&lt;/p>
&lt;p>Suppose you have to process dozens, hundreds, or even thousands of distributions.
In that case,
it may be extremely time-consuming to manually check visualizations of each distribution.
If you analyze distributions from the command line or send notifications about suspicious samples,
it may be impossible to embed images in the reports.
In these cases, there is a need to present a distribution using plain text.&lt;/p>
&lt;p>One way to do that is plain text histograms.
Unfortunately, this kind of visualization may occupy o lot of space.
In complicated cases, you may need 20 or 30 lines per a single distribution.&lt;/p>
&lt;p>Another way is to present classic &lt;a href="https://en.wikipedia.org/wiki/Summary_statistics">summary statistics&lt;/a>
like mean or median, standard deviation or median absolute deviation, quantiles, skewness, and kurtosis.
There is another problem here:
without experience, it&amp;rsquo;s hard to reconstruct the true distribution shape based on these values.
Even if you are an experienced researcher, the statistical metrics may become misleading in the case of multimodal distributions.
Multimodality is one of the most severe challenges in distribution analysis because it distorts basic summary statistics.
It&amp;rsquo;s important to not only find such distribution but also have a way to present brief information about multimodality effects.&lt;/p>
&lt;p>So, how can we condense the underlying distribution shape of a given sample to a short text line?
I didn&amp;rsquo;t manage to find an approach that works fine in my cases, so I came up with my own notation.
Most of the interpretation problems in my experiments arise from multimodality and outliers,
so I decided to focus on these two things and specifically highlight them.
Let&amp;rsquo;s consider this plot:&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg" target="_blank" alt="thumbnail">
 &lt;img
 src="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-dark.svg" target="_blank" alt="thumbnail">
 &lt;img
 src="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>I suggest describing it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="o">{&lt;/span>1.00, 2.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>7.16&lt;span class="p">;&lt;/span> 13.12&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>19.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>27.69&lt;span class="p">;&lt;/span> 32.34&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>37.00..39.00&lt;span class="o">}&lt;/span>_3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let me explain the suggested notation in detail.&lt;/p></description></item><item><title>Intermodal outliers</title><link>https://aakinshin.net/posts/intermodal-outliers/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/intermodal-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Outlier">Outlier&lt;/a> analysis is a typical step in distribution exploration.
Usually, we work with the &amp;ldquo;lower outliers&amp;rdquo; (extremely low values) and the &amp;ldquo;upper outliers&amp;rdquo; (extremely high values).
However, outliers are not always extreme values.
In the general case, an outlier is a value that significantly differs from other values in the same sample.
In the case of multimodal distribution, we can also consider outliers in the middle of the distribution.
Let&amp;rsquo;s call such outliers that we found between modes the &amp;ldquo;&lt;em>intermodal outliers&lt;/em>.&amp;rdquo;&lt;/p>








&lt;div class="flex my-7 justify-center">
 &lt;a class="img-light hidden" href="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg" target="_blank" alt="step4">
 &lt;img
 src="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg" 
 width='800'
 />
 &lt;/a>
 &lt;a class="img-dark hidden" href="https://aakinshin.net/posts/intermodal-outliers/img/step4-dark.svg" target="_blank" alt="step4">
 &lt;img
 src="https://aakinshin.net/posts/intermodal-outliers/img/step4-dark.svg" 
 width='800'
 />
 &lt;/a>
&lt;/div>


&lt;p>Look at the above density plot.
It&amp;rsquo;s a bimodal distribution that is formed as a combination of two unimodal distributions.
Each of the unimodal distributions may have its own lower and upper outliers.
When we merge them, the upper outliers of the first distribution and the lower outliers of the second distribution
stop being lower or upper outliers.
However, if these values don&amp;rsquo;t belong to the modes, they still are a subject of interest.
In this post, I will show you how to detect such intermodal outliers
and how they can be used to form a better distribution description.&lt;/p></description></item><item><title>DoubleMAD outlier detector based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</guid><description>&lt;p>Outlier detection is an important step in data processing.
Unfortunately, if the distribution is not normal (e.g., right-skewed and heavy-tailed), it&amp;rsquo;s hard to choose
a robust outlier detection algorithm that will not be affected by tricky distribution properties.
During the last several years, I tried many different approaches, but I was not satisfied with their results.
Finally, I found an algorithm to which I have (almost) no complaints.
It&amp;rsquo;s based on the &lt;em>double median absolute deviation&lt;/em> and the &lt;em>Harrell-Davis quantile estimator&lt;/em>.
In this post, I will show how it works and why it&amp;rsquo;s better than some other approaches.&lt;/p></description></item><item><title>Learning from outliers</title><link>https://aakinshin.net/library/quotes/103ff452-2341-44a3-8891-202422abfb08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/103ff452-2341-44a3-8891-202422abfb08/</guid><description>Conventional psychology consciously ignores outliers because they don’t fit the pattern. I’ve sought to do the opposite: Instead of deleting these outliers, I want to learn from them.</description></item></channel></rss>