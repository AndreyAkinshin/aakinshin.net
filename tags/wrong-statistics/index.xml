<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Wrong Statistics on Andrey Akinshin</title><link>https://aakinshin.net/tags/wrong-statistics/</link><description>Recent content in Wrong Statistics on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://aakinshin.net/tags/wrong-statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Contradicted and Initially Stronger Effects in Highly Cited Clinical Research</title><link>https://aakinshin.net/library/papers/ioannidis2005a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005a/</guid><description> Ioannidis “Contradicted and Initially Stronger Effects in Highly Cited Clinical Research” (2005) // JAMA. Publisher: American Medical Association (AMA). Vol. 294. No 2. Pp. 218. DOI: 10.1001/jama.294.2.218
Bib @Article{ioannidis2005, title = {Contradicted and Initially Stronger Effects in Highly Cited Clinical Research}, volume = {294}, issn = {0098-7484}, url = {http://dx.doi.org/10.1001/jama.294.2.218}, doi = {10.1001/jama.294.2.218}, number = {2}, journal = {JAMA}, publisher = {American Medical Association (AMA)}, author = {Ioannidis}, year = {2005}, month = {jul}, pages = {218}, }</description></item><item><title>Early extreme contradictory estimates may appear in published research: The Proteus phenomenon in molecular genetics research and randomized trials</title><link>https://aakinshin.net/library/papers/ioannidis2005b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005b/</guid><description> Ioannidis “Early extreme contradictory estimates may appear in published research: The Proteus phenomenon in molecular genetics research and randomized trials” (2005) // Journal of Clinical Epidemiology. Publisher: Elsevier BV. Vol. 58. No 6. Pp. 543–549. DOI: 10.1016/j.jclinepi.2004.10.019
Bib @Article{ioannidis2005, title = {Early extreme contradictory estimates may appear in published research: The Proteus phenomenon in molecular genetics research and randomized trials}, volume = {58}, issn = {0895-4356}, url = {http://dx.doi.org/10.1016/j.jclinepi.2004.10.019}, doi = {10.1016/j.jclinepi.2004.10.019}, number = {6}, journal = {Journal of Clinical Epidemiology}, publisher = {Elsevier BV}, author = {Ioannidis}, year = {2005}, month = {jun}, pages = {543–549}, }</description></item><item><title>Erroneous analyses of interactions in neuroscience: a problem of significance</title><link>https://aakinshin.net/library/papers/nieuwenhuis2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/nieuwenhuis2011/</guid><description>Sander Nieuwenhuis, Birte U Forstmann, Eric-Jan Wagenmakers “Erroneous analyses of interactions in neuroscience: a problem of significance” (2011) // Nature Neuroscience. Publisher: Springer Science and Business Media LLC. Vol. 14. No 9. Pp. 1105–1107. DOI: 10.1038/nn.2886
Bib @Article{nieuwenhuis2011, title = {Erroneous analyses of interactions in neuroscience: a problem of significance}, volume = {14}, issn = {1546-1726}, url = {http://dx.doi.org/10.1038/nn.2886}, doi = {10.1038/nn.2886}, number = {9}, journal = {Nature Neuroscience}, publisher = {Springer Science and Business Media LLC}, author = {Nieuwenhuis, Sander and Forstmann, Birte U and Wagenmakers, Eric-Jan}, year = {2011}, month = {aug}, pages = {1105–1107}, }</description></item><item><title>How confidence intervals become confusion intervals</title><link>https://aakinshin.net/library/papers/mccormack2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/mccormack2013/</guid><description>James McCormack, Ben Vandermeer, G Michael Allan “How confidence intervals become confusion intervals” (2013) // BMC Medical Research Methodology. Publisher: Springer Science and Business Media LLC. Vol. 13. No 1. DOI: 10.1186/1471-2288-13-134
Abstract In this paper, we review how researchers can look at very similar data yet have completely different conclusions based purely on an over-reliance of statistical significance and an unclear understanding of confidence intervals. The dogmatic adherence to statistical significant thresholds can lead authors to write dichotomized abso- lute conclusions while ignoring the broader interpretations of very consistent findings. We describe three examples of controversy around the potential benefit of a medication, a comparison between new medications, and a medi- cation with a potential harm. The examples include the highest levels of evidence, both meta-analyses and random- ized controlled trials. We will show how in each case the confidence intervals and point estimates were very similar. The only identifiable differences to account for the contrasting conclusions arise from the serendipitous finding of confidence intervals that either marginally cross or just fail to cross the line of statistical significance.
Bib @Article{mccormack2013, title = {How confidence intervals become confusion intervals}, abstract = {In this paper, we review how researchers can look at very similar data yet have completely different conclusions based purely on an over-reliance of statistical significance and an unclear understanding of confidence intervals. The dogmatic adherence to statistical significant thresholds can lead authors to write dichotomized abso- lute conclusions while ignoring the broader interpretations of very consistent findings. We describe three examples of controversy around the potential benefit of a medication, a comparison between new medications, and a medi- cation with a potential harm. The examples include the highest levels of evidence, both meta-analyses and random- ized controlled trials. We will show how in each case the confidence intervals and point estimates were very similar.</description></item><item><title>How to Lie with Statistics</title><link>https://aakinshin.net/library/books/huff-how-to-lie-with-statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/books/huff-how-to-lie-with-statistics/</guid><description/></item><item><title>Magnitude of effects in clinical trials published in high-impact general medical journals</title><link>https://aakinshin.net/library/papers/siontis2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/siontis2011/</guid><description> Siontis “Magnitude of effects in clinical trials published in high-impact general medical journals” (2011) // International Journal of Epidemiology. Publisher: Oxford University Press (OUP). Vol. 40. No 5. Pp. 1280–1291. DOI: 10.1093/ije/dyr095
Bib @Article{siontis2011, title = {Magnitude of effects in clinical trials published in high-impact general medical journals}, volume = {40}, issn = {0300-5771}, url = {http://dx.doi.org/10.1093/ije/dyr095}, doi = {10.1093/ije/dyr095}, number = {5}, journal = {International Journal of Epidemiology}, publisher = {Oxford University Press (OUP)}, author = {Siontis}, year = {2011}, month = {sep}, pages = {1280–1291}, }</description></item><item><title>Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction</title><link>https://aakinshin.net/library/papers/bennett2009/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/bennett2009/</guid><description>Craig M Bennett, Michael B Miller, George L Wolford “Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction” (2009) // NeuroImage. Publisher: Elsevier BV. Vol. 47. Pp. S125. DOI: 10.1016/s1053-8119(09)71202-9
Bib @Article{bennett2009, title = {Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction}, volume = {47}, issn = {1053-8119}, url = {http://dx.doi.org/10.1016/S1053-8119(09)71202-9}, doi = {10.1016/s1053-8119(09)71202-9}, journal = {NeuroImage}, publisher = {Elsevier BV}, author = {Bennett, Craig M and Miller, Michael B and Wolford, George L}, year = {2009}, month = {jul}, pages = {S125}, } Notes One of my favorite examples of statistics misuse.</description></item><item><title>Statistics Done Wrong: The Woefully Complete Guide</title><link>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</guid><description/></item><item><title>Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates</title><link>https://aakinshin.net/library/papers/hemenway1997/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hemenway1997/</guid><description> Hemenway “Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates” (1997) // The Journal of Criminal Law and Criminology (1973-). Publisher: JSTOR. Vol. 87. No 4. Pp. 1430. DOI: 10.2307/1144020
Abstract Gary Kleck and Marc Gertz conducted a survey of civilian defensive gun use in 1992. In 1993, Kleck began publicizing the estimate that civilians use guns in self-defense against offenders up to 2.5 million times each year.&amp;rsquo; This figure has been widely used by the National Rifle Association and by gun advocates. It is also often cited in the media and even in Congress. The Kleck and Gertz (K-G) paper has now been published. It is clear, however, that its conclusions cannot be accepted as valid.
Bib @Article{hemenway1997, title = {Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates}, abstract = {Gary Kleck and Marc Gertz conducted a survey of civilian defensive gun use in 1992. In 1993, Kleck began publicizing the estimate that civilians use guns in self-defense against offenders up to 2.5 million times each year.&amp;#39; This figure has been widely used by the National Rifle Association and by gun advocates. It is also often cited in the media and even in Congress. The Kleck and Gertz (K-G) paper has now been published. It is clear, however, that its conclusions cannot be accepted as valid.}, volume = {87}, issn = {0091-4169}, url = {http://dx.doi.org/10.2307/1144020}, doi = {10.2307/1144020}, number = {4}, journal = {The Journal of Criminal Law and Criminology (1973-)}, publisher = {JSTOR}, author = {Hemenway}, year = {1997}, pages = {1430}, }</description></item><item><title>The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant</title><link>https://aakinshin.net/library/papers/gelman2006/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/gelman2006/</guid><description>Andrew Gelman, Hal Stern “The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant” (2006) // The American Statistician. Publisher: Informa UK Limited. Vol. 60. No 4. Pp. 328–331. DOI: 10.1198/000313006x152649
Abstract It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary—for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities. The error we describe is conceptually different from other oftcited problems—that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between &amp;ldquo;significant&amp;rdquo; and &amp;ldquo;not significant&amp;rdquo; is not itself statistically significant.
Bib @Article{gelman2006, title = {The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant}, abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant.</description></item><item><title>The harm done by tests of significance</title><link>https://aakinshin.net/library/papers/hauer2004/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/hauer2004/</guid><description> Hauer “The harm done by tests of significance” (2004) // Accident Analysis &amp;amp; Prevention. Publisher: Elsevier BV. Vol. 36. No 3. Pp. 495–500. DOI: 10.1016/s0001-4575(03)00036-8
Abstract Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.
Bib @Article{hauer2004, title = {The harm done by tests of significance}, volume = {36}, issn = {0001-4575}, url = {http://dx.doi.org/10.1016/S0001-4575(03)00036-8}, abstract = {Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.}, doi = {10.1016/s0001-4575(03)00036-8}, number = {3}, journal = {Accident Analysis &amp;amp;amp; Prevention}, publisher = {Elsevier BV}, author = {Hauer}, year = {2004}, month = {may}, pages = {495–500}, }</description></item><item><title>The task of a statistical referee</title><link>https://aakinshin.net/library/papers/murray1988/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/murray1988/</guid><description> Murray “The task of a statistical referee” (1988) // British Journal of Surgery. Publisher: Oxford University Press (OUP). Vol. 75. No 7. Pp. 664–667. DOI: 10.1002/bjs.1800750714
Bib @Article{murray1988, title = {The task of a statistical referee}, volume = {75}, issn = {1365-2168}, url = {http://dx.doi.org/10.1002/bjs.1800750714}, doi = {10.1002/bjs.1800750714}, number = {7}, journal = {British Journal of Surgery}, publisher = {Oxford University Press (OUP)}, author = {Murray}, year = {1988}, month = {jul}, pages = {664–667}, }</description></item><item><title>Toward evidence-based medical statistics. 1: The P value fallacy</title><link>https://aakinshin.net/library/papers/goodman1999/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/goodman1999/</guid><description>Steven N Goodman “Toward evidence-based medical statistics. 1: The P value fallacy” (1999) // Annals of internal medicine. Publisher: American College of Physicians. Vol. 130. No 12. Pp. 995–1004. DOI: 10.7326/0003-4819-130-12-199906150-00008
Bib @Article{goodman1999, title = {Toward evidence-based medical statistics. 1: The P value fallacy}, author = {Goodman, Steven N}, journal = {Annals of internal medicine}, volume = {130}, number = {12}, pages = {995--1004}, year = {1999}, doi = {10.7326/0003-4819-130-12-199906150-00008}, publisher = {American College of Physicians}, }</description></item><item><title>US studies may overestimate effect sizes in softer research</title><link>https://aakinshin.net/library/papers/fanelli2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/fanelli2013/</guid><description>Daniele Fanelli, John PA Ioannidis “US studies may overestimate effect sizes in softer research” (2013) // Proceedings of the National Academy of Sciences. Publisher: National Acad Sciences. Vol. 110. No 37. Pp. 15031–15036. DOI: 10.1073/pnas.1302997110
Abstract Many biases affect scientific research, causing a waste of resources, posing a threat to human health, and hampering scientific progress. These problems are hypothesized to be worsened by lack of consensus on theories and methods, by selective publication processes, and by career systems too heavily oriented toward productivity, such as those adopted in the United States (US). Here, we extracted 1,174 primary outcomes appearing in 82 meta-analyses published in health-related biological and behavioral research sampled from the Web of Science categories Genetics &amp;amp; Heredity and Psychiatry and measured how individual results deviated from the overall summary effect size within their respective meta-analysis. We found that primary studies whose outcome included behavioral parameters were generally more likely to report extreme effects, and those with a corresponding author based in the US were more likely to deviate in the direction predicted by their experimental hypotheses, particularly when their outcome did not include additional biological parameters. Nonbehavioral studies showed no such &amp;ldquo;US effect&amp;rdquo; and were subject mainly to sampling variance and small-study effects, which were stronger for non-US countries. Although this latter finding could be interpreted as a publication bias against non-US authors, the US effect observed in behavioral research is unlikely to be generated by editorial biases. Behavioral studies have lower methodological consensus and higher noise, making US researchers potentially more likely to express an underlying propensity to report strong and significant findings.
Bib @Article{fanelli2013, title = {US studies may overestimate effect sizes in softer research}, author = {Fanelli, Daniele and Ioannidis, John PA}, journal = {Proceedings of the National Academy of Sciences}, abstract = {Many biases affect scientific research, causing a waste of resources, posing a threat to human health, and hampering scientific progress.</description></item><item><title>Why Most Discovered True Associations Are Inflated</title><link>https://aakinshin.net/library/papers/ioannidis2008/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2008/</guid><description>Ioannidis “Why Most Discovered True Associations Are Inflated” (2008) // Epidemiology. Publisher: Ovid Technologies (Wolters Kluwer Health). Vol. 19. No 5. Pp. 640–648. DOI: 10.1097/ede.0b013e31818131e7
Bib @Article{ioannidis2008, title = {Why Most Discovered True Associations Are Inflated}, volume = {19}, issn = {1044-3983}, url = {http://dx.doi.org/10.1097/EDE.0b013e31818131e7}, doi = {10.1097/ede.0b013e31818131e7}, number = {5}, journal = {Epidemiology}, publisher = {Ovid Technologies (Wolters Kluwer Health)}, author = {Ioannidis}, year = {2008}, month = {sep}, pages = {640–648}, } Notes On page 645, left column, third line from the bottom, the number 256 should be 461. The same correction should be made in the legend for Figure 2.</description></item><item><title>Why most published research findings are false</title><link>https://aakinshin.net/library/papers/ioannidis2005/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/ioannidis2005/</guid><description>John PA Ioannidis “Why most published research findings are false” (2005) // PLoS medicine. Publisher: Public Library of Science. Vol. 2. No 8. Pp. e124. DOI: 10.1371/journal.pmed.0020124
Abstract There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.
Bib @Article{ioannidis2005, title = {Why most published research findings are false}, author = {Ioannidis, John PA}, journal = {PLoS medicine}, abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance.</description></item></channel></rss>