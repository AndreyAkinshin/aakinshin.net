<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistical Power on Andrey Akinshin</title><link>https://aakinshin.net/tags/statistical-power/</link><description>Recent content in Statistical Power on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://aakinshin.net/tags/statistical-power/index.xml" rel="self" type="application/rss+xml"/><item><title>Analytically Calculating Power Can Be Difﬁcult or Downright Impossible</title><link>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</guid><description>Math is another possible explanation for why power calculations are so uncommon: analytically calculating power can be difﬁcult or downright impossible. Techniques for calculating power are not frequently taught in intro statistics courses. And some commercially available statistical software does not come with power calculation functions. It is possible to avoid hairy mathematics by simply simulating thousands of artiﬁcial datasets with the effect size you expect and running your statistical tests on the simulated data. The power is simply the fraction of datasets for which you obtain a statistically signiﬁcant result. But this approach requires programming experience, and simulating realistic data can be tricky.</description></item><item><title>Decline of Power in Psychology (1962..1984)</title><link>https://aakinshin.net/library/quotes/9eaab253-ea71-4eb4-add2-fc061c448725/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9eaab253-ea71-4eb4-add2-fc061c448725/</guid><description>The long-term impact of studies of statistical power is investigated using J. Cohen&amp;rsquo;s (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases.</description></item><item><title>Do Studies of Statistical Power Have an Effect on the Power of studies?</title><link>https://aakinshin.net/library/papers/sedlmeier1989/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/sedlmeier1989/</guid><description>Reference Peter Sedlmeier, Gerd Gigerenzer “Do studies of statistical power have an effect on the power of studies?” (1989) // Psychological Bulletin. Publisher: American Psychological Association (APA). Vol. 105. No 2. Pp. 309–316. DOI: 10.1037/0033-2909.105.2.309
Bib @Article{sedlmeier1989, title = {Do studies of statistical power have an effect on the power of studies?}, volume = {105}, issn = {0033-2909}, url = {http://dx.doi.org/10.1037/0033-2909.105.2.309}, doi = {10.1037/0033-2909.105.2.309}, number = {2}, journal = {Psychological Bulletin}, publisher = {American Psychological Association (APA)}, author = {Sedlmeier, Peter and Gigerenzer, Gerd}, year = {1989}, month = {mar}, pages = {309–316} }</description></item><item><title>Inadequate Statistical Power to Detect Clinically Significant Differences in Adverse Event Rates in Randomized Controlled Trials</title><link>https://aakinshin.net/library/papers/tsang2009/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/tsang2009/</guid><description>Reference Ruth Tsang, Lindsey Colley, Larry D Lynd “Inadequate statistical power to detect clinically significant differences in adverse event rates in randomized controlled trials” (2009) // Journal of Clinical Epidemiology. Publisher: Elsevier BV. Vol. 62. No 6. Pp. 609–616. DOI: 10.1016/j.jclinepi.2008.08.005
Bib @Article{tsang2009, title = {Inadequate statistical power to detect clinically significant differences in adverse event rates in randomized controlled trials}, volume = {62}, issn = {0895-4356}, url = {http://dx.doi.org/10.1016/j.jclinepi.2008.08.005}, doi = {10.1016/j.jclinepi.2008.08.005}, number = {6}, journal = {Journal of Clinical Epidemiology}, publisher = {Elsevier BV}, author = {Tsang, Ruth and Colley, Lindsey and Lynd, Larry D}, year = {2009}, month = {jun}, pages = {609–616} }</description></item><item><title>Investigating the Probability of Rejecting Null Hypotheses in Abnormal-Social Research</title><link>https://aakinshin.net/library/quotes/fcecf1bc-d282-46ec-b32a-86dc4b053695/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/fcecf1bc-d282-46ec-b32a-86dc4b053695/</guid><description>The results indicate that the investigators contributing to Volume 61 of the Journal of Abnormal and Social Psychology had, on the average, a relatively (or even absolutely) poor chance of rejecting their major null hypotheses, unless the effect they sought was large. This surprising (and discouraging) finding needs some further consideration to be seen in full perspective.
First, it may be noted that with few exceptions, the 70 studies did have significant results. This may then suggest that perhaps the definitions of size of effect were too severe, or perhaps, accepting the definitions, one might seek to conclude that the investigators were operating under circumstances wherein the effects were actually large, hence their success. Perhaps, then, research in the abnormal-social area is not as &amp;ldquo;weak&amp;rdquo; as the above results suggest. But this argument rests on the implicit assumption that the research which is published is representative of the research undertaken in this area. It seems obvious that investigators are less likely to submit for publication unsuccessful than successful research, to say nothing of a similar editorial bias in accepting research for publication. Consider this paradigm: 100 investigations are undertaken in which, in fact, there is actually a medium population effect. From the above findings, about SO get positive results and are likely to come to publication; the other SO fail to reject their (assumed false) null hypotheses and are unlikely to come to publication. Thus, the general success of the articles in the volume under review does not successfully argue for their antecedent probabilities of success being any higher than the results of the analysis suggest, or, equivalently, that the criteria for size of effect used were overly stringent.&amp;quot;</description></item><item><title>Low Statistical Power</title><link>https://aakinshin.net/library/quotes/982229dc-9c49-4800-8039-c08ca40858e3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/982229dc-9c49-4800-8039-c08ca40858e3/</guid><description>Objective. To describe the pattern over time in the level of statistical power and the reporting of sample size calculations in published randomized controlled trials (RCTs) with negative results.
Design. Ourstudy was a descriptive survey. Power to detect 25% and 50% relative differences was calculated for the subset of trials with negative results in which a simple two-group parallel design was used. Criteria were developed both to classify trial results as positive or negative and to identify the primary outcomes. Power calculations were based on results from the primary outcomes reported in the trials.
Population. We reviewed all 383 RCTs published in JAMA, Lancet, and the New England Journal of Medicine in 1975, 1980, 1985, and 1990.
Results. Twenty-sevenpercent of the 383 RCTs (n=102) were classified as having negative results. The number of published RCTs more than doubled from 1975 to 1990, with the proportion of trials with negative results remaining fairly stable. Of the simple two-group parallel design trials having negative results with dichotomous or continuous primary outcomes (n=70), only 16% and 36% had sufficient statistical power (80%) to detect a 25% or 50% relative difference, respectively. These percentages did not consistently increase overtime. Overall, only 32% of the trials with negative results reported sample size calculations, but the percentage doing so has improved over time from 0% in 1975 to 43% in 1990. Only 20 of the 102 reports made any statement related to the clinical significance of the observed differences.
Conclusions. Most trials with negative results did not have large enough sample sizes to detect a 25% or a 50% relative difference. This result has not changed over time. Few trials discussed whether the observed differences were clinically important. There are important reasons to change this practice. The reporting of statistical power and sample size also needs to be improved.</description></item><item><title>Power failure: Why Small Sample Size Undermines the Reliability of Neuroscience</title><link>https://aakinshin.net/library/papers/button2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/button2013/</guid><description>Reference Katherine S Button, John P A Ioannidis, Claire Mokrysz, Brian A Nosek, Jonathan Flint, Emma S J Robinson, Marcus R Munafò “Power failure: why small sample size undermines the reliability of neuroscience” (2013) // Nature Reviews Neuroscience. Publisher: Springer Science and Business Media LLC. Vol. 14. No 5. Pp. 365–376. DOI: 10.1038/nrn3475
Bib @Article{button2013, title = {Power failure: why small sample size undermines the reliability of neuroscience}, volume = {14}, issn = {1471-0048}, url = {http://dx.doi.org/10.1038/nrn3475}, doi = {10.1038/nrn3475}, number = {5}, journal = {Nature Reviews Neuroscience}, publisher = {Springer Science and Business Media LLC}, author = {Button, Katherine S and Ioannidis, John P A and Mokrysz, Claire and Nosek, Brian A and Flint, Jonathan and Robinson, Emma S J and Munafò, Marcus R}, year = {2013}, month = {apr}, pages = {365–376} }</description></item><item><title>Rules of Smaple Size Planning</title><link>https://aakinshin.net/library/quotes/63edb163-442a-4336-a5c4-fc51eb0e94e7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/63edb163-442a-4336-a5c4-fc51eb0e94e7/</guid><description> Sample size planning is important to enhance cumulative knowledge in the discipline as well as for the individual researcher. Sample size planning can be based on a goal of achieving adequate statistical power, or accurate parameter estimates, or both. Researchers are actively involved in developing methods for sample size planning, especially for complex designs and analyses. Sample sizes necessary to achieve accurate parameter estimates will often be larger than sample sizes necessary to detect even a small effect. Sample sizes necessary to obtain accurate parameter estimates or power to detect small effects may often require resources prohibitive to the individual researcher, thus suggesting the desirability of study registries accompanied by meta-analytic methods.</description></item><item><title>Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation</title><link>https://aakinshin.net/library/papers/maxwell2008/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/maxwell2008/</guid><description>Reference Scott E Maxwell, Ken Kelley, Joseph R Rausch “Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation” (2008) // Annual Review of Psychology. Publisher: Annual Reviews. Vol. 59. No 1. Pp. 537–563. DOI: 10.1146/annurev.psych.59.103006.093735
Bib @Article{maxwell2008, title = {Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation}, volume = {59}, issn = {1545-2085}, url = {http://dx.doi.org/10.1146/annurev.psych.59.103006.093735}, doi = {10.1146/annurev.psych.59.103006.093735}, number = {1}, journal = {Annual Review of Psychology}, publisher = {Annual Reviews}, author = {Maxwell, Scott E and Kelley, Ken and Rausch, Joseph R}, year = {2008}, month = {jan}, pages = {537–563} }</description></item><item><title>Statistical Power of Negative Randomized Controlled Trials Presented at American Society for Clinical Oncology Annual Meetings</title><link>https://aakinshin.net/library/papers/bedard2007/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/bedard2007/</guid><description>Reference Philippe L Bedard, Monika K Krzyzanowska, Melania Pintilie, Ian F Tannock “Statistical Power of Negative Randomized Controlled Trials Presented at American Society for Clinical Oncology Annual Meetings” (2007) // Journal of Clinical Oncology. Publisher: American Society of Clinical Oncology (ASCO). Vol. 25. No 23. Pp. 3482–3487. DOI: 10.1200/jco.2007.11.3670
Bib @Article{bedard2007, title = {Statistical Power of Negative Randomized Controlled Trials Presented at American Society for Clinical Oncology Annual Meetings}, volume = {25}, issn = {1527-7755}, url = {http://dx.doi.org/10.1200/JCO.2007.11.3670}, doi = {10.1200/jco.2007.11.3670}, number = {23}, journal = {Journal of Clinical Oncology}, publisher = {American Society of Clinical Oncology (ASCO)}, author = {Bedard, Philippe L and Krzyzanowska, Monika K and Pintilie, Melania and Tannock, Ian F}, year = {2007}, month = {aug}, pages = {3482–3487} }</description></item><item><title>Statistical power, Sample size, and Their Reporting in Randomized Controlled Trials</title><link>https://aakinshin.net/library/papers/moher1994/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/moher1994/</guid><description>Reference D Moher, C Dulberg, G Wells “Statistical power, sample size, and their reporting in randomized controlled trials” (1994) // JAMA: The Journal of the American Medical Association. Publisher: American Medical Association (AMA). Vol. 272. No 2. Pp. 122. DOI: 10.1001/jama.1994.03520020048013
Bib @Article{moher1994, title = {Statistical power, sample size, and their reporting in randomized controlled trials}, volume = {272}, issn = {0098-7484}, url = {http://dx.doi.org/10.1001/jama.1994.03520020048013}, doi = {10.1001/jama.1994.03520020048013}, number = {2}, journal = {JAMA: The Journal of the American Medical Association}, publisher = {American Medical Association (AMA)}, author = {D Moher and C Dulberg and G Wells}, year = {1994}, month = {jul}, pages = {122} }</description></item><item><title>Statistical power: An Historical Introduction</title><link>https://aakinshin.net/library/papers/descoteaux2007/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/descoteaux2007/</guid><description>Reference Descôteaux J “Statistical power: An historical introduction” (2007) // Tutor. Quant. Methods Psychol.. Vol. 3. Pp. 28.
Bib @Article{descoteaux2007, title = {Statistical power: An historical introduction}, volume = {3}, journal = {Tutor. Quant. Methods Psychol.}, author = {J, Descôteaux}, year = {2007}, pages = {28}, custom-url-pdf = {https://www.tqmp.org/RegularArticles/vol03-2/p028/p028.pdf} }</description></item><item><title>The Beta Error and Sample Size Determination in Clinical Trials in Emergency Medicine</title><link>https://aakinshin.net/library/papers/brown1987/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/brown1987/</guid><description>Reference Charles G Brown, Gabor D Kelen, James J Ashton, Howard A Werman “The beta error and sample size determination in clinical trials in emergency medicine” (1987) // Annals of Emergency Medicine. Publisher: Elsevier BV. Vol. 16. No 2. Pp. 183–187. DOI: 10.1016/s0196-0644(87)80013-6
Bib @Article{brown1987, title = {The beta error and sample size determination in clinical trials in emergency medicine}, volume = {16}, issn = {0196-0644}, url = {http://dx.doi.org/10.1016/S0196-0644(87)80013-6}, doi = {10.1016/s0196-0644(87)80013-6}, number = {2}, journal = {Annals of Emergency Medicine}, publisher = {Elsevier BV}, author = {Brown, Charles G and Kelen, Gabor D and Ashton, James J and Werman, Howard A}, year = {1987}, month = {feb}, pages = {183–187} }</description></item><item><title>The Neuroscience of Low Statistical Power</title><link>https://aakinshin.net/library/quotes/76c951e9-c936-4b30-919f-d3d3fd7f7227/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/76c951e9-c936-4b30-919f-d3d3fd7f7227/</guid><description>A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.</description></item><item><title>The Statistical Power of abnormal-social Psychological research: A Review</title><link>https://aakinshin.net/library/papers/cohen1962/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/cohen1962/</guid><description>Reference Jacob Cohen “The statistical power of abnormal-social psychological research: A review” (1962) // The Journal of Abnormal and Social Psychology. Publisher: American Psychological Association (APA). Vol. 65. No 3. Pp. 145–153. DOI: 10.1037/h0045186
Bib @Article{cohen1962, title = {The statistical power of abnormal-social psychological research: A review}, volume = {65}, issn = {0096-851X}, url = {http://dx.doi.org/10.1037/h0045186}, doi = {10.1037/h0045186}, number = {3}, journal = {The Journal of Abnormal and Social Psychology}, publisher = {American Psychological Association (APA)}, author = {Cohen, Jacob}, year = {1962}, month = {sep}, pages = {145–153} }</description></item><item><title>Type II Error Should Be as Essential a Requirement for Publication</title><link>https://aakinshin.net/library/quotes/c7ba1497-c183-41ab-9d9e-a4cffb4d2c84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c7ba1497-c183-41ab-9d9e-a4cffb4d2c84/</guid><description>Failure to achieve statistical significance between interventions does not prove the absence of any difference. Proper planning of a clinical trial with attention to beta error and sample size determination allows the critical investigator to acknowledge the probability of a type II error and therefore the probability of detecting a clinically meaningful difference if one exists. The reader and clinician must be aware that negative trials may in fact be falsely negative, and should look for specific reporting of alpha, beta, and $\Delta_c$ error to provide the details of the reliability of such conclusions. Type II error should be as essential a requirement for publication, and as rigorously analyzed, as the traditional and far more common type I (alpha) error.</description></item><item><title>Underpowered Negative Clinical Trials</title><link>https://aakinshin.net/library/quotes/10189519-eb48-41f5-a1ca-adac8092bf8e/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/10189519-eb48-41f5-a1ca-adac8092bf8e/</guid><description>Our survey of 423 negative clinical trials indicates that 55% of trials had too few participants to detect a medium effect size in favor of the experimental over the standard treatment arm for their primary end point with at least 80% statistical power. Although underpowered negative clinical trials have been widely reported in the general medical and subspecialty literature, there are few reports relating to trials evaluating treatment of cancer. A review of 22 negative randomized oncology trials published in major general medical or oncology journals during a 1-year period found that 16 trials (73%) lacked adequate statistical power to detect a 50% improvement in median survival in favor of the experimental arm.</description></item></channel></rss>