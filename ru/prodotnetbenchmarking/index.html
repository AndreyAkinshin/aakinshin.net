<!doctype html><html lang=ru class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.74.3"><meta name=author content="Андрей Акиньшин"><link href=/img/favicon.ico rel=icon type=image/x-icon><title>Pro .NET Benchmarking | Андрей Акиньшин</title><link href=/css/lumen-bootstrap.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/slate-bootstrap.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><link href=/css/syntax-light.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/syntax-dark.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.26d61b7027fe55e642f0001cef94cb75b567d721086492aab6b4e32d9ee7811d.js></script><script type=module src=https://googlechromelabs.github.io/dark-mode-toggle/src/dark-mode-toggle.mjs></script><link href=https://aakinshin.net/css/fontawesome-all.min.e78467baec0179d7ccd2ef995e0c94f25b4a63a3191a93c3547e22bdf590ef5d.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.2a76db7cc14055b3bafacc363f453917ab54a818a7db2a0a9b2409c0f6bf07c5.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.a9ad336a30411929ac042537ef03ddbe3b2b28af5aa50655711f870419408467.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/ru/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-ZVB6MXSX32');</script><script type=text/javascript>(function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym");ym(28700916,"init",{clickmap:true,trackLinks:true,accurateTrackBounce:true});</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-lg navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/ru/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/ru/posts/>Посты</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/ru/about/>Об авторе</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/ru/prodotnetbenchmarking/>Pro .NET Benchmarking</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" data-toggle=dropdown href=# role=button aria-haspopup=true aria-expanded=false>RU</a><div class=dropdown-menu><a class=dropdown-item href=https://aakinshin.net/pages/prodotnetbenchmarking/>EN</a></div></li><li class=nav-item></li><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub style=color:#fff></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter style=color:#fff></i></a></li><li class=nav-item><a class=nav-link href=https://aakinshin.net/ru/posts/index.xml><i class="fas fa-rss" title=RSS style=color:#fff></i></a></li></ul></nav></div></div><div class=container><main id=main><h1>Pro .NET Benchmarking</h1><p></p><div class=container-fluid><div class=row><div class=col-sm><img class=img-fluid src=/img/misc/prodotnetbenchmarking-cover.png></div><div class=col-md><h3>Learn how to measure application performance and analyze the results!</h3>Use this in-depth guide to correctly design benchmarks, measure key performance metrics of .NET applications, and analyze results.
This book presents dozens of case studies to help you understand complicated benchmarking topics.
You will avoid common pitfalls, control the accuracy of your measurements, and improve performance of your software.
Author Andrey Akinshin has maintained
<a href=https://github.com/dotnet/BenchmarkDotNet>BenchmarkDotNet</a>
(the most popular .NET library for benchmarking) for five years
and covers common mistakes that developers usually make in their benchmarks.
This book includes not only .NET-specific content but also essential knowledge about performance measurements
which can be applied to any language or platform (common benchmarking methodology, statistics, and low-level features of modern hardware).
With this book, you will learn:<ul><li>Be aware of the best practices for writing benchmarks and performance tests</li><li>Avoid the common benchmarking pitfalls</li><li>Know the hardware and software factors that affect application performance</li><li>Analyze performance measurements</li></ul><div class=text-center><a class="btn btn-primary" href=https://www.apress.com/us/book/9781484249406>Apress</a>
<a class="btn btn-primary" href=https://www.amazon.com/gp/product/1484249402/>Amazon US</a>
<a class="btn btn-primary" href=https://www.amazon.ca/Pro-NET-Benchmarking-Performance-Measurement/dp/1484249402>Amazon CA</a>
<a class="btn btn-primary" href=https://www.amazon.co.uk/Pro-NET-Benchmarking-Performance-Measurement/dp/1484249402>Amazon UK</a>
<a class="btn btn-primary" href=https://www.amazon.de/dp/1484249402/>Amazon DE</a>
<a class="btn btn-primary" href=https://www.amazon.fr/Pro-NET-Benchmarking-Performance-Measurement/dp/1484249402/>Amazon FR</a>
<a class="btn btn-primary" href=https://www.amazon.es/Pro-NET-Benchmarking-Performance-Measurement/dp/1484249402/>Amazon ES</a>
<a class="btn btn-primary" href=https://www.amazon.co.jp/Pro-NET-Benchmarking-Performance-Measurement/dp/1484249402/>Amazon JP</a>
<a class="btn btn-primary" href=https://www.springer.com/gp/book/9781484249406>Springer</a>
<a class="btn btn-primary" href="https://books.google.ru/books?id=IXCfDwAAQBAJ">Google Books</a>
<a class="btn btn-primary" href=https://www.oreilly.com/library/view/pro-net-benchmarking/9781484249413/>O’Reilly</a>
<a class="btn btn-primary" href=https://www.goodreads.com/book/show/45159905-pro-net-benchmarking>GoodReads</a>
<a class="btn btn-primary" href=https://www.researchgate.net/publication/334047447_Pro_NET_Benchmarking_The_Art_of_Performance_Measurement>ResearchGate</a>
<a class="btn btn-primary" href=https://github.com/Apress/pro-.net-benchmarking>GitHub (Examples)</a></div></div></div></div><p></p><h2 id=content>Content</h2><p>The book contains nine chapters:</p><ul><li><strong>Chapter 1 “Introducing Benchmarking”</strong><br>This chapter contains some basic information about benchmarking and other performance investigations, including benchmarking goals and requirements. We will also discuss performance spaces and why it’s so important to analyze benchmark results.</li><li><strong>Chapter 2 “Common Benchmarking Pitfalls”</strong><br>This chapter contains 15 examples of common mistakes that developers usually make during benchmarking. Each example is pretty small (so, you can easily understand what’s going on), but all of them demonstrate important problems and explain how to resolve them.</li><li><strong>Chapter 3 “How Environment Affects Performance”</strong><br>This chapter explains why the environment is so important and introduces a lot of terms that will be used in subsequent chapters. You will find 12 case studies that demonstrate how minor changes in the environment may significantly affect application performance.</li><li><strong>Chapter 4 “Statistics for Performance Engineers”</strong><br>This chapter contains the essential knowledge about statistics that you need during performance analysis. For each term,
you will find practical recommendations that will help you use statistical metrics during your performance investigations. It also contains some statistical approaches that are really useful for benchmarking. At the end of this chapter, you will find different ways to lie with benchmarking: this knowledge will protect you from incorrect result interpretation.</li><li><strong>Chapter 5 “Performance Analysis and Performance Testing”</strong><br>This chapter covers topics that you need to know if you want to control the performance level in a large product automatically. You will learn different kinds of performance tests, performance anomalies that you can observe, and how to protect yourself from these anomalies. At the end of this chapter, you will find a description of performance-driven development (an approach for writing performance tests) and a general discussion about performance culture.</li><li><strong>Chapter 6 “Diagnostic Tools”</strong><br>This chapter contains a brief overview of different tools that can be useful during performance investigations.</li><li><strong>Chapter 7 “CPU-Bound Benchmarks”</strong><br>This chapter contains 24 case studies that show different pitfalls in CPU-bound benchmarks. We will discuss some runtime- specific features like register allocation, inlining, and intrinsics; and hardware-specific features like instruction-level parallelism, branch prediction, and arithmetics (including IEEE 754).</li><li><strong>Chapter 8 “Memory-Bound Benchmarks”</strong><br>This chapter contains 12 case studies that show different pitfalls in memory-bound benchmarks. We will discuss some runtime- specific features about garbage collection and its settings; and hardware-specific features like CPU cache and physical memory layout.</li><li><strong>Chapter 9 “Hardware and Software Timers”</strong><br>This chapter contains all you need to know about timers. We will discuss basic terminology, different kinds of hardware timers, corresponding timestamping APIs on different operating systems, and the most common pitfalls of using these APIs. This chapter also contains a lot of “extra” content that you don’t actually need for benchmarking, but it may be interesting for people who want to learn more about timers.</li></ul><h2 id=table-of-content>Table of content</h2><ul><li><strong>Chapter 1 “Introducing Benchmarking”</strong><ul><li>Planning a Performance Investigation<ul><li>Define Problems and Goals</li><li>Pick Metrics</li><li>Select Approaches and Tools</li><li>Perform an Experiment to Get the Results</li><li>Complete the Analysis and Draw Conclusions</li></ul></li><li>Benchmarking Goals<ul><li>Performance Analysis</li><li>Benchmarks as a Marketing Tool</li><li>Scientific Interest</li><li>Benchmarking for Fun</li></ul></li><li>Benchmark Requirements<ul><li>Repeatability</li><li>Verifiability and Portability</li><li>Non-Invading Principle</li><li>Acceptable Level of Precision</li><li>Honesty</li></ul></li><li>Performance Spaces<ul><li>Basics</li><li>Performance Model</li><li>Source Code</li><li>Environment</li><li>Input Data</li><li>Distribution</li><li>The Space</li></ul></li><li>Analysis<ul><li>The Bad, the Ugly and the Good</li><li>Find Your Bottleneck</li><li>Statistics</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 2 “Common Benchmarking Pitfalls”</strong><ul><li>General Pitfalls<ul><li>Inaccurate Timestamping</li><li>Executing a Benchmark in the Wrong Way</li><li>Natural Noise</li><li>Tricky Distributions</li><li>Measuring Cold Start instead of Warmed Steady State</li><li>Insufficient Number of Invocations</li><li>Infrastructure Overhead</li><li>Unequal Iterations</li></ul></li><li>.NET-Specific Pitfalls<ul><li>Loop Unrolling</li><li>Dead Code Elimination</li><li>Constant Folding</li><li>Bound Check Elimination</li><li>Inlining</li><li>Conditional Jitting</li><li>Interface Method Dispatching</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 3 “How Environment Affects Performance”</strong><ul><li>Runtime<ul><li>.NET Framework</li><li>.NET Core</li><li>Mono</li><li>Case Study 1: StringBuilder and CLR Versions</li><li>Case Study 2: Dictionary and Randomized String Hashing</li><li>Case Study 3: IList.Count and Unexpected Performance Degradation</li><li>Case Study 4: Build Time and GetLastWriteTime Resolution</li><li>Summing Up</li></ul></li><li>Compilation<ul><li>IL Generation</li><li>Just-In-Time (JIT) Compilation</li><li>Ahead-Of-Time (AOT) Compilation</li><li>Case Study 1: Switch and C* Compiler Versions</li><li>Case Study 2: Params and Memory Allocations</li><li>Case Study 3: Swap and Unobvious IL</li><li>Case Study 4: Huge Methods and Jitting</li><li>Summing Up</li></ul></li><li>External Environment<ul><li>Operating System</li><li>Hardware</li><li>The Physical World</li><li>Case Study 1: Windows Updates and Changes in .NET Framework</li><li>Case Study 2: Meltdown, Spectre, and Critical Patches</li><li>Case Study 3: MSBuild and Windows Defender</li><li>Case Study 4: Pause Latency and Intel Skylake</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 4 “Statistics for Performance Engineers”</strong><ul><li>Descriptive Statistics<ul><li>Basic Sample Plots</li><li>Sample Size</li><li>Minimum, Maximum, and Range</li><li>Mean</li><li>Median</li><li>Quantiles, Quartiles, and Percentiles</li><li>Outliers</li><li>Box Plots</li><li>Frequency Trails</li><li>Modes</li><li>Variance and Standard Deviation</li><li>Normal Distribution</li><li>Skewness</li><li>Kurtosis</li><li>Standard Error and Confidence Intervals</li><li>The Central Limit Theorem</li><li>Summing Up</li></ul></li><li>Performance Analysis<ul><li>Distribution Comparison</li><li>Regression Models</li><li>Optional Stopping</li><li>Pilot Experiments</li><li>Summing Up</li></ul></li><li>How to Lie with Benchmarking<ul><li>Lie with Small Samples</li><li>Lie with Percents</li><li>Lie with Ratios</li><li>Lie with Plots</li><li>Lie with Data Dredging</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 5 “Performance Analysis and Performance Testing”</strong><ul><li>Performance Testing Goals<ul><li>Goal 1: Prevent Performance Degradations</li><li>Goal 2: Detect Not-Prevented Degradations</li><li>Goal 3: Detect Other Kinds of Performance Anomalies</li><li>Goal 4: Reduce Type I Error Rate</li><li>Goal 5: Reduce Type II Error Rate</li><li>Goal 6: Automate Everything</li><li>Summing Up</li></ul></li><li>Kinds of Benchmarks and Performance Tests<ul><li>Cold Start Tests</li><li>Warmed Up Tests</li><li>Asymptotic Tests</li><li>Latency and Throughput Tests</li><li>Unit and Integration Tests</li><li>Monitoring and Telemetry</li><li>Tests With External Dependencies</li><li>Other Kinds of Performance Tests</li><li>Summing Up</li></ul></li><li>Performance Anomalies<ul><li>Degradation</li><li>Acceleration</li><li>Temporal Clustering</li><li>Spatial Clustering</li><li>Huge Duration</li><li>Huge Variance</li><li>Huge Outliers</li><li>Multimodal Distributions</li><li>False Anomalies</li><li>Underlying Problems and Recommendations</li><li>Summing Up</li></ul></li><li>Strategies of Defense<ul><li>Pre-Commit Tests</li><li>Daily Tests</li><li>Retrospective Analysis</li><li>Checkpoints Testing</li><li>Pre-Release Testing</li><li>Manual Testing</li><li>Post-Release Telemetry and Monitoring</li><li>Summing Up</li></ul></li><li>Performance Subpaces<ul><li>Metric Subspace</li><li>Iteration Subspace</li><li>Test Subspace</li><li>Environment Subspace</li><li>Parameter Subspace</li><li>History Subspace</li><li>Summing Up</li></ul></li><li>Performance Asserts and Alarms<ul><li>Absolute Threshold</li><li>Relative Threshold</li><li>Adaptive Threshold</li><li>Manual Threshold</li><li>Summing Up</li></ul></li><li>Performance-Driven Development (PDD)<ul><li>Define a Task and Performance Goals</li><li>Write a Performance Test</li><li>Change the Code</li><li>Check the New Performance Space</li><li>Summing Up</li></ul></li><li>Performance Culture<ul><li>Shared Performance Goals</li><li>Reliable Performance Testing Infrastructure</li><li>Performance Cleanness</li><li>Personal Responsibility</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 6 “Diagnostic Tools”</strong><ul><li>BenchmarkDotNet</li><li>Visual Studio Tools<ul><li>Embedded Profilers</li><li>Disassembly View</li></ul></li><li>JetBrains Tools<ul><li>dotPeek</li><li>dotTrace and dotMemory</li><li>ReSharper</li><li>Rider</li></ul></li><li>Windows Sysinternals<ul><li>RAMMap</li><li>VMMap</li><li>Process Monitor</li></ul></li><li>Other Useful Tools<ul><li>ildasm and ilasm</li><li>monodis</li><li>ILSpy</li><li>dnSpy</li><li>WinDbg</li><li>Asm-Dude</li><li>Mono Console Tools</li><li>PerfView</li><li>perfcollect</li><li>Process Hacker</li><li>Intel VTune Amplifier</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 7 “CPU-Bound Benchmarks”</strong><ul><li>Registers and Stack<ul><li>Case Study 1: Struct Promotion</li><li>Case Study 2: Local Variables</li><li>Case Study 3: Try-Catch</li><li>Case Study 4: Number of Calls</li><li>Summing Up</li></ul></li><li>Inlining<ul><li>Case Study 1: Call Overhead</li><li>Case Study 2: Register Allocation</li><li>Case Study 3: Cooperative Optimizations</li><li>Case Study 4: The &ldquo;starg&rdquo; IL Instruction</li><li>Summing Up</li></ul></li><li>Instruction-Level Parallelism<ul><li>Case Study 1: Parallel Execution</li><li>Case Study 2: Data Dependencies</li><li>Case Study 3: Dependency Graph</li><li>Case Study 4: Extremely Short Loops</li><li>Summing Up</li></ul></li><li>Branch Prediction<ul><li>Case Study 1: Sorted and Unsorted Data</li><li>Case Study 2: Number of Conditions</li><li>Case Study 3: Minimum</li><li>Case Study 4: Patterns</li><li>Summing Up</li></ul></li><li>Arithmetic<ul><li>Case Study 1: Denormalized Numbers</li><li>Case Study 2: Math.Abs</li><li>Case Study 3: double.ToString</li><li>Case Study 4: Integer Division</li><li>Summing Up</li></ul></li><li>Intrinsics<ul><li>Case Study 1: Math.Round</li><li>Case Study 2: Rotate Bits</li><li>Case Study 3: Vectorization</li><li>Case Study 4: System.Runtime.Intrinsics</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 8 “Memory-Bound Benchmarks”</strong><ul><li>CPU Cache<ul><li>Case Study 1: Memory Access Patterns</li><li>Case Study 2: Cache Levels</li><li>Case Study 3: Cache Associativity</li><li>Case Study 4: False Sharing</li><li>Summing Up</li></ul></li><li>Memory Layout<ul><li>Case Study 1: Struct Alignment</li><li>Case Study 2: Cache Bank Conflicts</li><li>Case Study 3: Cache Line Splits</li><li>Case Study 4: 4K Aliasing</li><li>Summing Up</li></ul></li><li>Garbage Collector<ul><li>Case Study 1: GC Modes</li><li>Case Study 2: Nursery Size in Mono</li><li>Case Study 3: Large Object Heaps</li><li>Case Study 4: Finalization</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li><li><strong>Chapter 9 “Hardware and Software Timers”</strong><ul><li>Terminology<ul><li>Time Units</li><li>Frequency Units</li><li>Main Components of a Hardware Timer</li><li>Ticks and Quantizing Errors</li><li>Basic Timer Characteristics</li><li>Summing Up</li></ul></li><li>Hardware Timers<ul><li>TSC</li><li>HPET and ACPI PM</li><li>History of Magic Numbers</li><li>Summing Up</li></ul></li><li>OS Timestamping API<ul><li>Timestamping API on Windows: System Timer</li><li>Timestamping API on Windows: QPC</li><li>Timestamping API on Unix</li><li>Summing Up</li></ul></li><li>.NET Timestamping API<ul><li>DateTime.UtcNow</li><li>Environment.TickCount</li><li>Stopwatch.GetTimestamp</li><li>Summing Up</li></ul></li><li>Timestamping Pitfalls<ul><li>Small Resolution</li><li>Counter Overflow</li><li>Time Components and Total Properties</li><li>Changes in Current Time</li><li>Sequential Reads</li><li>Summing Up</li></ul></li><li>Summary</li></ul></li></ul><h2 id=details>Details</h2><ul><li><strong>Title:</strong> Pro .NET Benchmarking</li><li><strong>Subtitle:</strong> The Art of Performance Measurement</li><li><strong>Author:</strong> Andrey Akinshin</li><li><strong>Publisher:</strong>: Apress</li><li><strong>Edition:</strong> 1</li><li><strong>Date:</strong> June, 2019</li><li><strong>Number of Pages:</strong> 690</li><li><strong>Number of Illustrations:</strong> 65</li><li><strong>Language:</strong> English</li><li><strong>Softcover ISBN-10:</strong> 1484249402</li><li><strong>Softcover ISBN-13:</strong> 978-1-4842-4940-6</li><li><strong>eBook ISBN-13:</strong> 978-1-4842-4941-3</li><li><strong>DOI:</strong> 10.1007/978-1-4842-4941-3</li></ul><p><strong>How to cite:</strong>
Akinshin, Andrey.
<em>Pro .NET Benchmarking.</em>
Apress, 2019.</p><p><strong>BibTeX reference:</strong></p><div class=highlight><pre class=chroma><code class=language-tex data-lang=tex>@book<span class=nb>{</span>Akinshin2019,
  author    = <span class=nb>{</span>Akinshin, Andrey<span class=nb>}</span>, 
  title     = <span class=nb>{</span>Pro .NET Benchmarking<span class=nb>}</span>,
  publisher = <span class=nb>{</span>Apress<span class=nb>}</span>,
  year      = <span class=nb>{</span>2019<span class=nb>}</span>,
  edition   = <span class=nb>{</span>1<span class=nb>}</span>,
  pages     = <span class=nb>{</span>690<span class=nb>}</span>,
  isbn      = <span class=nb>{</span>978-1-4842-4940-6<span class=nb>}</span>,
  doi       = <span class=nb>{</span>10.1007/978-1-4842-4941-3<span class=nb>}</span>
<span class=nb>}</span>
</code></pre></div></main></div><footer class=blog-footer><div class=container><p>&copy; 2013–2020 Андрей Акиньшин | <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.ea4f51ac7f59c8e13f4df787df946eb1a447174ce2f1fc5349483de93a607ecb.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/anchor.min.js></script><script src=https://aakinshin.net/js/custom.min.11932490dde776463ed165b345838c701accc0cfdedf2e0868f13415f41f0872.js></script></body></html>