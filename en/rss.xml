<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Andrey Akinshin</title><link>https://aakinshin.net/posts/</link><description>Recent content in Posts on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 28 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aakinshin.net/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Beta distribution highest density interval of the given width</title><link>https://aakinshin.net/posts/beta-hdi/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/beta-hdi/</guid><description>&lt;p>In one of &lt;a href="https://aakinshin.net/posts/kosqe5/">the previous posts&lt;/a>, I discussed the idea of the trimmed Harrell-Davis quantile estimator
based on the highest density interval of the given width.
Since the Harrell-Davis quantile estimator uses the Beta distribution,
we should be able to find the beta distribution highest density interval of the given width.
In this post, I will show how to do this.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 8: Winsorized Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe8/</link><pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe8/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe7/">previous post&lt;/a>, we have discussed
the trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of size &lt;span class="math inline">\(\sqrt{n}/n\)&lt;/span>.
This quantile estimator showed a decent level of statistical efficiency.
However, the research wouldn&amp;rsquo;t be complete without comparison with the winsorized modification.
Let&amp;rsquo;s fix it!&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 7: Optimal threshold for the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe7/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe7/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe6/">previous post&lt;/a>, we have obtained a nice quantile estimator.
To be specific, we considered a trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of the given size.
The interval size is a parameter that controls the trade-off between statistical efficiency and robustness.
While it&amp;rsquo;s nice to have the ability to control this trade-off, there is also a need for the default value,
which could be used as a starting point
when we have neither estimator breakdown point requirements nor prior knowledge about distribution properties.&lt;/p>
&lt;p>After a series of unsuccessful attempts, it seems that I have found an acceptable solution.
We should build the new estimator based on &lt;span class="math inline">\(\sqrt{n}/n\)&lt;/span> order statistics.
In this post, I&amp;rsquo;m going to briefly explain the idea behind the suggested estimator and
share some numerical simulations that compare the proposed estimator
and the classic Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 6: Continuous trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe6/</link><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe6/</guid><description>&lt;p>In my &lt;a href="https://aakinshin.net/posts/kosqe5/">previous post&lt;/a>,
I tried the idea of using the trimmed modification of the Harrell-Davis quantile estimator
based on the highest density interval of the given width.
The width was defined so that it covers exactly k order statistics (the width equals &lt;span class="math inline">\((k-1)/n\)&lt;/span>).
I was pretty satisfied with the result and decided to continue evolving this approach.
While &amp;ldquo;k order statistics&amp;rdquo; is a good mental model that described the trimmed interval,
it doesn&amp;rsquo;t actually require an integer k.
In fact, we can use any real number as the trimming percentage.&lt;/p>
&lt;p>In this post, we are going to perform numerical simulations that check the statistical efficiency
of the trimmed Harrell-Davis quantile estimator with different trimming percentages.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 5: Improving trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe5/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe5/</guid><description>&lt;p>During the last several months,
I have been experimenting with different variations of the trimmed Harrell-Davis quantile estimator.
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">My original idea&lt;/a>
of using the highest density interval based on the fixed area percentage (e.g., HDI 95% or HDI 99%)
led to a set of problems with &lt;a href="https://aakinshin.net/posts/thdqe-overtrimming/">overtrimming&lt;/a>.
I tried to solve them with &lt;a href="https://aakinshin.net/posts/customized-wthdqe/">manually customized&lt;/a> trimming strategy,
but this approach turned out to be too inconvenient;
it was too hard to come up with &lt;a href="https://aakinshin.net/posts/thdqe-threshold/">optimal thresholds&lt;/a>.
One of the main problems was about the suboptimal number of elements
that we actually aggregate to obtain the quantile estimation.
So, I decided to try an &lt;a href="https://aakinshin.net/posts/kosqe1/">approach that involves exactly k order statistics&lt;/a>.
The idea was so promising,
but numerical simulations &lt;a href="https://aakinshin.net/posts/kosqe4/">haven&amp;rsquo;t shown&lt;/a> the appropriate efficiency level.&lt;/p>
&lt;p>This bothered me the whole week.
It sounded so reasonable to trim the Harrell-Davis quantile estimator using exactly k order statistics.
Why didn&amp;rsquo;t this work as expected?
Finally, I have found a fatal flaw in &lt;a href="https://aakinshin.net/posts/kosqe4/">my previous approach&lt;/a>:
while it was a good idea to fix the size of the trimming window,
I mistakenly chose its location following the equation from the Hyndman-Fan Type 7 quantile estimator!&lt;/p>
&lt;p>In this post, we fix this problem and try another modification of the trimmed Harrell-Davis quantile estimator based on
k order statistics &lt;strong>and&lt;/strong> highest density intervals at the same time.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 4: Adopting trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/kosqe4/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe4/</guid><description>&lt;p>In the previous posts, I discussed various aspects of
&lt;a href="https://aakinshin.net/posts/kosqe1/">quantile estimators based on k order statistics&lt;/a>.
I already tried a few weight functions that aggregate the sample values to the quantile estimators
(see posts about &lt;a href="https://aakinshin.net/posts/kosqe2/">an extension of the Hyndman-Fan Type 7 equation&lt;/a> and
about &lt;a href="https://aakinshin.net/posts/kosqe3/">adjusted regularized incomplete beta function&lt;/a>).
In this post, I continue my experiments and try to adopt the
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed modifications of the Harrell-Davis quantile estimator&lt;/a> to this approach.&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 3: Playing with the Beta function</title><link>https://aakinshin.net/posts/kosqe3/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe3/</guid><description>&lt;p>In the previous two posts, I discussed the idea of quantile estimators based on k order statistics.
A already covered the &lt;a href="https://aakinshin.net/posts/kosqe1/">motivation behind this idea&lt;/a>
and the statistical efficiency of such estimators using the &lt;a href="https://aakinshin.net/posts/kosqe2/">extended Hyndman-Fan equations&lt;/a>
as a weight function.
Now it&amp;rsquo;s time to experiment with the Beta function as a primary way to aggregate k order statistics
into a single quantile estimation!&lt;/p></description></item><item><title>Quantile estimators based on k order statistics, Part 2: Extending Hyndman-Fan equations</title><link>https://aakinshin.net/posts/kosqe2/</link><pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe2/</guid><description>&lt;p>In the &lt;a href="https://aakinshin.net/posts/kosqe1/">previous post&lt;/a>,
I described the idea of using quantile estimators based on k order statistics.
Potentially, such estimators could be more robust than estimators based on all samples elements (like
Harrell-Davis,
&lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis&lt;/a>, or
&lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir&lt;/a>)
and more statistically efficient than traditional quantile estimators (based on 1 or 2 order statistics).
Moreover, we should be able to control this trade-off based on the business requirements
(e.g., setting the desired breakdown point).&lt;/p>
&lt;p>The only challenging thing here is choosing the weight function
that aggregates k order statistics to a single quantile estimation.
We are going to try several options, perform Monte-Carlo simulations for each of them, and compare the results.
A reasonable starting point is an extension of the traditional quantile estimators.
In this post, we are going to extend the Hyndman-Fan Type 7 quantile estimator
(nowadays, it&amp;rsquo;s one of the most popular estimators).
It estimates quantiles as a linear interpolation of two subsequent order statistics.
We are going to make some modifications, so a new version is going to be based on k order statistics.&lt;/p>
&lt;p>&lt;strong>Spoiler: this approach doesn&amp;rsquo;t seem like an optimal one.&lt;/strong>
I&amp;rsquo;m pretty disappointed with its statistical efficiency on samples from light-tailed distributions.
So, what&amp;rsquo;s the point of writing a blog post about an inefficient approach?
Because of the following reasons:&lt;/p>
&lt;ol>
&lt;li>I believe it&amp;rsquo;s crucial to share negative results.
Sometimes, knowledge about approaches that don&amp;rsquo;t work
could be more important than knowledge about more effective techniques.
Negative results give you a broader view of the problem
and protect you from wasting your time on potential promising (but not so useful) ideas.&lt;/li>
&lt;li>Negative results improve research completeness.
When we present an approach, it&amp;rsquo;s essential to not only show why it solves problems well,
but also why it solves problems better than other similar approaches.&lt;/li>
&lt;li>While I wouldn&amp;rsquo;t recommend my extension of the Hyndman-Fan Type 7 quantile estimator to the k order statistics case
as the default quantile estimator, there are some specific cases where it could be useful.
For example, if we estimate the median based on small samples from a symmetric light-tailed distribution,
it could outperform not only the original version but also the Harrell-Davis quantile estimator.
The &amp;ldquo;negativity&amp;rdquo; of the negative results always exists in a specific context.
So, there may be cases when negative results for the general case transform to positive results
for a particular niche problem.&lt;/li>
&lt;li>Finally, it&amp;rsquo;s my personal blog, so I have the freedom to write on any topic I like.
My blog posts are not publications to scientific journals (which typically don&amp;rsquo;t welcome negative results),
but rather research notes about conducted experiments.
It&amp;rsquo;s important for me to keep records of all the experiments I perform regardless of the usefulness of the results.&lt;/li>
&lt;/ol>
&lt;p>So, let&amp;rsquo;s briefly look at the results of this not-so-useful approach.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-light.png" target="_blank" class="imgldlink" alt="LightAndHeavy__N15_Efficiency">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/kosqe2/img/LightAndHeavy__N15_Efficiency-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Quantile estimators based on k order statistics, Part 1: Motivation</title><link>https://aakinshin.net/posts/kosqe1/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kosqe1/</guid><description>&lt;p>It&amp;rsquo;s not easy to choose a good quantile estimator.
In my previous posts, I considered several groups of quantile estimators:&lt;/p>
&lt;ul>
&lt;li>Quantile estimators based 1 or 2 order statistics (Hyndman-Fan Type1-9)&lt;/li>
&lt;li>Quantile estimators based on all order statistics
(the Harrell-Davis quantile estimator,
the &lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis quantile estimator&lt;/a>, and
the &lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir quantile estimator&lt;/a>)&lt;/li>
&lt;li>Quantile estimators based on a variable number of order statistics
(the &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a> and &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> modifications
of the Harrell-Davis quantile estimator)&lt;/li>
&lt;/ul>
&lt;p>Unfortunately, all of these estimators have significant drawbacks
(e.g., poor statistical efficiency or poor robustness).
In this post, I want to discuss all of the advantages and disadvantages of each approach
and suggest another family of quantile estimators that are based on k order statistics.&lt;/p></description></item><item><title>Avoiding over-trimming with the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-overtrimming/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-overtrimming/</guid><description>&lt;p>Previously, I already discussed the
&lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed modification of the Harrell-Davis quantile estimator&lt;/a> several times.
I performed several numerical simulations that compare the statistical efficiency of this estimator
with the efficiency of the &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">classic Harrell-Davis quantile estimator&lt;/a> (HDQE)
and its &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized modification&lt;/a>;
I showed how we can improve the efficiency using &lt;a href="https://aakinshin.net/posts/customized-wthdqe/">custom trimming strategies&lt;/a>
and how to choose a &lt;a href="https://aakinshin.net/posts/thdqe-threshold/">good trimming threshold value&lt;/a>.&lt;/p>
&lt;p>In the heavy-tailed cases, the trimmed HDQE provides better estimations than the classic HDQE
because of its higher breakdown point.
However, in the light-tailed cases, we could get efficiency that is worse than
the baseline Hyndman-Fan Type 7 (HF7) quantile estimator.
In many cases, such an effect arises because of the over-trimming effect.
If the trimming percentage is too high or if the evaluated quantile is too far from the median,
the trimming strategy based on the highest-density interval may lead to an estimation
that is based on single order statistics.
In this case, we get an efficiency level similar to the Hyndman-Fan Type 1-3 quantile estimators
(which are also based on single order statistics).
In the light-tailed case, such a result is less preferable than Hyndman-Fan Type 4-9 quantile estimators
(which are based on two subsequent order statistics).&lt;/p>
&lt;p>In order to improve the situation, we could introduce the lower bound for the number of order statistics
that contribute to the final quantile estimations.
In this post, I look at some numerical simulations
that compare trimmed HDQEs with different lower bounds.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png" target="_blank" class="imgldlink" alt="LightAndHeavy__N05_Efficiency">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/thdqe-overtrimming/img/LightAndHeavy__N05_Efficiency-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Optimal threshold of the trimmed Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/thdqe-threshold/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/thdqe-threshold/</guid><description>&lt;p>The traditional quantile estimators (which are based on 1 or 2 order statistics) have great robustness.
However, the statistical efficiency of these estimators is not so great.
The Harrell-Davis quantile estimator has much better efficiency (at least in the light-tailed case),
but it&amp;rsquo;s not robust (because it calculates a weighted sum of all sample values).
I already wrote a &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">post about trimmed Harrell-Davis quantile estimator&lt;/a>:
this approach suggest dropping some of the low-weight sample values to improve robustness
(keeping good statistical efficiency).
I also perform a numerical simulations that &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">compare efficiency&lt;/a>
of the original Harrell-Davis quantile estimator against its trimmed and winsorized modifications.
It&amp;rsquo;s time to discuss how to choose the optimal trimming threshold
and how it affects the estimator efficiency.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png" target="_blank" class="imgldlink" alt="LightAndHeavy__N40_Efficiency">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/thdqe-threshold/img/LightAndHeavy__N40_Efficiency-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Estimating quantile confidence intervals: Maritz-Jarrett vs. jackknife</title><link>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/maritz-jarrett-vs-jackknife/</guid><description>&lt;p>When it comes to estimating quantiles of the given sample,
my estimator of choice is the Harrell-Davis quantile estimator
(to be more specific, its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed version&lt;/a>).
If I need to get a confidence interval for the obtained quantiles,
I use the &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/#the-maritz-jarrett-method">Maritz-Jarrett method&lt;/a>
because it provides a &lt;a href="https://aakinshin.net/posts/quantile-ci-coverage/">decent coverage percentage&lt;/a>.
Both approaches work pretty nicely together.&lt;/p>
&lt;p>However, in the original paper by &lt;a href="https://doi.org/10.2307/2335999">Harrell and Davis (1982)&lt;/a>,
the authors suggest using the jackknife variance estimator in order to get the confidence intervals.
The obvious question here is which approach better: the Maritz-Jarrett method or the jackknife estimator?
In this post, I perform a numerical simulation that compares both techniques using different distributions.&lt;/p></description></item><item><title>Using Kish's effective sample size with weighted quantiles</title><link>https://aakinshin.net/posts/kish-ess-weighted-quantiles/</link><pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kish-ess-weighted-quantiles/</guid><description>&lt;p>In my previous posts, I described how to calculate
&lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted quantiles&lt;/a> and
their &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">confidence intervals&lt;/a>
using the Harrell-Davis quantile estimator.
This powerful technique allows applying
&lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">quantile exponential smoothing&lt;/a> and
&lt;a href="https://aakinshin.net/posts/dispersion-exponential-smoothing/">dispersion exponential smoothing&lt;/a> for
time series in order to get its moving properties.&lt;/p>
&lt;p>When we work with weighted samples, we need a way to calculate the
&lt;a href="https://en.wikipedia.org/wiki/Effective_sample_size">effective samples size&lt;/a>.
Previously, I used the sum of all weights normalized by the maximum weight.
In most cases, it worked OK.&lt;/p>
&lt;p>Recently, &lt;a href="https://www.soz.unibe.ch/about_us/people/prof_dr_jann_ben/index_eng.html">Ben Jann&lt;/a> pointed out
that it would be better to use the Kish&amp;rsquo;s formula to calculate the effective sample size.
In this post, you find the formula and a few numerical simulations that illustrate the actual impact of
the underlying sample size formula.&lt;/p></description></item><item><title>Partial binning compression of performance series</title><link>https://aakinshin.net/posts/partial-binning-compression/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partial-binning-compression/</guid><description>&lt;p>Let&amp;rsquo;s start with a problem from real life.
Imagine we have thousands of application components that should be initialized.
We care about the total initialization time of the whole application,
so we want to automatically track the slowest components using a continuous integration (CI) system.
The easiest way to do it is to measure the initialization time of each component in each CI build
and save all the measurements to a database.
Unfortunately, if the total number of components is huge, the overall artifact size may be quite extensive.
Thus, this approach may introduce an unwanted negative impact on the database size and data processing time.&lt;/p>
&lt;p>However, we don&amp;rsquo;t actually need all the measurements.
We want to track only the slowest components.
Typically, it&amp;rsquo;s possible to introduce a reasonable threshold that defines such components.
For example, we can say that all components that are initialized in less than 1ms are &amp;ldquo;fast enough,&amp;rdquo;
so there is no need to know the exact initialization time for them.
Since these time values are insignificant, we can just omit all the measurements below the given thresholds.
This allows to significantly reduce the data traffic without losing any important information.&lt;/p>
&lt;p>The suggested trick can be named &lt;em>partial binning compression&lt;/em>.
Indeed, we introduce a single bin (perform &lt;em>binning&lt;/em>) and
omit all the values inside this bin (perform &lt;em>compression&lt;/em>).
On the other hand, we don&amp;rsquo;t build an honest histogram since we keep all the raw values outside the given bin
(the binning is &lt;em>partial&lt;/em>).&lt;/p>
&lt;p>Let&amp;rsquo;s discuss a few aspects of using partial binning compression.&lt;/p></description></item><item><title>Calculating gamma effect size for samples with zero median absolute deviation</title><link>https://aakinshin.net/posts/zero-mad-gamma-es/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/zero-mad-gamma-es/</guid><description>&lt;p>In previous posts, I discussed the &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">gamma effect size&lt;/a>
which is a Cohen&amp;rsquo;s d-consistent nonparametric and robust measure of the effect size.
Also, I discussed &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size2/">various ways to customize this metric&lt;/a>
and adjust it to different kinds of business requirements.
In this post, I want to briefly cover one more corner case that requires special adjustments.
We are going to discuss the situation when the median absolute deviation is zero.&lt;/p></description></item><item><title>Discrete performance distributions</title><link>https://aakinshin.net/posts/discrete-performance-distributions/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/discrete-performance-distributions/</guid><description>&lt;p>When we collect software performance measurements, we get a bunch of time intervals.
Typically, we tend to interpret time values as continuous values.
However, the obtained values are actually discrete due to the limited resolution of our measurement tool.
In simple cases, we can treat these discrete values as continuous and get meaningful results.
Unfortunately, discretization may produce strange phenomena like pseudo-multimodality or zero dispersion.
If we want to set up a reliable system that automatically analyzes such distributions,
we should be aware of such problems so we could correctly handle them.&lt;/p>
&lt;p>In this post, I want to share a few of discretization problems in real-life performance data sets
(based on the &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> performance tests).&lt;/p></description></item><item><title>Customization of the nonparametric Cohen's d-consistent effect size</title><link>https://aakinshin.net/posts/nonparametric-effect-size2/</link><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nonparametric-effect-size2/</guid><description>&lt;p>One year ago, I publish a post called &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">Nonparametric Cohen&amp;#39;s d-consistent effect size&lt;/a>.
During this year, I got a lot of internal and external feedback from
my own statistical experiments and
&lt;a href="https://twitter.com/ViljamiSairanen/status/1400457118340108293">people&lt;/a>
&lt;a href="https://sherbold.github.io/autorank/autorank/">who&lt;/a>
&lt;a href="https://github.com/Ramon-Diaz/Thesis-Project/blob/85df6b11050c7e05c4394d873585f701a7e3f32e/_util.py#L100">tried&lt;/a>
to use the suggested approach.
It seems that the nonparametric version of Cohen&amp;rsquo;s d works much better with real-life not-so-normal data.
While the classic Cohen&amp;rsquo;s d based on
the non-robust arithmetic mean and
the &lt;a href="https://aakinshin.net/posts/misleading-stddev/">non-robust standard deviation&lt;/a>
can be easily &lt;a href="https://aakinshin.net/posts/cohend-and-outliers/">corrupted by a single outlier&lt;/a>,
my approach is much more resistant to unexpected extreme values.
Also, it allows exploring
&lt;a href="https://aakinshin.net/posts/comparing-distributions-using-gamma-es/">the difference between specific quantiles of considered samples&lt;/a>,
which can be useful in the non-parametric case.&lt;/p>
&lt;p>However, I wasn&amp;rsquo;t satisfied with the results of all of my experiments.
While I still like the basic idea
(replace the mean with the median; replace the standard deviation with the median absolute deviation),
it turned out that the final results heavily depend on the used quantile estimator.
To be more specific, the original Harrell-Davis quantile estimator is not always optimal;
in most cases, it&amp;rsquo;s better to replace it with its &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a> modification.
However, the particular choice of the quantile estimators depends on the situation.
Also, the consistency constant for the median absolute deviation
should be adjusted according to the current sample size and the used quantile estimator.
Of course, it also can be replaced by other dispersion estimators
that can be used as consistent estimators of the standard deviation.&lt;/p>
&lt;p>In this post, I want to get a brief overview of possible customizations of the suggested metrics.&lt;/p></description></item><item><title>Robust alternative to statistical efficiency</title><link>https://aakinshin.net/posts/robust-statistical-efficiency/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/robust-statistical-efficiency/</guid><description>&lt;p>Statistical efficiency is a common measure of the quality of an estimator.
Typically, it&amp;rsquo;s expressed via the mean square error (&lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span>).
For the given estimator &lt;span class="math inline">\(T\)&lt;/span> and the true parameter value &lt;span class="math inline">\(\theta\)&lt;/span>,
the &lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span> can be expressed as follows:&lt;/p>
&lt;p>&lt;span class="math display">\[\operatorname{MSE}(T) = \operatorname{E}[(T-\theta)^2]
\]&lt;/span>&lt;/p>
&lt;p>In numerical simulations, the &lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span> can&amp;rsquo;t be used as a robust metric
because its breakdown point is zero
(a corruption of a single measurement leads to a corrupted result).
Typically, it&amp;rsquo;s not a problem for light-tailed distributions.
Unfortunately, in the heavy-tailed case,
the &lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span> becomes an unreliable and unreproducible metric
because it can be easily spoiled by a single outlier.&lt;/p>
&lt;p>I suggest an alternative way to compare statistical estimators.
Instead of using non-robust &lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span>,
we can use robust quantile estimations of the absolute error distribution.
In this post, I want to share numerical simulations
that show a problem of irreproducible &lt;span class="math inline">\(\operatorname{MSE}\)&lt;/span> values
and how they can be replaced by reproducible quantile values.&lt;/p></description></item><item><title>Improving the efficiency of the Harrell-Davis quantile estimator for special cases using custom winsorizing and trimming strategies</title><link>https://aakinshin.net/posts/customized-wthdqe/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/customized-wthdqe/</guid><description>&lt;p>Let&amp;rsquo;s say we want to
&lt;strong>estimate the median&lt;/strong>
based on a &lt;strong>small sample&lt;/strong> (3 &lt;span class="math inline">\(\leq n \leq 7\)&lt;/span>)
from a &lt;strong>right-skewed heavy-tailed distribution&lt;/strong>
with &lt;strong>high statistical efficiency&lt;/strong>.&lt;/p>
&lt;p>The traditional median estimator is the most robust estimator, but it&amp;rsquo;s not the most efficient one.
Typically, the Harrell-Davis quantile estimator provides better efficiency,
but it&amp;rsquo;s not robust (its breakdown point is zero),
so it may have worse efficiency in the given case.
The &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>
modifications of the Harrell-Davis quantile estimator provide a good trade-off
between efficiency and robustness, but they require a proper winsorizing/trimming rule.
A reasonable choice of such a rule for medium-size samples is based on the highest density interval of the Beta function
(as described &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">here&lt;/a>).
Unfortunately, this approach may be suboptimal for small samples.
E.g., if we use the 99% highest density interval to estimate the median,
it starts to trim sample values only for &lt;span class="math inline">\(n \geq 8\)&lt;/span>.&lt;/p>
&lt;p>In this post, we are going to discuss custom winsorizing/trimming strategies for special cases of the quantile estimation problem.&lt;/p></description></item><item><title>Comparing the efficiency of the Harrell-Davis, Sfakianakis-Verginis, and Navruz-Özdemir quantile estimators</title><link>https://aakinshin.net/posts/hd-sv-no-efficiency/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hd-sv-no-efficiency/</guid><description>&lt;p>In the previous posts, I discussed the statistical efficiency of different quantile estimators
(&lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">Efficiency of the Harrell-Davis quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators&lt;/a>).&lt;/p>
&lt;p>In this post, I continue this research and compare the efficiency of
the Harrell-Davis quantile estimator,
the &lt;a href="https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/">Sfakianakis-Verginis quantile estimators&lt;/a>, and
the &lt;a href="https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/">Navruz-Özdemir quantile estimator&lt;/a>.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" class="imgldlink" alt="LightAndHeavy_N10_Efficiency">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Dispersion exponential smoothing</title><link>https://aakinshin.net/posts/dispersion-exponential-smoothing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dispersion-exponential-smoothing/</guid><description>&lt;p>In this &lt;a href="https://aakinshin.net/posts/quantile-exponential-smoothing/">previous post&lt;/a>,
I showed how to apply exponential smoothing to quantiles
using the &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">weighted Harrell-Davis quantile estimator&lt;/a>.
This technique allows getting smooth and stable moving median estimations.
In this post, I&amp;rsquo;m going to discuss how to use the same approach
to estimate moving dispersion.&lt;/p></description></item><item><title>Quantile exponential smoothing</title><link>https://aakinshin.net/posts/quantile-exponential-smoothing/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-exponential-smoothing/</guid><description>&lt;p>One of the popular problems in time series analysis is estimating the moving &amp;ldquo;average&amp;rdquo; value.
Let&amp;rsquo;s define the &amp;ldquo;average&amp;rdquo; as a central tendency metric like the mean or the median.
When we talk about the moving value, we assume that we are interested in
the average value &amp;ldquo;at the end&amp;rdquo; of the time series
instead of the average of all available observations.&lt;/p>
&lt;p>One of the most straightforward approaches to estimate the moving average is the &lt;em>simple moving mean&lt;/em>.
Unfortunately, this approach is not robust: outliers can instantly spoil the evaluated mean value.
As an alternative, we can consider &lt;em>simple moving median&lt;/em>.
I already discussed a few of such methods:
&lt;a href="https://aakinshin.net/posts/mp2-quantile-estimator/">the MP² quantile estimator&lt;/a> and
&lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/">a moving quantile estimator based on partitioning heaps&lt;/a>
(a modification of the Hardle-Steiger method).
When we talk about &lt;em>simple moving averages&lt;/em>, we typically assume
that we estimate the average value over the last &lt;span class="math inline">\(k\)&lt;/span> observations (&lt;span class="math inline">\(k\)&lt;/span> is the &lt;em>window size&lt;/em>).
This approach is also known as &lt;em>unweighted moving averages&lt;/em> because
all target observations have the same weight.&lt;/p>
&lt;p>As an alternative to the simple moving average, we can also consider the &lt;em>weighted moving average&lt;/em>.
In this case, we assign a weight for each observation and aggregate the whole time series according to these weights.
A famous example of such a weight function is &lt;em>exponential smoothing&lt;/em>.
And the simplest form of exponential smoothing is the &lt;em>exponentially weighted moving mean&lt;/em>.
This approach estimates the weighted moving mean using exponentially decreasing weights.
Switching from the simple moving mean to the exponentially weighted moving mean provides some benefits
in terms of smoothness and estimation efficiency.&lt;/p>
&lt;p>Although exponential smoothing has advantages over the simple moving mean,
it still estimates the mean value which is not robust.
We can improve the robustness of this approach if we reuse the same idea for weighted moving quantiles.
It&amp;rsquo;s possible because the quantiles also can be estimated for weighted samples.
In one of my previous posts, I &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">showed&lt;/a> how to adapt
the Hyndman-Fan Type 7 and Harrell-Davis quantile estimators to the weighted samples.
In this post, I&amp;rsquo;m going to show how we can use this technique to estimate
the weighted moving quantiles using exponentially decreasing weights.&lt;/p></description></item><item><title>Improving quantile-respectful density estimation for discrete distributions using jittering</title><link>https://aakinshin.net/posts/qrde-discrete/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qrde-discrete/</guid><description>&lt;p>In my previous posts, I already discussed the &lt;a href="https://aakinshin.net/posts/kde-discrete/">problem&lt;/a> that arise
when we try to build the kernel density estimation (KDE) for samples with ties.
We may get such samples in real life from discrete or mixed discrete/continuous distributions.
Even if the original distribution is continuous,
we may observe artificial sample discretization due to a limited resolution of the measuring tool.
Such discretization may lead to inaccurate density plots due to undersmoothing.
The problem can be resolved using a nice technique called &lt;em>jittering&lt;/em>.
I also discussed &lt;a href="https://aakinshin.net/posts/discrete-sample-jittering/">how to apply&lt;/a> jittering to get a smoother version of KDE.&lt;/p>
&lt;p>However, I&amp;rsquo;m not a huge fan of KDE because of two reasons.
The first one is the &lt;a href="https://aakinshin.net/posts/kde-bw/">problem of choosing a proper bandwidth value&lt;/a>.
With poorly chosen bandwidth, we can easily get oversmoothing or undersmoothing even without the discretization problem.
The second one is an inconsistency between the KDE-based probability density function and evaluated sample quantiles.
It could lead to inconsistent visualizations (e.g., KDE-based violin plots with non-KDE-based quantile values)
or it could introduce problems for algorithms that require density function and quantile values at the same time.
The inconsistency could be resolved using &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimation&lt;/a> (QRDE).
This kind of estimation builds the density function which matches the evaluated sample quantiles.
To get a smooth QRDE, we also need a smooth quantile estimator like the Harrell-Davis quantile estimator.
The robustness and componential efficiency of this approach can be improved using
the &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>
modifications of the Harrell-Davis quantile estimator
(which also have a &lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/">decent statistical efficiency level&lt;/a>).&lt;/p>
&lt;p>Unfortunately, the straightforward QRDE calculation is not always applicable for samples with ties
because it&amp;rsquo;s impossible to build an &amp;ldquo;honest&amp;rdquo; density function for discrete distributions
without using the Dirac delta function.
This is a severe problem for QRDE-based algorithms like the
&lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/">lowland multimodality detection algorithm&lt;/a>.
In this post, I will show how jittering could help to solve this problem and get a smooth QRDE on samples with ties.&lt;/p></description></item><item><title>How to build a smooth density estimation for a discrete sample using jittering</title><link>https://aakinshin.net/posts/discrete-sample-jittering/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/discrete-sample-jittering/</guid><description>&lt;p>Let&amp;rsquo;s say you have a sample with tied values.
If you draw a kernel density estimation (KDE) for such a sample,
you may get a serrated pattern like this:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-light.png" target="_blank" class="imgldlink" alt="intro">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/discrete-sample-jittering/img/intro-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>KDE requires samples from continuous distributions
while tied values arise in discrete or mixture distributions.
Even if the original distribution is continuous,
you may observe artificial sample discretization due to a limited resolution of the measuring tool.
This effect may lead to distorted density plots like in the above picture.&lt;/p>
&lt;p>The problem could be solved using a nice technique called &lt;em>jittering&lt;/em>.
In the simplest case, jittering just adds random noise to each measurement.
Such a trick removes all ties from the sample and allows building a smooth density estimation.&lt;/p>
&lt;p>However, there are many different ways to apply jittering.
The trickiest question here is how to choose proper noise values.
In this post, I want to share one of my favorite jittering approaches.
It generates a non-randomized noise pattern with a low risk of noticeable sample corruption.&lt;/p></description></item><item><title>Kernel density estimation and discrete values</title><link>https://aakinshin.net/posts/kde-discrete/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kde-discrete/</guid><description>&lt;p>Kernel density estimation (KDE) is a popular technique of data visualization.
Based on the given sample, it allows estimating the probability density function (PDF) of the underlying distribution.
Here is an example of KDE for &lt;code>x = {3.82, 4.61, 4.89, 4.91, 5.31, 5.6, 5.66, 7.00, 7.00, 7.00}&lt;/code>
(normal kernel, Sheather &amp;amp; Jones bandwidth selector):&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/kde-discrete/img/intro-light.png" target="_blank" class="imgldlink" alt="intro">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/kde-discrete/img/intro-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/kde-discrete/img/intro-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/kde-discrete/img/intro-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>KDE is a simple and straightforward way to build a PDF, but it&amp;rsquo;s not always the best one.
In addition to my &lt;a href="https://aakinshin.net/posts/kde-bw/">concerns about bandwidth selection&lt;/a>,
continuous use of KDE creates an illusion that all distributions are smooth and continuous.
In practice, it&amp;rsquo;s not always true.&lt;/p>
&lt;p>In the above picture, the distribution looks pretty continuous.
However, the picture hides the fact that we have three &lt;code>7.00&lt;/code> elements in the original sample.
With continuous distributions, the probability of getting tied observations (that have the same value) is almost zero.
If a sample contains ties, we are most likely working with
either a discrete distribution or a mixture of discrete and continuous distributions.
A KDE for such a sample may significantly differ from the actual PDF.
Thus, this technique may mislead us instead of providing insights about the true underlying distribution.&lt;/p>
&lt;p>In this post, we discuss the usage of PDF and PMF with continuous and discrete distributions.
Also, we look at examples of corrupted density estimation plots for distributions with discrete features.&lt;/p></description></item><item><title>Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators</title><link>https://aakinshin.net/posts/wthdqe-efficiency/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wthdqe-efficiency/</guid><description>&lt;p>In previous posts, I suggested two modifications of the Harrell-Davis quantile estimator:
&lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">winsorized&lt;/a> and &lt;a href="https://aakinshin.net/posts/trimmed-hdqe/">trimmed&lt;/a>.
Both modifications have a higher level of robustness in comparison to the original estimator.
Also, I &lt;a href="https://aakinshin.net/posts/hdqe-efficiency/">discussed&lt;/a> the &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>
of the Harrell-Davis quantile estimator.
In this post, I&amp;rsquo;m going to continue numerical simulation and estimate the efficiency of
the winsorized and trimmed modifications.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png" target="_blank" class="imgldlink" alt="LightAndHeavy_N10_Efficiency">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/wthdqe-efficiency/img/LightAndHeavy_N10_Efficiency-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Trimmed modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/trimmed-hdqe/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/trimmed-hdqe/</guid><description>&lt;p>In one of &lt;a href="https://aakinshin.net/posts/winsorized-hdqe/">the previous posts&lt;/a>, I discussed winsorized Harrell-Davis quantile estimator.
This estimator is more robust than the classic Harrell-Davis quantile estimator.
In this post, I want to suggest another modification that may be better for some corner cases:
the &lt;em>trimmed&lt;/em> Harrell-Davis quantile estimator.&lt;/p></description></item><item><title>Efficiency of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/hdqe-efficiency/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/hdqe-efficiency/</guid><description>&lt;p>One of the most essential properties of a quantile estimator is
its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
In simple words, the efficiency describes the estimator accuracy.
The Harrell-Davis quantile estimator is a good option to achieve higher efficiency.
However, this estimator may provide lower efficiency in some special cases.
In this post, we will conduct a set of simulations that show the actual efficiency numbers.
We compare different distributions (symmetric and right-skewed, heavy-tailed and light-tailed),
quantiles, and sample sizes.&lt;/p></description></item><item><title>Navruz-Özdemir quantile estimator</title><link>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/</guid><description>&lt;p>The Navruz-Özdemir quantile estimator
suggests the following equation to estimate the &lt;span class="math inline">\(p^\textrm{th}\)&lt;/span> quantile of sample &lt;span class="math inline">\(X\)&lt;/span>:&lt;/p>
&lt;p>&lt;span class="math display">\[\begin{split}
\operatorname{NO}_p =
&amp; \Big( (3p-1)X_{(1)} + (2-3p)X_{(2)} - (1-p)X_{(3)} \Big) B_0 +\\
&amp; +\sum_{i=1}^n \Big((1-p)B_{i-1}+pB_i\Big)X_{(i)} +\\
&amp; +\Big( -pX_{(n-2)} + (3p-1)X_{(n-1)} + (2-3p)X_{(n)} \Big) B_n
\end{split}
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(B_i = B(i; n, p)\)&lt;/span> is probability mass function of the binomial distribution &lt;span class="math inline">\(B(n, p)\)&lt;/span>,
&lt;span class="math inline">\(X_{(i)}\)&lt;/span> are order statistics of sample &lt;span class="math inline">\(X\)&lt;/span>.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1111/bmsp.12198">&amp;ldquo;A new quantile estimator with weights based on a subsampling approach&amp;rdquo;&lt;/a> (2020)
by Gözde Navruz and A. Fırat Özdemir.
Also, I add some additional explanations,
simplify the final equation,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Sfakianakis-Verginis quantile estimator</title><link>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/</guid><description>&lt;p>There are dozens of different ways to estimate quantiles.
One of these ways is to use the Sfakianakis-Verginis quantile estimator.
To be more specific, it&amp;rsquo;s a family of three estimators.
If we want to estimate the &lt;span class="math inline">\(p^\textrm{th}\)&lt;/span> quantile of sample &lt;span class="math inline">\(X\)&lt;/span>,
we can use one of the following equations:&lt;/p>
&lt;p>&lt;span class="math display">\[\begin{split}
\operatorname{SV1}_p =&amp;
\frac{B_0}{2} \big( X_{(1)}+X_{(2)}-X_{(3)} \big) +
\sum_{i=1}^{n} \frac{B_i+B_{i-1}}{2} X_{(i)} +
\frac{B_n}{2} \big(- X_{(n-2)}+X_{(n-1)}-X_{(n)} \big),\\
\operatorname{SV2}_p =&amp; \sum_{i=1}^{n} B_{i-1} X_{(i)} + B_n \cdot \big(2X_{(n)} - X_{(n-1)}\big),\\
\operatorname{SV3}_p =&amp; \sum_{i=1}^n B_i X_{(i)} + B_0 \cdot \big(2X_{(1)}-X_{(2)}\big).
\end{split}
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(B_i = B(i; n, p)\)&lt;/span> is probability mass function of the binomial distribution &lt;span class="math inline">\(B(n, p)\)&lt;/span>,
&lt;span class="math inline">\(X_{(i)}\)&lt;/span> are order statistics of sample &lt;span class="math inline">\(X\)&lt;/span>.&lt;/p>
&lt;p>In this post, I derive these equations following the paper
&lt;a href="https://doi.org/10.1080/03610910701790491">&amp;ldquo;A new family of nonparametric quantile estimators&amp;rdquo;&lt;/a> (2008)
by Michael E. Sfakianakis and Dimitris G. Verginis.
Also, I add some additional explanations,
reconstruct missing steps,
simplify the final equations,
and provide reference implementations in C# and R.&lt;/p></description></item><item><title>Winsorized modification of the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/winsorized-hdqe/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/winsorized-hdqe/</guid><description>&lt;p>The Harrell-Davis quantile estimator is one of my favorite quantile estimators
because of its &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficiency&lt;/a>.
It has a small mean square error which allows getting accurate estimations.
However, it has a severe drawback: it&amp;rsquo;s not robust.
Indeed, since the estimator includes all sample elements with positive weights,
its &lt;a href="https://en.wikipedia.org/wiki/Robust_statistics#Breakdown_point">breakdown point&lt;/a> is zero.&lt;/p>
&lt;p>In this post, I want to suggest modifications of the Harrell-Davis quantile estimator
which increases its &lt;em>robustness&lt;/em> keeping almost the same level of &lt;em>efficiency&lt;/em>.&lt;/p></description></item><item><title>Misleading standard deviation</title><link>https://aakinshin.net/posts/misleading-stddev/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/misleading-stddev/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation&lt;/a> may be an extremely misleading metric.
Even minor deviations from normality could make it completely unreliable and deceiving.
Let me demonstrate this problem using an example.&lt;/p>
&lt;p>Below you can see three density plots of some distributions.
Could you guess their standard deviations?&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/misleading-stddev/img/density1-light.png" target="_blank" class="imgldlink" alt="density1">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/misleading-stddev/img/density1-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/misleading-stddev/img/density1-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/misleading-stddev/img/density1-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>The correct answers are &lt;span class="math inline">\(1.0, 3.0, 11.0\)&lt;/span>.
And here is a more challenging problem: could you match these values with the corresponding distributions?&lt;/p></description></item><item><title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/unbiased-mad-hd/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad-hd/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (&lt;span class="math inline">\(\textrm{MAD}\)&lt;/span>)
is a robust measure of scale.
In the previous post, I &lt;a href="https://aakinshin.net/posts/unbiased-mad/">showed&lt;/a>
how to use the &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased&lt;/a>
version of the &lt;span class="math inline">\(\textrm{MAD}\)&lt;/span> estimator
as a robust alternative to the standard deviation.
&amp;ldquo;Unbiasedness&amp;rdquo; means that such estimator&amp;rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the &lt;a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias–variance tradeoff&lt;/a>:
when we remove the bias of the &lt;span class="math inline">\(\textrm{MAD}\)&lt;/span> estimator,
we increase its variance and mean squared error (&lt;span class="math inline">\(\textrm{MSE}\)&lt;/span>).&lt;/p>
&lt;p>In this post, I want to suggest a more &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)">efficient&lt;/a>
unbiased &lt;span class="math inline">\(\textrm{MAD}\)&lt;/span> estimator.
It&amp;rsquo;s also a consistent estimator for the standard deviation, but it has smaller &lt;span class="math inline">\(\textrm{MSE}\)&lt;/span>.
To build this estimator,
we should replace the classic &amp;ldquo;straightforward&amp;rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&amp;rsquo;s discuss this approach in detail.&lt;/p></description></item><item><title>Unbiased median absolute deviation</title><link>https://aakinshin.net/posts/unbiased-mad/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unbiased-mad/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (&lt;span class="math inline">\(\textrm{MAD}\)&lt;/span>)
is a robust measure of scale.
For distribution &lt;span class="math inline">\(X\)&lt;/span>, it can be calculated as follows:&lt;/p>
&lt;p>&lt;span class="math display">\[\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(C\)&lt;/span> is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the &lt;span class="math inline">\(\textrm{MAD}\)&lt;/span> as a &lt;a href="https://en.wikipedia.org/wiki/Consistent_estimator">consistent estimator&lt;/a>
for the standard deviation under the normal distribution,
we should set&lt;/p>
&lt;p>&lt;span class="math display">\[C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(\Phi^{-1}\)&lt;/span> is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If &lt;span class="math inline">\(X\)&lt;/span> is the normal distribution, we get &lt;span class="math inline">\(\textrm{MAD} = \sigma\)&lt;/span> where &lt;span class="math inline">\(\sigma\)&lt;/span> is the standard deviation.&lt;/p>
&lt;p>Not let&amp;rsquo;s consider a sample &lt;span class="math inline">\(x = \{ x_1, x_2, \ldots x_n \}\)&lt;/span>.
Let&amp;rsquo;s denote the median absolute deviation for a sample of size &lt;span class="math inline">\(n\)&lt;/span> as &lt;span class="math inline">\(\textrm{MAD}_n\)&lt;/span>.
The corresponding equation looks similar to the definition of &lt;span class="math inline">\(\textrm{MAD}\)&lt;/span> for a distribution:&lt;/p>
&lt;p>&lt;span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
\]&lt;/span>&lt;/p>
&lt;p>Let&amp;rsquo;s assume that &lt;span class="math inline">\(\textrm{median}\)&lt;/span> is the straightforward definition of the median
(if &lt;span class="math inline">\(n\)&lt;/span> is odd, the median is the middle element of the sorted sample,
if &lt;span class="math inline">\(n\)&lt;/span> is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use &lt;span class="math inline">\(C_n = C_{\infty}\)&lt;/span> for extremely large sample sizes.
However, for small &lt;span class="math inline">\(n\)&lt;/span>, &lt;span class="math inline">\(\textrm{MAD}_n\)&lt;/span> becomes a &lt;a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased estimator&lt;/a>.
If we want to get an unbiased version, we should adjust the value of &lt;span class="math inline">\(C_n\)&lt;/span>.&lt;/p>
&lt;p>In this post, we look at the possible approaches and learn the way to get the exact value of &lt;span class="math inline">\(C_n\)&lt;/span>
that makes &lt;span class="math inline">\(\textrm{MAD}_n\)&lt;/span> unbiased estimator of the median absolute deviation for any &lt;span class="math inline">\(n\)&lt;/span>.&lt;/p></description></item><item><title>Comparing distribution quantiles using gamma effect size</title><link>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/comparing-distributions-using-gamma-es/</guid><description>&lt;p>There are several ways to describe the difference between two distributions.
Here are a few examples:&lt;/p>
&lt;ul>
&lt;li>Effect sizes based on differences between means (e.g., Cohen&amp;rsquo;s d, Glass&amp;rsquo; Δ, Hedges&amp;rsquo; g)&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/">The shift and ration functions&lt;/a> that
estimate differences between matched quantiles.&lt;/li>
&lt;/ul>
&lt;p>In one of the previous post, I &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">described&lt;/a>
the gamma effect size which is defined not for the mean but for quantiles.
In this post, I want to share a few case studies that demonstrate
how the suggested metric combines the advantages of the above approaches.&lt;/p></description></item><item><title>A single outlier could completely distort your Cohen's d value</title><link>https://aakinshin.net/posts/cohend-and-outliers/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cohend-and-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&amp;rsquo;s d&lt;/a> is a popular way to estimate
the &lt;a href="https://en.wikipedia.org/wiki/Effect_size">effect size&lt;/a> between two samples.
It works excellent for perfectly normal distributions.
Usually, people think that slight deviations from normality
shouldn&amp;rsquo;t produce a noticeable impact on the result.
Unfortunately, it&amp;rsquo;s not always true.
In fact, a single outlier value can completely distort the result even in large samples.&lt;/p>
&lt;p>In this post, I will present some illustrations for this problem and will show how to fix it.&lt;/p></description></item><item><title>Better moving quantile estimations using the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">have discussed&lt;/a> the Hardle-Steiger method.
This algorithm allows estimating &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>
using &lt;span class="math inline">\(O(L)\)&lt;/span> memory and &lt;span class="math inline">\(O(log(L))\)&lt;/span> element processing complexity (where &lt;span class="math inline">\(L\)&lt;/span> is the window size).
Also, I have shown how to adapt this approach to estimate &lt;em>any&lt;/em> moving quantile.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the &lt;a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics&lt;/a>
which is the &lt;span class="math inline">\(k\textrm{th}\)&lt;/span> smallest element from the sample.
It means that the estimated quantile value always equals one of the last &lt;span class="math inline">\(L\)&lt;/span> observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for &lt;span class="math inline">\(x = \{4, 5, 6, 7\}\)&lt;/span>,
some estimators return &lt;span class="math inline">\(5.5\)&lt;/span> (which is the arithmetical mean of &lt;span class="math inline">\(5\)&lt;/span> and &lt;span class="math inline">\(6\)&lt;/span>)
instead of &lt;span class="math inline">\(5\)&lt;/span> or &lt;span class="math inline">\(6\)&lt;/span> (which are order statistics).&lt;/p>
&lt;p>Let&amp;rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.&lt;/p></description></item><item><title>MP² quantile estimator: estimating the moving median without storing values</title><link>https://aakinshin.net/posts/mp2-quantile-estimator/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mp2-quantile-estimator/</guid><description>&lt;p>In one of the previous posts, I &lt;a href="https://aakinshin.net/posts/p2-quantile-estimator/">described&lt;/a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.&lt;/p>
&lt;p>After the publication, I got a lot of questions about &lt;em>moving&lt;/em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I &lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/">wrote&lt;/a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last &lt;span class="math inline">\(L\)&lt;/span> numbers
(&lt;span class="math inline">\(L\)&lt;/span> is known as the window size).
However, it requires &lt;span class="math inline">\(O(L)\)&lt;/span> memory, and it takes &lt;span class="math inline">\(O(log(L))\)&lt;/span> time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&amp;rsquo;t allow implementing low-overhead telemetry in the case of large &lt;span class="math inline">\(L\)&lt;/span>.&lt;/p>
&lt;p>In this post, I&amp;rsquo;m going to present a moving modification of the P² quantile estimator.
Let&amp;rsquo;s call it MP² (moving P²).
It requires &lt;span class="math inline">\(O(1)\)&lt;/span> memory, it takes &lt;span class="math inline">\(O(1)\)&lt;/span> to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.&lt;/p>
&lt;p>Let&amp;rsquo;s discuss MP² in detail!&lt;/p></description></item><item><title>Case study: Accuracy of the MAD estimation using the Harrell-Davis quantile estimator (Gumbel distribution)</title><link>https://aakinshin.net/posts/cs-mad-hd-gumbel/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/cs-mad-hd-gumbel/</guid><description>&lt;p>In some of my previous posts, I used
the &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (MAD)
to describe the distribution dispersion:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/">DoubleMAD outlier detector based on the Harrell-Davis quantile estimator&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/">Nonparametric Cohen&amp;rsquo;s d-consistent effect size&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aakinshin.net/posts/qad/">Quantile absolute deviation: estimating statistical dispersion around quantiles&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The MAD estimation depends on the chosen median estimator:
we may get different MAD values with different median estimators.
To get better accuracy,
I always encourage readers to use the Harrell-Davis quantile estimator
instead of the classic Type 7 quantile estimator.&lt;/p>
&lt;p>In this case study, I decided to compare these two quantile estimators using
the &lt;a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution&lt;/a>
(it&amp;rsquo;s a good model for slightly right-skewed distributions).
According to the performed Monte Carlo simulation,
the Harrell-Davis quantile estimator always has better accuracy:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-light.png" target="_blank" class="imgldlink" alt="summary">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/cs-mad-hd-gumbel/img/summary-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Fast implementation of the moving quantile based on the partitioning heaps</title><link>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/</guid><description>&lt;p>Imagine you have a time series.
Let&amp;rsquo;s say, after each new observation, you want to know an &amp;ldquo;average&amp;rdquo; value across the last &lt;span class="math inline">\(L\)&lt;/span> observations.
Such a metric is known as &lt;a href="https://en.wikipedia.org/wiki/Moving_average">a moving average&lt;/a>
(or rolling/running average).&lt;/p>
&lt;p>The most popular moving average example is &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average">the moving mean&lt;/a>.
It&amp;rsquo;s easy to efficiently implement this metric.
However, it has a major drawback: it&amp;rsquo;s not robust.
Outliers can easily spoil the moving mean and transform it into a meaningless and untrustable metric.&lt;/p>
&lt;p>Fortunately, we have a good alternative: &lt;a href="https://en.wikipedia.org/wiki/Moving_average#Moving_median">the moving median&lt;/a>.
Typically, it generates a stable and smooth series of values.
In the below figure, you can see the difference between the moving mean and the moving median on noisy data.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png" target="_blank" class="imgldlink" alt="example">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/img/example-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>The moving median also has a drawback: it&amp;rsquo;s not easy to efficiently implement it.
Today we going to discuss the Hardle-Steiger method to estimate the median
(memory: &lt;span class="math inline">\(O(L)\)&lt;/span>, element processing complexity: &lt;span class="math inline">\(O(log(L))\)&lt;/span>, median estimating complexity: &lt;span class="math inline">\(O(1)\)&lt;/span>).
Also, we will learn how to calculate &lt;em>the moving quantiles&lt;/em> based on this method.&lt;/p>
&lt;p>In this post, you will find the following:&lt;/p>
&lt;ul>
&lt;li>An overview of the Hardle-Steiger method&lt;/li>
&lt;li>A simple way to implement the Hardle-Steiger method&lt;/li>
&lt;li>Moving quantiles inspired by the Hardle-Steiger method&lt;/li>
&lt;li>How to process initial elements&lt;/li>
&lt;li>Reference C# implementation&lt;/li>
&lt;/ul></description></item><item><title>Coverage of quantile confidence intervals</title><link>https://aakinshin.net/posts/quantile-ci-coverage/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/quantile-ci-coverage/</guid><description>&lt;p>There is a common &lt;a href="https://en.wikipedia.org/wiki/Confidence_interval#Misunderstandings">misunderstanding&lt;/a>
that a 95% confidence interval is an interval that covers the true parameter value with 95% probability.
Meanwhile, the correct definition assumes that
the true parameter value will be covered by 95% of 95% confidence intervals &lt;em>in the long run&lt;/em>.
These two statements sound similar, but there is a huge difference between them.
95% in this context is not a property of a single confidence interval.
Once you get a calculated interval, it may cover the true value (100% probability) or
it may don&amp;rsquo;t cover it (0% probability).
In fact, 95% is a &lt;em>prediction&lt;/em> about the percentage of &lt;em>future&lt;/em> confidence intervals
that cover the true value &lt;em>in the long run&lt;/em>.&lt;/p>
&lt;p>However, even if you know the correct definition, you still may experience some troubles.
The first thing people usually forgot is the &amp;ldquo;long run&amp;rdquo; part.
For example, if we collected 100 samples and calculated a 95% confidence interval of a parameter for each of them,
we shouldn&amp;rsquo;t expect that 95 of these intervals cover the true parameter value.
In fact, we can observe a situation when none of these intervals covers the true value.
Of course, this is an unlikely event, but if you automatically perform thousands of different experiments,
you will definitely get some extreme situations.&lt;/p>
&lt;p>The second thing that may create trouble is the &amp;ldquo;prediction&amp;rdquo; part.
If weather forecasters predicted that it will rain tomorrow, this does not mean that it will rain tomorrow.
The same works for statistical predictions.
The actual prediction reliability may depend on many factors.
If you estimate confidence intervals around the mean for the normal distribution, you are most likely safe.
However, if you estimate confidence intervals around quantiles for non-parametric distributions,
you should care about the following things:&lt;/p>
&lt;ul>
&lt;li>The used approach to estimate confidence intervals&lt;/li>
&lt;li>The underlying distribution&lt;/li>
&lt;li>The sample size&lt;/li>
&lt;li>The position of the target quantile&lt;/li>
&lt;/ul>
&lt;p>I &lt;a href="https://aakinshin.net/posts/weighted-quantiles-ci/">have already showed&lt;/a> how to estimate
the confidence interval around the given quantile using the Maritz-Jarrett method.
It&amp;rsquo;s time to verify the reliability of this approach.
In this post, I&amp;rsquo;m going to show some Monte-Carlo simulations that evaluate the coverage percentage in different situations.&lt;/p></description></item><item><title>Statistical approaches for performance analysis</title><link>https://aakinshin.net/posts/statistics-for-performance/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/statistics-for-performance/</guid><description>&lt;p>Software performance is a complex discipline that requires knowledge in different areas
from benchmarking to the internals of modern runtimes, operating systems, and hardware.
Surprisingly, the most difficult challenges in performance analysis are not about programming,
they are about mathematical statistics!&lt;/p>
&lt;p>Many software developers can drill into performance problems and implement excellent optimizations,
but they are not always know how to correctly verify these optimizations.
This may not look like a problem in the case of a single performance investigation.
However, the situation became worse when developers try to set up an infrastructure that
should automatically find performance problems or prevent degradations from merging.
In order to make such an infrastructure reliable and useful,
it&amp;rsquo;s crucial to achieve an extremely low false-positive rate (otherwise, it&amp;rsquo;s not trustable)
and be able to detect most of the degradations (otherwise, it&amp;rsquo;s not so useful).
It&amp;rsquo;s not easy if you don&amp;rsquo;t know which statistical approaches should be used.
If you try to google it, you may find thousands of papers about statistics,
but only a small portion of them really works in practice.&lt;/p>
&lt;p>In this post, I want to share some approaches that I use for performance analysis in everyday life.
I have been analyzing performance distributions for the last seven years,
and I have found a lot of approaches, metrics, and tricks which nice to have
in your statistical toolbox.
I would not say that all of them are must have to know,
but they can definitely help you to improve the reliability of your statistical checks
in different problems of performance analysis.
Consider the below list as a letter to a younger version of myself with a brief list of topics that are good to learn.&lt;/p></description></item><item><title>Quantile confidence intervals for weighted samples</title><link>https://aakinshin.net/posts/weighted-quantiles-ci/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles-ci/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>When you work with non-parametric distributions,
quantile estimations are essential to get the main distribution properties.
Once you get the estimation values, you may be interested in measuring the accuracy of these estimations.
Without it, it&amp;rsquo;s hard to understand how trustable the obtained values are.
One of the most popular ways to evaluate accuracy is confidence interval estimation.&lt;/p>
&lt;p>Now imagine that you collect some measurements every day.
Each day you get a small sample of values that is not enough to get the accurate daily quantile estimations.
However, the full time-series over the last several weeks has a decent size.
You suspect that past measurements should be similar to today measurements,
but you are not 100% sure about it.
You feel a temptation to extend the up-to-date sample by the previously collected values,
but it may spoil the estimation (e.g., in the case of recent change points or positive/negative trends).&lt;/p>
&lt;p>One of the possible approaches in this situation is to use &lt;em>weighted samples&lt;/em>.
This assumes that we add past measurements to the &amp;ldquo;today sample,&amp;rdquo;
but these values should have smaller weight.
The older measurement we take, the smaller weight it gets.
If you have consistent values across the last several days,
this approach works like a charm.
If you have any recent changes, you can detect such situations by huge confidence intervals
due to the sample inconsistency.&lt;/p>
&lt;p>So, how do we estimate confidence intervals around quantiles for the weighted samples?
In one of the previous posts, I have already shown how to &lt;a href="https://aakinshin.net/posts/weighted-quantiles/">estimate quantiles on weighted samples&lt;/a>.
In this post, I will show how to estimate quantile confidence intervals for weighted samples.&lt;/p></description></item><item><title>Quantile absolute deviation: estimating statistical dispersion around quantiles</title><link>https://aakinshin.net/posts/qad/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qad/</guid><description>&lt;p>There are many different metrics for &lt;a href="https://en.wikipedia.org/wiki/Statistical_dispersion">statistical dispersion&lt;/a>.
The most famous one is the &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation&lt;/a>.
The standard deviation is the most popular way to describe the spread around the mean when
you work with normally distributed data.
However, if you work with non-normal distributions, this metric may be misleading.&lt;/p>
&lt;p>In the world of non-parametric distributions,
the most common measure of &lt;a href="https://en.wikipedia.org/wiki/Central_tendency">central tendency&lt;/a> is the median.
For the median, you can describe dispersion using the
&lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation around the median&lt;/a> (MAD).
It works great if the median is the only &lt;a href="https://en.wikipedia.org/wiki/Summary_statistics">summary statistic&lt;/a> that you care about.
However, if you work with multimodal distributions
(they can be detected using the &lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/">lowland multimodality detector&lt;/a>),
you may be interested in other quantiles as well.
So, it makes sense to learn how to describe dispersion around the given quantile.
Which metric should we choose?&lt;/p>
&lt;p>Recently, I came up with a great solution to this problem.
We can generalize the median absolute deviation into the quantile absolute deviation (QAD) around the given quantile based on the Harrell-Davis quantile estimator.
I will show how to calculate it, how to interpret it, and how to get insights about distribution properties
from images like this one:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/qad/img/modal5-light.png" target="_blank" class="imgldlink" alt="modal5">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/qad/img/modal5-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/qad/img/modal5-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/qad/img/modal5-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>P² quantile estimator: estimating the median without storing values</title><link>https://aakinshin.net/posts/p2-quantile-estimator/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/p2-quantile-estimator/</guid><description>&lt;p>Imagine that you are implementing performance telemetry in your application.
There is an operation that is executed millions of times, and you want to get its &amp;ldquo;average&amp;rdquo; duration.
It&amp;rsquo;s not a good idea to use the arithmetic mean because the obtained value can be easily spoiled by outliers.
It&amp;rsquo;s much better to use the median which is one of the most robust ways to describe the average.&lt;/p>
&lt;p>The straightforward median estimation approach requires storing all the values.
In our case, it&amp;rsquo;s a bad idea to keep all the values because it will significantly increase the memory footprint.
Such telemetry is harmful because it may become a new bottleneck instead of monitoring the actual performance.&lt;/p>
&lt;p>Another way to get the median value is to use a sequential quantile estimator
(also known as an online quantile estimator or a streaming quantile estimator).
This is an algorithm that allows calculating the median value (or any other quantile value)
using a fixed amount of memory.
Of course, it provides only an approximation of the real median value,
but it&amp;rsquo;s usually enough for typical telemetry use cases.&lt;/p>
&lt;p>In this post, I will show one of the simplest sequential quantile estimators that is called the P² quantile estimator
(or the Piecewise-Parabolic quantile estimator).&lt;/p></description></item><item><title>Plain-text summary notation for multimodal distributions</title><link>https://aakinshin.net/posts/modality-summary-notation/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/modality-summary-notation/</guid><description>&lt;p>Let&amp;rsquo;s say you collected a lot of data and want to explore the underlying distributions of collected samples.
If you have only a few distributions, the best way to do that is to look at the density plots
(expressed via histograms, kernel density estimations, or &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimations&lt;/a>).
However, it&amp;rsquo;s not always possible.&lt;/p>
&lt;p>Suppose you have to process dozens, hundreds, or even thousands of distributions.
In that case,
it may be extremely time-consuming to manually check visualizations of each distribution.
If you analyze distributions from the command line or send notifications about suspicious samples,
it may be impossible to embed images in the reports.
In these cases, there is a need to present a distribution using plain text.&lt;/p>
&lt;p>One way to do that is plain text histograms.
Unfortunately, this kind of visualization may occupy o lot of space.
In complicated cases, you may need 20 or 30 lines per a single distribution.&lt;/p>
&lt;p>Another way is to present classic &lt;a href="https://en.wikipedia.org/wiki/Summary_statistics">summary statistics&lt;/a>
like mean or median, standard deviation or median absolute deviation, quantiles, skewness, and kurtosis.
There is another problem here:
without experience, it&amp;rsquo;s hard to reconstruct the true distribution shape based on these values.
Even if you are an experienced researcher, the statistical metrics may become misleading in the case of multimodal distributions.
Multimodality is one of the most severe challenges in distribution analysis because it distorts basic summary statistics.
It&amp;rsquo;s important to not only find such distribution but also have a way to present brief information about multimodality effects.&lt;/p>
&lt;p>So, how can we condense the underlying distribution shape of a given sample to a short text line?
I didn&amp;rsquo;t manage to find an approach that works fine in my cases, so I came up with my own notation.
Most of the interpretation problems in my experiments arise from multimodality and outliers,
so I decided to focus on these two things and specifically highlight them.
Let&amp;rsquo;s consider this plot:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg" target="_blank" class="imgldlink" alt="thumbnail">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/modality-summary-notation/img/thumbnail-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>I suggest describing it like this:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">{&lt;/span>1.00, 2.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>7.16&lt;span class="p">;&lt;/span> 13.12&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>19.00&lt;span class="o">}&lt;/span> + &lt;span class="o">[&lt;/span>27.69&lt;span class="p">;&lt;/span> 32.34&lt;span class="o">]&lt;/span>_100 + &lt;span class="o">{&lt;/span>37.00..39.00&lt;span class="o">}&lt;/span>_3
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let me explain the suggested notation in detail.&lt;/p></description></item><item><title>Intermodal outliers</title><link>https://aakinshin.net/posts/intermodal-outliers/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/intermodal-outliers/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Outlier">Outlier&lt;/a> analysis is a typical step in distribution exploration.
Usually, we work with the &amp;ldquo;lower outliers&amp;rdquo; (extremely low values) and the &amp;ldquo;upper outliers&amp;rdquo; (extremely high values).
However, outliers are not always extreme values.
In the general case, an outlier is a value that significantly differs from other values in the same sample.
In the case of multimodal distribution, we can also consider outliers in the middle of the distribution.
Let&amp;rsquo;s call such outliers that we found between modes the &amp;ldquo;&lt;em>intermodal outliers&lt;/em>.&amp;rdquo;&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg" target="_blank" class="imgldlink" alt="step4">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/intermodal-outliers/img/step4-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/intermodal-outliers/img/step4-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Look at the above density plot.
It&amp;rsquo;s a bimodal distribution that is formed as a combination of two unimodal distributions.
Each of the unimodal distributions may have its own lower and upper outliers.
When we merge them, the upper outliers of the first distribution and the lower outliers of the second distribution
stop being lower or upper outliers.
However, if these values don&amp;rsquo;t belong to the modes, they still are a subject of interest.
In this post, I will show you how to detect such intermodal outliers
and how they can be used to form a better distribution description.&lt;/p></description></item><item><title>Lowland multimodality detection</title><link>https://aakinshin.net/posts/lowland-multimodality-detection/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/lowland-multimodality-detection/</guid><description>&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-light.svg" target="_blank" class="imgldlink" alt="data5">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/lowland-multimodality-detection/img/data5-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Multimodality is an essential feature of a distribution, which may create many troubles during automatic analysis.
One of the best ways to work with such distributions is to detect all the modes in advance based on the given samples.
Unfortunately, this problem is much harder than it looks like.&lt;/p>
&lt;p>I tried many different approaches for multimodality detection, but none of them was good enough.
During the past several years, my approach of choice was the &lt;a href="http://www.brendangregg.com/FrequencyTrails/modes.html">mvalue-based modal test&lt;/a> by Brendan Gregg.
It works nicely in simple cases, but I was constantly stumbling over noisy samples where this algorithm doesn&amp;rsquo;t produce reliable results.
Also, it has some limitations that make it unapplicable to some corner cases.&lt;/p>
&lt;p>So, I needed a better approach.
Here are my main requirements:&lt;/p>
&lt;ul>
&lt;li>It should detect the exact mode locations and ranges&lt;/li>
&lt;li>It should provide reliable results even on noisy samples&lt;/li>
&lt;li>It should be able to detect multimodality even when some modes are extremely close to each other&lt;/li>
&lt;li>It should work out of the box without tricky parameter tuning for each specific distribution&lt;/li>
&lt;/ul>
&lt;p>I failed to find such an algorithm anywhere, so I came up with my own!
The current working title is &amp;ldquo;the lowland multimodality detector.&amp;rdquo;
It takes an estimation of the probability density function (PDF) and tries to find &amp;ldquo;lowlands&amp;rdquo; (areas that are much lower than neighboring peaks).
Next, it splits the plot by these lowlands and detects modes between them.
For the PDF estimation, it uses the &lt;a href="https://aakinshin.net/posts/qrde-hd/">quantile-respectful density estimation based on the Harrell-Davis quantile estimator&lt;/a> (QRDE-HD).
Let me explain how it works in detail.&lt;/p></description></item><item><title>Quantile-respectful density estimation based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/qrde-hd/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/qrde-hd/</guid><description>&lt;p>The idea of this post was born when I was working on a presentation for my recent &lt;a href="https://dotnext.ru/en/">DotNext&lt;/a> &lt;a href="https://www.youtube.com/watch?v=gc3yVybPuaY&amp;amp;list=PL21xssNXOJNGUROqzSTOC8uZL4W2QZpvK&amp;amp;index=1">talk&lt;/a>.
It had a &lt;a href="https://slides.aakinshin.net/dotnext-piter2020/#193">slide&lt;/a> with a density plot like this:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/qrde-hd/img/riddle-light.png" target="_blank" class="imgldlink" alt="riddle">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/qrde-hd/img/riddle-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/qrde-hd/img/riddle-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/qrde-hd/img/riddle-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Here we can see a density plot based on a sample with highlighted &lt;a href="https://en.wikipedia.org/wiki/Decile">decile&lt;/a> locations that split the plot into 10 equal parts.
Before the conference, I have been reviewed by &lt;a href="https://twitter.com/VladimirSitnikv">@VladimirSitnikv&lt;/a>.
He raised a reasonable concern: it doesn&amp;rsquo;t look like all the density plot segments are equal and contain exactly 10% of the whole plot.
And he was right!&lt;/p>
&lt;p>However, I didn&amp;rsquo;t make any miscalculations.
I generated a real sample with 61 elements.
Next, I build a density plot with the kernel density estimation (KDE) using the Sheather &amp;amp; Jones method and the normal kernel.
Next, I calculated decile values using the Harrell-Davis quantile estimator.
Although both the density plot and the decile values are calculated correctly and consistent with the sample,
they are not consistent with each other!
Indeed, such a density plot is just an estimation of the underlying distribution.
It has its own decile values, which are not equal to the sample decile values regardless of the used quantile estimator.
This problem is common for different kinds of visualization that presents density and quantiles at the same time (e.g., &lt;a href="https://towardsdatascience.com/violin-plots-explained-fb1d115e023d">violin plots&lt;/a>)&lt;/p>
&lt;p>It leads us to a question: how should we present the shape of our data together with quantile values without confusing inconsistency in the final image?
Today I will present a good solution: we should use the quantile-respectful density estimation based on the Harrell-Davis quantile estimator!
I know the title is a bit long, but it&amp;rsquo;s not so complicated as it sounds.
In this post, I will show how to build such plots.
Also I will compare them to the classic histograms and kernel density estimations.
As a bonus, I will demonstrate how awesome these plots are for multimodality detection.&lt;/p></description></item><item><title>Misleading histograms</title><link>https://aakinshin.net/posts/misleading-histograms/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/misleading-histograms/</guid><description>&lt;p>Below you see two histograms.
What could you say about them?&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-light.svg" target="_blank" class="imgldlink" alt="hist-riddle">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/misleading-histograms/img/hist-riddle-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Most likely, you say that the first histogram is based on a uniform distribution,
and the second one is based on a multimodal distribution with four modes.
Although this is not obvious from the plots,
both histograms are based on the same sample:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="m">20.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.94&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.03&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.15&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.99&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.20&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.99&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.22&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.86&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">19.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">20.06&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="m">29.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.73&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.13&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.98&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.12&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.18&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.95&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.97&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">29.93&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">30.07&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="m">40.10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.93&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.05&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.82&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.91&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.75&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.07&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.86&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.04&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">39.91&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">40.14&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="m">49.95&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.03&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.92&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.15&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.06&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.00&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.70&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.02&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">49.96&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.05&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">50.13&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Thus, the only difference between histograms is the offset!&lt;/p>
&lt;p>Visualization is a simple way to understand the shape of your data.
Unfortunately, this way may easily become a slippery slope.
In the &lt;a href="https://aakinshin.net/posts/kde-bw/">previous post&lt;/a>, I have shown how density plots may deceive you when the bandwidth is poorly chosen.
Today, we talk about histograms and why you can&amp;rsquo;t trust them in the general case.&lt;/p></description></item><item><title>The importance of kernel density estimation bandwidth</title><link>https://aakinshin.net/posts/kde-bw/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/kde-bw/</guid><description>&lt;p>Below see two kernel density estimations.
What could you say about them?&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/kde-bw/img/kde-riddle-light.svg" target="_blank" class="imgldlink" alt="kde-riddle">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/kde-bw/img/kde-riddle-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/kde-bw/img/kde-riddle-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/kde-bw/img/kde-riddle-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Most likely, you say that the first plot is based on a uniform distribution,
and the second one is based on a multimodal distribution with four modes.
Although this is not obvious from the plots,
both density plots are based on the same sample:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-txt" data-lang="txt">21.370, 19.435, 20.363, 20.632, 20.404, 19.893, 21.511, 19.905, 22.018, 19.93,
31.304, 32.286, 28.611, 29.721, 29.866, 30.635, 29.715, 27.343, 27.559, 31.32,
39.693, 38.218, 39.828, 41.214, 41.895, 39.569, 39.742, 38.236, 40.460, 39.36,
50.455, 50.704, 51.035, 49.391, 50.504, 48.282, 49.215, 49.149, 47.585, 50.03
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The only difference between plots is in &lt;a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#Bandwidth_selection">bandwidth selection&lt;/a>!&lt;/p>
&lt;p>Bandwidth selection is crucial when you are trying to visualize your distributions.
Unfortunately, most people just call a regular function to build a density plot and don&amp;rsquo;t think about how the bandwidth will be chosen.
As a result, the plot may present data in the wrong way, which may lead to incorrect conclusions.
Let&amp;rsquo;s discuss bandwidth selection in detail and figure out how to improve the correctness of your density plots.
In this post, we will cover the following topics:&lt;/p>
&lt;ul>
&lt;li>Kernel density estimation&lt;/li>
&lt;li>How bandwidth selection affects plot smoothness&lt;/li>
&lt;li>Which bandwidth selectors can we use&lt;/li>
&lt;li>Which bandwidth selectors should we use&lt;/li>
&lt;li>Insidious default bandwidth selectors in statistical packages&lt;/li>
&lt;/ul></description></item><item><title>The median absolute deviation value of the Gumbel distribution</title><link>https://aakinshin.net/posts/gumbel-mad/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gumbel-mad/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution&lt;/a> is not only a useful model in the &lt;a href="https://en.wikipedia.org/wiki/Extreme_value_theory">extreme value theory&lt;/a>,
but it&amp;rsquo;s also a nice example of a slightly right-skewed distribution (skewness &lt;span class="math inline">\(\approx 1.14\)&lt;/span>).
Here is its density plot:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg" target="_blank" class="imgldlink" alt="gumbel">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/gumbel-mad/img/gumbel-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='600'
src="https://aakinshin.net/posts/gumbel-mad/img/gumbel-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>In some of my statistical experiments, I like to use the Gumbel distribution as a sample generator for hypothesis checking or unit tests.
I also prefer the &lt;a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation&lt;/a> (MAD) over the standard deviation as a measure of dispersion because it&amp;rsquo;s more robust in the case of non-parametric distributions.
Numerical hypothesis verification often requires the exact value of the median absolute deviation of the original distribution.
I didn&amp;rsquo;t find this value in the reference tables, so I decided to do another exercise and derive it myself.
In this post, you will find a short derivation and the result (spoiler: the exact value is &lt;code>0.767049251325708 * β&lt;/code>).
The general approach of the MAD derivation is common for most distributions, so it can be easily reused.&lt;/p></description></item><item><title>Weighted quantile estimators</title><link>https://aakinshin.net/posts/weighted-quantiles/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/weighted-quantiles/</guid><description>&lt;p>&lt;strong>Update 2021-07-06:
the approach was updated using the &lt;a href="https://aakinshin.net/posts/kish-ess-weighted-quantiles/">Kish&amp;rsquo;s effective sample size&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>In this post, I will show how to calculate weighted quantile estimates and how to use them in practice.&lt;/p>
&lt;p>Let&amp;rsquo;s start with a problem from real life.
Imagine that you measure the total duration of a unit test executed daily on a CI server.
Every day you get a single number that corresponds to the test duration from the latest revision for this day:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg" target="_blank" class="imgldlink" alt="moving1">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/weighted-quantiles/img/moving1-dark.svg"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='600'
src="https://aakinshin.net/posts/weighted-quantiles/img/moving1-light.svg">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>You collect a history of such measurements for 100 days.
Now you want to describe the &amp;ldquo;actual&amp;rdquo; distribution of the performance measurements.&lt;/p>
&lt;p>However, for the latest &amp;ldquo;actual&amp;rdquo; revision, you have only a single measurement, which is not enough to build a distribution.
Also, you can&amp;rsquo;t build a distribution based on the last N measurements because they can contain change points that will spoil your results.
So, what you really want to do is to use all the measurements, but older values should have a lower impact on the final distribution form.&lt;/p>
&lt;p>Such a problem can be solved using the weighted quantiles!
This powerful approach can be applied to any time series regardless of the domain area.
In this post, we learn how to calculate and apply weighted quantiles.&lt;/p></description></item><item><title>Nonparametric Cohen's d-consistent effect size</title><link>https://aakinshin.net/posts/nonparametric-effect-size/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nonparametric-effect-size/</guid><description>&lt;p>&lt;strong>Update: the second part of this post is available &lt;a href="https://aakinshin.net/posts/nonparametric-effect-size2/">here&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>The effect size is a common way to describe a difference between two distributions.
When these distributions are normal, one of the most popular approaches to express the effect size is &lt;a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&amp;rsquo;s d&lt;/a>.
Unfortunately, it doesn&amp;rsquo;t work great for non-normal distributions.&lt;/p>
&lt;p>In this post, I will show a robust Cohen&amp;rsquo;s d-consistent effect size formula for nonparametric distributions.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/nonparametric-effect-size/img/blackboard.png" target="_blank" alt="blackboard">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/nonparametric-effect-size/img/blackboard.png" />
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>DoubleMAD outlier detector based on the Harrell-Davis quantile estimator</title><link>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/</guid><description>&lt;p>Outlier detection is an important step in data processing.
Unfortunately, if the distribution is not normal (e.g., right-skewed and heavy-tailed), it&amp;rsquo;s hard to choose
a robust outlier detection algorithm that will not be affected by tricky distribution properties.
During the last several years, I tried many different approaches, but I was not satisfied with their results.
Finally, I found an algorithm to which I have (almost) no complaints.
It&amp;rsquo;s based on the &lt;em>double median absolute deviation&lt;/em> and the &lt;em>Harrell-Davis quantile estimator&lt;/em>.
In this post, I will show how it works and why it&amp;rsquo;s better than some other approaches.&lt;/p></description></item><item><title>How ListSeparator Depends on Runtime and Operating System</title><link>https://aakinshin.net/posts/how-listseparator-depends-on-runtime-and-operating-system/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/how-listseparator-depends-on-runtime-and-operating-system/</guid><description>&lt;p>&lt;em>This blog post was &lt;a href="https://blog.jetbrains.com/dotnet/2020/05/20/listseparator-depends-runtime-operating-system/">originally posted&lt;/a> on &lt;a href="https://blog.jetbrains.com/dotnet/">JetBrains .NET blog&lt;/a>.&lt;/em>&lt;/p>
&lt;p>In the two previous blog posts from this series, we discussed how socket errors and socket orders depend on the runtime and operating systems. For some, it may be obvious that some things are indeed specific to the operating system or the runtime, but often these issues come as a surprise and are only discovered when running our code on different systems.
An interesting example that may bite us at runtime is using &lt;code>ListSeparator&lt;/code> in our code. It should give us a common separator for list elements in a string. But is it really common?
Let&amp;rsquo;s start our investigation by printing &lt;code>ListSeparator&lt;/code> for the Russian language:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CultureInfo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;ru-ru&amp;#34;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="n">TextInfo&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ListSeparator&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>On Windows, you will get the same result for .NET Framework, .NET Core, and Mono: the &lt;code>ListSeparator&lt;/code> is &lt;code>;&lt;/code> (a semicolon). You will also get a semicolon on Mono+Unix. However, on .NET Core+Unix, you will get a &lt;a href="https://en.wikipedia.org/wiki/Non-breaking_space">non-breaking space&lt;/a>.&lt;/p></description></item><item><title>How Sorting Order Depends on Runtime and Operating System</title><link>https://aakinshin.net/posts/how-sorting-order-depends-on-runtime-and-operating-system/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/how-sorting-order-depends-on-runtime-and-operating-system/</guid><description>&lt;p>&lt;em>This blog post was &lt;a href="https://blog.jetbrains.com/dotnet/2020/05/13/sorting-order-depends-runtime-operating-system/">originally posted&lt;/a> on &lt;a href="https://blog.jetbrains.com/dotnet/">JetBrains .NET blog&lt;/a>.&lt;/em>&lt;/p>
&lt;p>In &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a>, we have unit tests that enumerate files in your project and dump a sorted list of these files. In one of our test projects, we had the following files: &lt;code>jquery-1.4.1.js&lt;/code>, &lt;code>jquery-1.4.1.min.js&lt;/code>, &lt;code>jquery-1.4.1-vsdoc.js&lt;/code>. On Windows, .NET Framework, .NET Core, and Mono produce the same sorted list:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-bash" data-lang="bash">jquery-1.4.1.js
jquery-1.4.1.min.js
jquery-1.4.1-vsdoc.js
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>How Socket Error Codes Depend on Runtime and Operating System</title><link>https://aakinshin.net/posts/how-socket-error-codes-depend-on-runtime-and-operating-system/</link><pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/how-socket-error-codes-depend-on-runtime-and-operating-system/</guid><description>&lt;p>&lt;em>This blog post was &lt;a href="https://blog.jetbrains.com/dotnet/2020/04/27/socket-error-codes-depend-runtime-operating-system/">originally posted&lt;/a> on &lt;a href="https://blog.jetbrains.com/dotnet/">JetBrains .NET blog&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> consists of several processes that send messages to each other via sockets. To ensure the reliability of the whole application, it&amp;rsquo;s important to properly handle all the socket errors. In our codebase, we had the following code which was adopted from &lt;a href="https://github.com/mono/debugger-libs/blob/master/Mono.Debugging.Soft/SoftDebuggerSession.cs#L273">Mono Debugger Libs&lt;/a> and helps us communicate with debugger processes:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">protected&lt;/span> &lt;span class="k">virtual&lt;/span> &lt;span class="kt">bool&lt;/span> &lt;span class="n">ShouldRetryConnection&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">Exception&lt;/span> &lt;span class="n">ex&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">attemptNumber&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">sx&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">ex&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">SocketException&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">sx&lt;/span> &lt;span class="p">!=&lt;/span> &lt;span class="k">null&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">sx&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ErrorCode&lt;/span> &lt;span class="p">==&lt;/span> &lt;span class="m">10061&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1">//connection refused
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="k">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the case of a failed connection because of a “ConnectionRefused” error, we are retrying the connection attempt. It works fine with .NET Framework and Mono. However, once we migrated to .NET Core, this method no longer correctly detects the &amp;ldquo;connection refused&amp;rdquo; situation on Linux and macOS. If we open the &lt;code>SocketException&lt;/code> &lt;a href="https://docs.microsoft.com/en-us/dotnet/api/system.net.sockets.socketexception?view=netframework-4.8">documentation&lt;/a>, we will learn that this class has three different properties with error codes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>SocketError SocketErrorCode&lt;/code>: Gets the error code that is associated with this exception.&lt;/li>
&lt;li>&lt;code>int ErrorCode&lt;/code>: Gets the error code that is associated with this exception.&lt;/li>
&lt;li>&lt;code>int NativeErrorCode&lt;/code>: Gets the Win32 error code associated with this exception.&lt;/li>
&lt;/ul>
What's the difference between these properties? Should we expect different values on different runtimes or different operating systems? Which one should we use in production? Why do we have problems with &lt;code>ShouldRetryConnection&lt;/code> on .NET Core? Let's figure it all out!</description></item><item><title>.NET Core performance revolution in Rider 2020.1</title><link>https://aakinshin.net/posts/netcore-performance-revolution-in-rider-2020-1/</link><pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/netcore-performance-revolution-in-rider-2020-1/</guid><description>&lt;p>&lt;em>This blog post was &lt;a href="https://blog.jetbrains.com/dotnet/2020/04/14/net-core-performance-revolution-rider-2020-1/">originally posted&lt;/a> on &lt;a href="https://blog.jetbrains.com/dotnet/">JetBrains .NET blog&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Many &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> users may know that &lt;a href="https://www.codemag.com/Article/1811091/Building-a-.NET-IDE-with-JetBrains-Rider">the IDE has two main processes&lt;/a>: frontend (Java-application based on the IntelliJ platform) and backend (.NET-application based on ReSharper). Since the first release of Rider, we’ve used Mono as the backend runtime on Linux and macOS. A few years ago, we decided to migrate to .NET Core. After resolving hundreds of technical challenges, &lt;strong>we are finally ready to present the .NET Core edition of Rider!&lt;/strong>&lt;/p>
&lt;p>In this blog post, we want to share the results of some benchmarks that compare the Mono-powered and the .NET Core-powered editions of Rider. You may find this interesting if you are also thinking about migrating to .NET Core, or if you just want a high-level overview of the improvements to Rider in terms of performance and footprint, following the migration. (Spoiler: they’re huge!)&lt;/p></description></item><item><title>Introducing perfolizer</title><link>https://aakinshin.net/posts/introducing-perfolizer/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/introducing-perfolizer/</guid><description>&lt;p>Over the last 7 years, I&amp;rsquo;ve been maintaining &lt;a href="https://github.com/dotnet/BenchmarkDotNet">BenchmarkDotNet&lt;/a>;
it&amp;rsquo;s a library that helps you to transform methods into benchmarks, track their performance, and share reproducible measurement experiments.
Today, BenchmarkDotNet became the most popular .NET library for benchmarking which was adopted by &lt;a href="https://github.com/dotnet/BenchmarkDotNet#who-use-benchmarkdotnet">3500+&lt;/a> projects including .NET Core.&lt;/p>
&lt;p>While it has tons of features for benchmarking that allows getting reliable and accurate measurements,
it has a limited set of features for performance analysis.
And it&amp;rsquo;s a problem for many developers.
Lately, I started to get a lot of emails when people ask me
&amp;ldquo;OK, I benchmarked my application and got tons of numbers. What should I do next?&amp;rdquo;
It&amp;rsquo;s an excellent question that requires special tools.
So, I decided to start another project that focuses specifically on performance analysis.&lt;/p>
&lt;p>Meet &lt;a href="https://github.com/AndreyAkinshin/perfolizer">perfolizer&lt;/a> — a toolkit for performance analysis!
The source code is available on &lt;a href="https://github.com/AndreyAkinshin/perfolizer">GitHub&lt;/a> under the MIT license.&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/introducing-perfolizer/img/perfolizer.svg" target="_blank" alt="perfolizer">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/introducing-perfolizer/img/perfolizer.svg" />
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Distribution comparison via the shift and ratio functions</title><link>https://aakinshin.net/posts/shift-and-ratio-functions/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/shift-and-ratio-functions/</guid><description>&lt;p>When we compare two distributions, it&amp;rsquo;s not always enough to detect a statistically significant difference between them.
In many cases, we also want to evaluate the magnitude of this difference.
Let&amp;rsquo;s look at the following image:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-light.png" target="_blank" class="imgldlink" alt="compare1">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare1-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>On the left side, we can see a timeline plot with 2000 points
(at the middle of this plot, the distribution was significantly changed).
On the right side, you can see density plots for the left and the right side of
the timeline plot (before and after the change).
It&amp;rsquo;s a pretty simple case, the difference between distributions be expressed via the
difference between mean values.&lt;/p>
&lt;p>Now let&amp;rsquo;s look at a more tricky case:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-light.png" target="_blank" class="imgldlink" alt="compare2">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare2-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Here we have a bimodal distribution; after the change, the left mode &amp;ldquo;moved right.&amp;rdquo;
Now it&amp;rsquo;s much harder to evaluate the difference between distributions
because the mean and the median values almost not changed:
the right mode has the biggest impact on these metrics than the left more.&lt;/p>
&lt;p>And here is a much more tricky case:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-light.png" target="_blank" class="imgldlink" alt="compare3">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/shift-and-ratio-functions/img/compare3-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Here we also have a bimodal distribution; after the change, both modes moved:
the left mode &amp;ldquo;moved right&amp;rdquo; and the right mode &amp;ldquo;moved left.&amp;rdquo;
How should we describe the difference between these distributions now?&lt;/p></description></item><item><title>Normality is a myth</title><link>https://aakinshin.net/posts/normality/</link><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/normality/</guid><description>&lt;p>In many statistical papers, you can find the following phrase: &amp;ldquo;assuming that we have a normal distribution.&amp;rdquo;
Probably, you saw plots of the normal distribution density function in some statistics textbooks,
it looks like this:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/normality/img/normal-light.png" target="_blank" class="imgldlink" alt="normal">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/normality/img/normal-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/normality/img/normal-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/normality/img/normal-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>The normal distribution is a pretty user-friendly mental model when we are trying to interpret the statistical metrics
like mean and standard deviation.
However, it may also be an insidious and misleading model when your distribution is not normal.
There is a great sentence in the &lt;a href="https://doi.org/10.1093/biomet/34.3-4.209">&amp;ldquo;Testing for normality&amp;rdquo;&lt;/a> paper by R.C. Geary, 1947 (the quote was found &lt;a href="https://garstats.wordpress.com/2019/06/17/myth/">here&lt;/a>):&lt;/p>
&lt;blockquote>
&lt;p>Normality is a myth; there never was, and never will be, a normal distribution.&lt;/p>
&lt;/blockquote>
&lt;p>I 100% agree with this statement.
At least, if you are working with performance distributions
(that are based on the multiple iterations of your benchmarks that measure the performance metrics of your applications),
you should forget about normality.
That&amp;rsquo;s how a typical performance distribution looks like
(I built the below picture based on a real benchmark that measures the load time of assemblies
when we open the &lt;a href="https://github.com/OrchardCMS/Orchard">Orchard&lt;/a> solution in &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> on Linux):&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/normality/img/performance-light.png" target="_blank" class="imgldlink" alt="performance">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/normality/img/performance-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/normality/img/performance-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/normality/img/performance-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>Implementation of efficient algorithm for changepoint detection: ED-PELT</title><link>https://aakinshin.net/posts/edpelt/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/edpelt/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Change_detection">Changepoint detection&lt;/a> is an important task that has a lot of applications.
For example, I use it to detect changes in the &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> performance test suite.
It&amp;rsquo;s very important to detect not only performance degradations, but any kinds of performance changes
(e.g., the variance may increase, or a unimodal distribution may be split to several modes).
You can see examples of such changes on the following picture (we change the color when a changepoint is detected):&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/edpelt/img/edpelt-light.png" target="_blank" class="imgldlink" alt="edpelt">
&lt;picture>
&lt;source
theme='dark'
srcset="https://aakinshin.net/posts/edpelt/img/edpelt-dark.png"
media="(prefers-color-scheme: dark)">
&lt;source
theme='light'
srcset="https://aakinshin.net/posts/edpelt/img/edpelt-light.png"
media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/edpelt/img/edpelt-light.png">
&lt;/picture>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>Unfortunately, it&amp;rsquo;s pretty hard to write a reliable and fast algorithm for changepoint detection.
Recently, I found a cool paper (&lt;a href="https://link.springer.com/article/10.1007/s11222-016-9687-5">Haynes, K., Fearnhead, P. &amp;amp; Eckley, I.A. &amp;ldquo;A computationally efficient nonparametric approach for changepoint detection,&amp;rdquo; Stat Comput (2017) 27: 1293&lt;/a>) that describes the ED-PELT algorithm.
It has &lt;code>O(N*log(N))&lt;/code> complexity and pretty good detection accuracy.
The reference implementation can be used via the &lt;a href="https://cran.r-project.org/web/packages/changepoint.np/index.html">changepoint.np&lt;/a> R package.
However, I can&amp;rsquo;t use &lt;a href="https://www.r-project.org/">R&lt;/a> on our build server, so I decided to write my own C# implementation.&lt;/p></description></item><item><title>A story about slow NuGet package browsing</title><link>https://aakinshin.net/posts/nuget-package-browsing/</link><pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nuget-package-browsing/</guid><description>&lt;p>In &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a>, we have integration tests which interact with &lt;a href="https://api.nuget.org/">api.nuget.org&lt;/a>.
Also, we have an internal service which monitors the performance of these tests.
Two days ago, I noticed that some of these tests sometimes are running for too long.
For example, &lt;code>nuget_NuGetTest_shouldUpgradeVersionForDotNetCore&lt;/code> usually takes around &lt;code>10 sec&lt;/code>.
However, in some cases, it takes around &lt;code>110 sec&lt;/code>, &lt;code>210 sec&lt;/code>, or &lt;code>310 sec&lt;/code>:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/nuget-package-browsing/img/perf-chart.png" target="_blank" alt="perf-chart">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/nuget-package-browsing/img/perf-chart.png" />
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br />
&lt;p>It looks very suspicious and increases the whole test suite duration.
Also, our dashboard with performance degradations contains only such tests
and some real degradations (which are introduced by the changes in our codebase) can go unnoticed.
So, my colleagues and I decided to investigate it.&lt;/p></description></item><item><title>Cross-runtime .NET disassembly with BenchmarkDotNet</title><link>https://aakinshin.net/posts/dotnet-crossruntime-disasm/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/dotnet-crossruntime-disasm/</guid><description>&lt;p>&lt;a href="https://github.com/dotnet/BenchmarkDotNet">BenchmarkDotNet&lt;/a> is a cool tool for benchmarking.
It has a lot of useful features that help you with performance investigations.
However, you can use these features even if you are not actually going to benchmark something.
One of these features is &lt;code>DisassemblyDiagnoser&lt;/code>.
It shows you a disassembly listing of your code for all required runtimes.
In this post, I will show you how to get disassembly listing for .NET Framework, .NET Core, and Mono with one click!
You can do it with a very small code snippet like this:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="na">[DryCoreJob, DryMonoJob, DryClrJob(Platform.X86)]&lt;/span>
&lt;span class="na">[DisassemblyDiagnoser]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="k">class&lt;/span> &lt;span class="nc">IntroDisasm&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="na"> [Benchmark]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">double&lt;/span> &lt;span class="n">Sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">double&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">64&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">++)&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="p">+=&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>BenchmarkDotNet v0.10.14</title><link>https://aakinshin.net/posts/bdn-v0_10_14/</link><pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/bdn-v0_10_14/</guid><description>&lt;p>BenchmarkDotNet v0.10.14 has been released! This release includes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Per-method parameterization&lt;/strong> (&lt;a href="http://benchmarkdotnet.org/Advanced/Arguments.htm">Read more&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Console histograms and multimodal disribution detection&lt;/strong> (&lt;a href="https://aakinshin.net/posts/dotnet-crossruntime-disasm/">Read more&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Many improvements for Mono disassembly support on Windows&lt;/strong> (A blog post is coming soon)&lt;/li>
&lt;li>&lt;strong>Many bugfixes&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>In the &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues?q=milestone:v0.10.14">v0.10.14&lt;/a> scope,
8 issues were resolved and 11 pull requests where merged.
This release includes 47 commits by 8 contributors.&lt;/p></description></item><item><title>BenchmarkDotNet v0.10.13</title><link>https://aakinshin.net/posts/bdn-v0_10_13/</link><pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/bdn-v0_10_13/</guid><description>&lt;p>BenchmarkDotNet v0.10.13 has been released! This release includes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mono Support for DisassemblyDiagnoser:&lt;/strong>
Now you can easily get an assembly listing not only on .NET Framework/.NET Core, but also on Mono.
It works on Linux, macOS, and Windows (Windows requires installed cygwin with &lt;code>obj&lt;/code> and &lt;code>as&lt;/code>).
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/541">#541&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Support ANY CoreFX and CoreCLR builds:&lt;/strong>
BenchmarkDotNet allows the users to run their benchmarks against ANY CoreCLR and CoreFX builds.
You can compare your local build vs MyGet feed or Debug vs Release or one version vs another.
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/651">#651&lt;/a>)&lt;/li>
&lt;li>&lt;strong>C# 7.2 support&lt;/strong>
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/643">#643&lt;/a>)&lt;/li>
&lt;li>&lt;strong>.NET 4.7.1 support&lt;/strong>
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/commit/28aa946a9a277b6c2b1166af0397134b02bedf2d">28aa94&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Support Visual Basic project files (.vbroj) targeting .NET Core&lt;/strong>
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/626">#626&lt;/a>)&lt;/li>
&lt;li>&lt;strong>DisassemblyDiagnoser now supports generic types&lt;/strong>
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/640">#640&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Now it&amp;rsquo;s possible to benchmark both Mono and .NET Core from the same app&lt;/strong>
(See &lt;a href="https://github.com/dotnet/BenchmarkDotNet/issues/653">#653&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Many bug fixes&lt;/strong>
(See details below)&lt;/li>
&lt;/ul></description></item><item><title>Analyzing distribution of Mono GC collections</title><link>https://aakinshin.net/posts/mono-gc-collects/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mono-gc-collects/</guid><description>&lt;p>Sometimes I want to understand the GC performance impact on an application quickly.
I know that there are many powerful diagnostic tools and approaches,
but I&amp;rsquo;m a fan of the &amp;ldquo;right tool for the job&amp;rdquo; idea.
In simple cases, I prefer simple noninvasive approaches which provide a quick way
to get an overview of the current situation
(if everything is terrible, I always can switch to an advanced approach).
Today I want to share with you my favorite way to quickly get statistics
of GC pauses in Mono and generate nice plots like this:&lt;/p>
&lt;div class="row">
&lt;div class="mx-auto">
&lt;a href="https://aakinshin.net/posts/mono-gc-collects/img/plot-64.png" target="_blank" alt="plot-64">
&lt;img
class="mx-auto d-block img-fluid"
width='800'
src="https://aakinshin.net/posts/mono-gc-collects/img/plot-64.png" />
&lt;/a>
&lt;/div>
&lt;/div>
&lt;br /></description></item><item><title>BenchmarkDotNet v0.10.12</title><link>https://aakinshin.net/posts/bdn-v0_10_12/</link><pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/bdn-v0_10_12/</guid><description>&lt;p>BenchmarkDotNet v0.10.12 has been released! This release includes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Improved DisassemblyDiagnoser:&lt;/strong>
BenchmarkDotNet contains an embedded disassembler so that it can print assembly code for all benchmarks;
it&amp;rsquo;s not easy, but the disassembler evolves in every release.&lt;/li>
&lt;li>&lt;strong>Improved MemoryDiagnoser:&lt;/strong>
it has a better precision level, and it takes less time to evaluate memory allocations in a benchmark.&lt;/li>
&lt;li>&lt;strong>New TailCallDiagnoser:&lt;/strong>
now you get notifications when JIT applies the tail call optimizations to your methods.&lt;/li>
&lt;li>&lt;strong>Better environment info:&lt;/strong>
when your share performance results, it&amp;rsquo;s very important to share information about your environment.
The library generates the environment summary for you by default.
Now it contains information about the amount of physical CPU, physical cores, and logic cores.
If you run a benchmark on a virtual machine, you will get the name of the hypervisor
(e.g., Hyper-V, VMware, or VirtualBox).&lt;/li>
&lt;li>&lt;strong>Better summary table:&lt;/strong>
one of the greatest features of BenchmarkDotNet is the summary table.
It shows all important information about results in a compact and understandable form.
Now it has better customization options: you can display relative performance of different environments
(e.g., compare .NET Framework and .NET Core) and group benchmarks by categories.&lt;/li>
&lt;li>&lt;strong>New GC settings:&lt;/strong> now we support &lt;code>NoAffinitize&lt;/code>, &lt;code>HeapAffinitizeMask&lt;/code>, &lt;code>HeapCount&lt;/code>.&lt;/li>
&lt;li>Other minor improvements and bug fixes&lt;/li>
&lt;/ul></description></item><item><title>BenchmarkDotNet v0.10.10</title><link>https://aakinshin.net/posts/bdn-v0_10_10/</link><pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/bdn-v0_10_10/</guid><description>&lt;p>BenchmarkDotNet v0.10.10 has been released!
This release includes many new features like Disassembly Diagnoser, ParamsSources, .NET Core x86 support, Environment variables, and more!&lt;/p></description></item><item><title>Reflecting on performance testing</title><link>https://aakinshin.net/posts/reflecting-on-performance-testing/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/reflecting-on-performance-testing/</guid><description>&lt;p>Performance is an important feature for many projects.
Unfortunately, it&amp;rsquo;s an all too common situation when a developer accidentally spoils the performance adding some new code.
After a series of such incidents, people often start to think about performance regression testing.&lt;/p>
&lt;p>As developers, we write unit tests all the time.
These tests check that our business logic work as designed and that new features don&amp;rsquo;t break existing code.
It looks like a good idea to write some perf tests as well, which will verify that we don&amp;rsquo;t have any performance regressions.&lt;/p>
&lt;p>Turns out this is harder than it sounds.
A lot of developers don&amp;rsquo;t write perf tests at all.
Some teams write perf tests, but almost all of them use their own infrastructure for analysis
(which is not a bad thing in general because it&amp;rsquo;s usually designed for specific projects and requirements).
There are a lot of books about test-driven development (TDD),
but there are no books about performance-driven development (PDD).
There are well-known libraries for unit-testing (like xUnit/NUnit/MSTest for .NET),
but there are almost no libraries for performance regression testing.
Yeah, of course, there are &lt;em>some&lt;/em> libraries which you can use.
But there are troubles with &lt;em>well-known all recognized&lt;/em> libraries, approaches, and tools.
Ask your colleagues about it: some of them will give you different answers, the rest of them will start Googling it.&lt;/p>
&lt;p>There is no common understanding of what performance testing should look like.
This situation exists because it&amp;rsquo;s really hard to develop a solution which solves &lt;em>all problems&lt;/em> for &lt;em>all kind of projects&lt;/em>.
However, it doesn&amp;rsquo;t mean that we shouldn&amp;rsquo;t try.
And we should try, we should share our experience and discuss best practices.&lt;/p></description></item><item><title>Measuring Performance Improvements in .NET Core with BenchmarkDotNet (Part 1)</title><link>https://aakinshin.net/posts/stephen-toub-benchmarks-part1/</link><pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/stephen-toub-benchmarks-part1/</guid><description>&lt;p>A few days ago &lt;a href="https://github.com/stephentoub">Stephen Toub&lt;/a> published a great post
at the &lt;a href="https://blogs.msdn.microsoft.com/dotnet/">Microsoft .NET Blog&lt;/a>:
&lt;a href="https://blogs.msdn.microsoft.com/dotnet/2017/06/07/performance-improvements-in-net-core/">Performance Improvements in .NET Core&lt;/a>.
He showed some significant performance changes in .NET Core 2.0 Preview 1 (compared with .NET Framework 4.7).
The .NET Core uses RyuJIT for generating assembly code.
When I first tried RyuJIT (e.g.,
&lt;a href="https://blogs.msdn.microsoft.com/dotnet/2014/02/27/ryujit-ctp2-getting-ready-for-prime-time/">CTP2&lt;/a>,
&lt;a href="https://blogs.msdn.microsoft.com/clrcodegeneration/2014/10/30/ryujit-ctp5-getting-closer-to-shipping-and-with-better-simd-support/">CTP5&lt;/a>, 2014),
I wasn&amp;rsquo;t excited about this: the preview versions had some bugs, and it worked slowly on my applications.
However, the idea of a rethought and open-source JIT-compiler was a huge step forward and investment in the future.
RyuJIT had been developed very actively in recent years: not only by Microsoft but with the help of the community.
I&amp;rsquo;m still not happy about the generated assembly code in some methods, but I have to admit that the RyuJIT (as a part of .NET Core) works pretty well today:
it shows a good performance level not only on artificial benchmarks but also on real user code.
Also, there are a lot of changes
not only in &lt;a href="https://github.com/dotnet/coreclr">dotnet/coreclr&lt;/a> (the .NET Core runtime),
but also in &lt;a href="https://github.com/dotnet/corefx">dotnet/corefx&lt;/a> (the .NET Core foundational libraries).
It&amp;rsquo;s very nice to watch how the community helps to optimize well-used classes which have not changed for years.&lt;/p>
&lt;p>Now let&amp;rsquo;s talk about benchmarks.
For the demonstration, Stephen wrote a set of handwritten benchmarks.
A few people (in
&lt;a href="https://blogs.msdn.microsoft.com/dotnet/2017/06/07/performance-improvements-in-net-core/#comments">comments&lt;/a> and on &lt;a href="https://news.ycombinator.com/item?id=14507936">HackerNews&lt;/a>)
asked about &lt;a href="https://github.com/dotnet/BenchmarkDotNet">BenchmarkDotNet&lt;/a> regarding these samples (as a better tool for performance measurements).
So, I decided to try all these benchmarks on BenchmarkDotNet.&lt;/p>
&lt;p>In this post, we will discuss
how can BenchmarkDotNet help in such performance investigations,
which benchmarking approaches (and when) are better to use,
and how can we improve these measurements.&lt;/p></description></item><item><title>BenchmarkDotNet v0.10.7</title><link>https://aakinshin.net/posts/bdn-v0_10_7/</link><pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/bdn-v0_10_7/</guid><description>&lt;p>BenchmarkDotNet v0.10.7 has been released.
In this post, I will briefly cover the following features:&lt;/p>
&lt;ul>
&lt;li>LINQPad support&lt;/li>
&lt;li>Filters and categories&lt;/li>
&lt;li>Updated Setup/Cleanup attributes&lt;/li>
&lt;li>Better Value Types support&lt;/li>
&lt;li>Building Sources on Linux&lt;/li>
&lt;/ul></description></item><item><title>65535 interfaces ought to be enough for anybody</title><link>https://aakinshin.net/posts/mono-and-65535interfaces/</link><pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mono-and-65535interfaces/</guid><description>&lt;p>It was a bright, sunny morning.
There were no signs of trouble.
I came to work, opened Slack, and received many messages from my coworkers about failed tests.&lt;/p>
&lt;div class="mx-auto">
&lt;img class="mx-auto d-block" width="800" src="https://aakinshin.net/img/posts/dotnet/mono-and-65535interfaces/front.png" />
&lt;/div>
&lt;p>After a few hours of investigation, the situation became clear:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;m responsible for the unit tests subsystem in &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a>, and only tests from this subsystem were failing.&lt;/li>
&lt;li>I didn&amp;rsquo;t commit anything to the subsystem for a week because I worked with a local branch.
Other developers also didn&amp;rsquo;t touch this code.&lt;/li>
&lt;li>The unit tests subsystem is completely independent.
It&amp;rsquo;s hard to imagine a situation when only the corresponded tests would fail, thousands of other tests pass, and there are no changes in the source code.&lt;/li>
&lt;li>&lt;code>git blame&lt;/code> helped to find the &amp;ldquo;bad commit&amp;rdquo;: it didn&amp;rsquo;t include anything suspicious, only a few additional classes in other subsystems.&lt;/li>
&lt;li>Only tests on Linux and MacOS were red.
On Windows, everything was ok.&lt;/li>
&lt;li>Stacktraces in failed tests were completely random.
We had a new stack trace in each test from different subsystems.
There was no connection between these stack traces, unit tests source code, and the changes in the &amp;ldquo;bad commit.&amp;rdquo;
There was no clue where we should look for a problem.&lt;/li>
&lt;/ul>
&lt;p>So, what was special about this &amp;ldquo;bad commit&amp;rdquo;? Spoiler: after these changes, we sometimes have more than 65535 interface implementations at runtime.&lt;/p></description></item><item><title>A bug story about named mutex on Mono</title><link>https://aakinshin.net/posts/namedmutex-on-mono/</link><pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/namedmutex-on-mono/</guid><description>&lt;p>When you write some multithreading magic on .NET,
you can use a cool synchronization primitive called &lt;a href="https://msdn.microsoft.com/en-us/library/system.threading.mutex(v=vs.110).aspx">Mutex&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">var&lt;/span> &lt;span class="n">mutex&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Mutex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">false&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;Global\\MyNamedMutex&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You also can make it &lt;a href="https://msdn.microsoft.com/en-us/library/f55ddskf(v=vs.110).aspx">named&lt;/a> (and share the mutex between processes)
which works perfectly on Windows:&lt;/p>
&lt;div class="mx-auto">
&lt;img class="mx-auto d-block" width="600" src="https://aakinshin.net/img/posts/dotnet/namedmutex-on-mono/front.png" />
&lt;/div>
&lt;p>However, today the .NET Framework is cross-platform, so this code should work on any operation system.
What will happen if you use named mutex on Linux or MacOS with the help of Mono or CoreCLR?
Is it possible to create some tricky bug based on this case?
Of course, it does.
Today I want to tell you a story about such bug in &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a> which was a headache for several weeks.&lt;/p></description></item><item><title>InvalidDataException in Process.GetProcesses</title><link>https://aakinshin.net/posts/invaliddataexception-in-getprocesses/</link><pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/invaliddataexception-in-getprocesses/</guid><description>&lt;p>Consider the following program:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">static&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">string&lt;/span>&lt;span class="p">[]&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">try&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Process&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetProcesses&lt;/span>&lt;span class="p">();&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">catch&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">Exception&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>It seems that all exceptions should be caught.
However, &lt;em>sometimes&lt;/em>, I had the following exception on Linux with &lt;code>dotnet cli-1.0.0-preview2&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="err">$&lt;/span> &lt;span class="n">dotnet&lt;/span> &lt;span class="n">run&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">IO&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">InvalidDataException&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Found&lt;/span> &lt;span class="n">invalid&lt;/span> &lt;span class="n">data&lt;/span> &lt;span class="k">while&lt;/span> &lt;span class="n">decoding&lt;/span>&lt;span class="p">.&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">IO&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">StringParser&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ParseNextChar&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">Interop&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">procfs&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">TryParseStatFile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span> &lt;span class="n">statFilePath&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ParsedStat&lt;/span>&lt;span class="p">&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ReusableTextReader&lt;/span> &lt;span class="n">reusableReader&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Diagnostics&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ProcessManager&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">CreateProcessInfo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ParsedStat&lt;/span> &lt;span class="n">procFsStat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ReusableTextReader&lt;/span> &lt;span class="n">reusableReader&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Diagnostics&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ProcessManager&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">CreateProcessInfo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Int32&lt;/span> &lt;span class="n">pid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ReusableTextReader&lt;/span> &lt;span class="n">reusableReader&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Diagnostics&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ProcessManager&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetProcessInfos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span> &lt;span class="n">machineName&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Diagnostics&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Process&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetProcesses&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span> &lt;span class="n">machineName&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Diagnostics&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Process&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetProcesses&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">at&lt;/span> &lt;span class="n">DotNetCoreConsoleApplication&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Program&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">[]&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">in&lt;/span> &lt;span class="p">/&lt;/span>&lt;span class="n">home&lt;/span>&lt;span class="p">/&lt;/span>&lt;span class="n">akinshin&lt;/span>&lt;span class="p">/&lt;/span>&lt;span class="n">Program&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">cs&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">line&lt;/span> &lt;span class="m">12&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>How is that possible?&lt;/p></description></item><item><title>Why is NuGet search in Rider so fast?</title><link>https://aakinshin.net/posts/rider-nuget-search/</link><pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/rider-nuget-search/</guid><description>&lt;p>I&amp;rsquo;m the guy who develops the NuGet manager in &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a>.
It&amp;rsquo;s not ready yet, there are some bugs here and there, but it already works pretty well.
The feature which I am most proud of is smart and fast search:&lt;/p>
&lt;div class="mx-auto">
&lt;img class="mx-auto d-block" width="400" src="https://aakinshin.net/img/posts/dotnet/rider-nuget-search/front.gif" />
&lt;/div>
&lt;p>Today I want to share with you some technical details about how it was implemented.&lt;/p></description></item><item><title>NuGet2 and a DirectorySeparatorChar bug</title><link>https://aakinshin.net/posts/nuget2-and-directoryseparatorchar/</link><pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/nuget2-and-directoryseparatorchar/</guid><description>&lt;p>In &lt;a href="https://www.jetbrains.com/rider/">Rider&lt;/a>, we care a lot about performance.
I like to improve the application responsiveness and do interesting optimizations all the time.
Rider is already well-optimized, and it&amp;rsquo;s often hard to make significant performance improvements, so usually I do micro-optimizations which do not have a very big impact on the whole application.
However, sometimes it&amp;rsquo;s possible to improve the speed of a feature 100 times with just a few lines of code.&lt;/p>
&lt;p>Rider is based on &lt;a href="https://www.jetbrains.com/resharper/">ReSharper&lt;/a>, so we have a lot of cool features out of the box.
One of these features is &lt;a href="https://www.jetbrains.com/help/resharper/2016.3/Code_Analysis__Solution-Wide_Analysis.html">Solution-Wide Analysis&lt;/a>
which lets you constantly keep track of issues in your solution.
Sometimes, solution-wide analysis takes a lot of time to run because there are many files which should be analyzed.
Of course, it works super fast on small and projects.&lt;/p>
&lt;p>Let&amp;rsquo;s talk about a performance bug (&lt;a href="https://youtrack.jetbrains.com/issue/RIDER-3742">#RIDER-3742&lt;/a>) that we recently had.&lt;/p>
&lt;ul>
&lt;li>&lt;em>Repro:&lt;/em> Open Rider, create a new &amp;ldquo;ASP .NET MVC Application&amp;rdquo;, enable solution wide-analysis.&lt;/li>
&lt;li>&lt;em>Expected:&lt;/em> The analysis should take 1 second.&lt;/li>
&lt;li>&lt;em>Actual:&lt;/em> The analysis takes 1 second on Windows and &lt;strong>2 minutes&lt;/strong> on Linux and MacOS.&lt;/li>
&lt;/ul></description></item><item><title>Performance exercise: Division</title><link>https://aakinshin.net/posts/perfex-div/</link><pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/perfex-div/</guid><description>&lt;p>In the previous post, we &lt;a href="https://aakinshin.net/en/blog/dotnet/perfex-min/">discussed&lt;/a> the performance space of the minimum function
which was implemented via a simple ternary operator and with the help of bit magic.
Now we continue to talk about performance and bit hacks.
In particular, we will divide a positive number by three:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">uint&lt;/span> &lt;span class="n">Div3Simple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">uint&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="p">/&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">uint&lt;/span> &lt;span class="n">Div3BitHacks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">uint&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">uint&lt;/span>&lt;span class="p">)((&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="p">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">ulong&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="n">xAAAAAAAB&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">&amp;gt;&amp;gt;&lt;/span> &lt;span class="m">33&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>As usual, it&amp;rsquo;s hard to say which method is faster in advanced because the performance depends on the environment.
Here are some interesting results:&lt;/p>
&lt;table class="table table-sm">
&lt;tr> &lt;th>&lt;/th> &lt;th>Simple&lt;/th> &lt;th>BitHacks&lt;/th> &lt;/tr>
&lt;tr> &lt;th>LegacyJIT-x86&lt;/th> &lt;td class="norm">≈8.3ns&lt;/td> &lt;td class="fast">≈2.6ns&lt;/td> &lt;/tr>
&lt;tr> &lt;th>LegacyJIT-x64&lt;/th> &lt;td class="fast">≈2.6ns&lt;/td> &lt;td class="fast">≈1.7ns&lt;/td> &lt;/tr>
&lt;tr> &lt;th>RyuJIT-x64 &lt;/th> &lt;td class="norm">≈6.9ns&lt;/td> &lt;td class="fast">≈1.5ns&lt;/td> &lt;/tr>
&lt;tr> &lt;th>Mono4.6.2-x86&lt;/th> &lt;td class="norm">≈8.5ns&lt;/td> &lt;td class="slow">≈14.4ns&lt;/td> &lt;/tr>
&lt;tr> &lt;th>Mono4.6.2-x64&lt;/th> &lt;td class="norm">≈8.3ns&lt;/td> &lt;td class="fast">≈2.8ns&lt;/td> &lt;/tr>
&lt;/table></description></item><item><title>Performance exercise: Minimum</title><link>https://aakinshin.net/posts/perfex-min/</link><pubDate>Tue, 20 Dec 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/perfex-min/</guid><description>&lt;p>Performance is tricky. Especially, if you are working with very fast operations. In today benchmarking exercise, we will try to measure performance of two simple methods which calculate minimum of two numbers. Sounds easy? Ok, let&amp;rsquo;s do it, here are our guinea pigs for today:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">int&lt;/span> &lt;span class="n">MinTernary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="p">?&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">MinBitHacks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">&amp;amp;&lt;/span> &lt;span class="p">((&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="p">-&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">&amp;gt;&amp;gt;&lt;/span> &lt;span class="m">31&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="p">&amp;amp;&lt;/span> &lt;span class="p">(~(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="p">-&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">&amp;gt;&amp;gt;&lt;/span> &lt;span class="m">31&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And here are some results:&lt;/p>
&lt;table class="table table-sm">
&lt;style type="text/css" scoped>
td.slow { color: #ff4444; }
td.fast { color: #00C851; }
&lt;/style>
&lt;tr> &lt;th>&lt;/th> &lt;th colspan="2">Random&lt;/th> &lt;th colspan="2">Const&lt;/th> &lt;/tr>
&lt;tr> &lt;th>&lt;/th> &lt;th>Ternary&lt;/th> &lt;th>BitHacks&lt;/th> &lt;th>Ternary&lt;/th> &lt;th>BitHacks&lt;/th> &lt;/tr>
&lt;tr> &lt;th>LegacyJIT-x86&lt;/th>
&lt;td class="slow">≈643µs&lt;/td>
&lt;td class="fast">≈227µs&lt;/td>
&lt;td class="fast">≈160µs&lt;/td>
&lt;td class="slow">≈226µs&lt;/td>
&lt;/tr>
&lt;tr> &lt;th>LegacyJIT-x64&lt;/th>
&lt;td class="slow">≈450µs&lt;/td>
&lt;td class="fast">≈123µs&lt;/td>
&lt;td class="fast">≈68µs&lt;/td>
&lt;td class="slow">≈123µs&lt;/td>
&lt;/tr>
&lt;tr> &lt;th>RyuJIT-x64&lt;/th>
&lt;td class="slow">≈594µs&lt;/td>
&lt;td class="fast">≈241µs&lt;/td>
&lt;td class="fast">≈180µs&lt;/td>
&lt;td class="slow">≈241µs&lt;/td>
&lt;/tr>
&lt;tr> &lt;th>Mono-x64&lt;/th>
&lt;td class="fast">≈203µs&lt;/td>
&lt;td class="slow">≈283µs&lt;/td>
&lt;td class="fast">≈204µs&lt;/td>
&lt;td class="slow">≈282µs&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>What&amp;rsquo;s going on here? Let&amp;rsquo;s discuss it in detail.&lt;/p></description></item><item><title>Stopwatch under the hood</title><link>https://aakinshin.net/posts/stopwatch/</link><pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/stopwatch/</guid><description>&lt;p>&lt;strong>Update:&lt;/strong>
You can find an updated and significantly improved version of this post in my book &lt;a href="https://aakinshin.net/prodotnetbenchmarking/">&amp;ldquo;Pro .NET Benchmarking&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>In &lt;a href="https://aakinshin.net/en/blog/dotnet/datetime/">the previous post&lt;/a>, we discussed &lt;code>DateTime&lt;/code>.
This structure can be used in situations when you don&amp;rsquo;t need a good level of precision.
If you want to do high-precision time measurements, you need a better tool because &lt;code>DateTime&lt;/code> has a small resolution and a big latency.
Also, time is tricky, you can create wonderful bugs if you don&amp;rsquo;t understand how it works (see &lt;a href="http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">Falsehoods programmers believe about time&lt;/a> and &lt;a href="http://infiniteundo.com/post/25509354022/more-falsehoods-programmers-believe-about-time">More falsehoods programmers believe about time&lt;/a>).&lt;/p>
&lt;p>In this post, we will briefly talk about the &lt;a href="https://msdn.microsoft.com/library/system.diagnostics.stopwatch.aspx">Stopwatch&lt;/a> class:&lt;/p>
&lt;ul>
&lt;li>Which kind of hardware timers could be a base for &lt;code>Stopwatch&lt;/code>&lt;/li>
&lt;li>High precision timestamp API on Windows and Linux&lt;/li>
&lt;li>Latency and Resolution of &lt;code>Stopwatch&lt;/code> in different environments&lt;/li>
&lt;li>Common pitfalls: which kind of problems could we get trying to measure small time intervals&lt;/li>
&lt;/ul>
&lt;p>If you are not a .NET developer, you can also find a lot of useful information in this post: mainly we will discuss low-level details of high-resolution timestamping (probably your favorite language also uses the same API).
As usual, you can also find useful links for further reading.&lt;/p></description></item><item><title>DateTime under the hood</title><link>https://aakinshin.net/posts/datetime/</link><pubDate>Fri, 19 Aug 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/datetime/</guid><description>&lt;p>&lt;strong>Update:&lt;/strong>
You can find an updated and significantly improved version of this post in my book &lt;a href="https://aakinshin.net/prodotnetbenchmarking/">&amp;ldquo;Pro .NET Benchmarking&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>&lt;a href="https://msdn.microsoft.com/library/system.datetime.aspx">DateTime&lt;/a> is a widely used .NET type. A lot of developers use it all the time, but not all of them really know how it works. In this post, I discuss &lt;a href="https://msdn.microsoft.com/library/system.datetime.utcnow.aspx">DateTime.UtcNow&lt;/a>: how it&amp;rsquo;s implemented, what the latency and the resolution of &lt;code>DateTime&lt;/code> on Windows and Linux, how the resolution can be changed, and how it can affect your application. This post is an overview, so you probably will not see super detailed explanations of some topics, but you will find a lot of useful links for further reading.&lt;/p></description></item><item><title>LegacyJIT-x86 and first method call</title><link>https://aakinshin.net/posts/legacyjitx86-and-first-method-call/</link><pubDate>Mon, 04 Apr 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/legacyjitx86-and-first-method-call/</guid><description>&lt;p>Today I tell you about one of my favorite benchmarks (this method doesn&amp;rsquo;t return a useful value, we need it only as an example):&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="na">[Benchmark]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">string&lt;/span> &lt;span class="n">Sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">double&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">sw&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Stopwatch&lt;/span>&lt;span class="p">();&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">10001&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">++)&lt;/span>
&lt;span class="n">a&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kt">string&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Format&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;{0}{1}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sw&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ElapsedMilliseconds&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>An interesting fact: if you call &lt;code>Stopwatch.GetTimestamp()&lt;/code> before the first call of the &lt;code>Sum&lt;/code> method, you improve &lt;code>Sum&lt;/code> performance several times (works only with LegacyJIT-x86).&lt;/p></description></item><item><title>Visual Studio and ProjectTypeGuids.cs</title><link>https://aakinshin.net/posts/projecttypeguids/</link><pubDate>Sat, 27 Feb 2016 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/projecttypeguids/</guid><description>&lt;p>It&amp;rsquo;s a story about how I tried to open a project in Visual Studio for a few hours. The other day, I was going to do some work. I pulled last commits from a repo, opened Visual Studio, and prepared to start coding. However, one of a project in my solution failed to open with a strange message:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-txt" data-lang="txt">error : The operation could not be completed.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the Solution Explorer, I had &lt;em>&amp;ldquo;load failed&amp;rdquo;&lt;/em> as a project status and the following message instead of the file tree: &lt;em>&amp;ldquo;The project requires user input. Reload the project for more information.&amp;quot;&lt;/em> Hmm, ok, I reloaded the project and got a few more errors:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-txt" data-lang="txt">error : The operation could not be completed.
error : The operation could not be completed.
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Blittable types</title><link>https://aakinshin.net/posts/blittable/</link><pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/blittable/</guid><description>&lt;p>Challenge of the day: what will the following code display?&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="na">[StructLayout(LayoutKind.Explicit)]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="k">struct&lt;/span> &lt;span class="nc">UInt128&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="na"> [FieldOffset(0)]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">ulong&lt;/span> &lt;span class="n">Value1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="na"> [FieldOffset(8)]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">ulong&lt;/span> &lt;span class="n">Value2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="na">[StructLayout(LayoutKind.Sequential)]&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="k">struct&lt;/span> &lt;span class="nc">MyStruct&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="n">UInt128&lt;/span> &lt;span class="n">UInt128&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">char&lt;/span> &lt;span class="n">Char&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">Program&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="k">static&lt;/span> &lt;span class="k">unsafe&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Main&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">myStruct&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">MyStruct&lt;/span>&lt;span class="p">();&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">baseAddress&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">)&amp;amp;&lt;/span>&lt;span class="n">myStruct&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">uInt128Adress&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">)&amp;amp;&lt;/span>&lt;span class="n">myStruct&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">UInt128&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uInt128Adress&lt;/span> &lt;span class="p">-&lt;/span> &lt;span class="n">baseAddress&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Marshal&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">OffsetOf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">typeof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MyStruct&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s">&amp;#34;UInt128&amp;#34;&lt;/span>&lt;span class="p">));&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>A hint: two zeros or two another same values are wrong answers in the general case. The following table shows the console output on different runtimes:&lt;/p>
&lt;table>
&lt;tr>&lt;th>&lt;/th>&lt;th>MS.NET-x86&lt;/th>&lt;th>MS.NET-x64&lt;/th>&lt;th>Mono&lt;/th>&lt;/tr>
&lt;tr>&lt;td>uInt128Adress - baseAddress &lt;/td>&lt;td>4&lt;/td>&lt;td>8&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>Marshal.OffsetOf(typeof(MyStruct), "UInt128")&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;/table>
&lt;p>If you want to know why it happens, you probably should learn some useful information about blittable types.&lt;/p></description></item><item><title>RyuJIT RC and constant folding</title><link>https://aakinshin.net/posts/ryujit-rc-and-constant-folding/</link><pubDate>Tue, 12 May 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/ryujit-rc-and-constant-folding/</guid><description>&lt;p>&lt;strong>Update:&lt;/strong> The below results are valid for the release version of RyuJIT in .NET Framework 4.6 without updates.&lt;/p>
&lt;p>The challenge of the day: which method is faster?&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="kt">double&lt;/span> &lt;span class="n">Sqrt13&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span>
&lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">6&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">7&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">8&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">9&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">10&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span>
&lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">11&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">12&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">13&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">double&lt;/span> &lt;span class="n">Sqrt14&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span>
&lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">6&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">7&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">8&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">9&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">10&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span>
&lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">11&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">12&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">13&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">14&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>I have measured the methods performance with help of &lt;a href="https://github.com/AndreyAkinshin/BenchmarkDotNet">BenchmarkDotNet&lt;/a> for RyuJIT RC (a part of .NET Framework 4.6 RC) and received the following results:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-md" data-lang="md">// BenchmarkDotNet=v0.7.4.0
// OS=Microsoft Windows NT 6.2.9200.0
// Processor=Intel(R) Core(TM) i7-4702MQ CPU ＠ 2.20GHz, ProcessorCount=8
// CLR=MS.NET 4.0.30319.0, Arch=64-bit [RyuJIT]
Common: Type=Math_DoubleSqrtAvx Mode=Throughput Platform=X64 Jit=RyuJit .NET=Current
Method | AvrTime | StdDev | op/s |
------- |--------- |---------- |------------- |
Sqrt13 | 55.40 ns | 0.571 ns | 18050993.06 |
Sqrt14 | 1.43 ns | 0.0224 ns | 697125029.18 |
&lt;/code>&lt;/pre>&lt;/div>&lt;p>How so? If I add one more &lt;code>Math.Sqrt&lt;/code> to the expression, the method starts work 40 times faster! Let&amp;rsquo;s examine the situation..&lt;/p></description></item><item><title>Unrolling of small loops in different JIT versions</title><link>https://aakinshin.net/posts/unrolling-of-small-loops-in-different-jit-versions/</link><pubDate>Mon, 02 Mar 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/unrolling-of-small-loops-in-different-jit-versions/</guid><description>&lt;p>Challenge of the day: what will the following code display?&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">struct&lt;/span> &lt;span class="nc">Point&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">Y&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">static&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Point&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">X&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="s">&amp;#34; &amp;#34;&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Y&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">static&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Main&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Point&lt;/span>&lt;span class="p">();&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">X&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">X&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">++)&lt;/span>
&lt;span class="n">Print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The right answer: it depends. There is a bug in CLR2 JIT-x86 which spoil this wonderful program. This story is about optimization that called unrolling of small loops. This is a very interesting theme, let&amp;rsquo;s discuss it in detail.&lt;/p></description></item><item><title>RyuJIT CTP5 and loop unrolling</title><link>https://aakinshin.net/posts/ryujit-ctp5-and-loop-unrolling/</link><pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/ryujit-ctp5-and-loop-unrolling/</guid><description>&lt;p>RyuJIT will be available soon. It is a next generation JIT-compiler for .NET-applications. Microsoft likes to tell us about the benefits of SIMD using and JIT-compilation time reducing. But what about basic code optimization which is usually applying by a compiler? Today we talk about the loop unrolling (unwinding) optimization. In general, in this type of code optimization, the code&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">1024&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">++)&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>transforms to&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">1024&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">+=&lt;/span> &lt;span class="m">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Such approach can significantly increase performance of your code. So, what&amp;rsquo;s about loop unrolling in .NET?&lt;/p></description></item><item><title>JIT version determining in runtime</title><link>https://aakinshin.net/posts/jit-version-determining-in-runtime/</link><pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/jit-version-determining-in-runtime/</guid><description>&lt;p>Sometimes I want to know used JIT compiler version in my little C# experiments. It is clear that it is possible to determine the version in advance based on the environment. However, sometimes I want to know it in runtime to perform specific code for the current JIT compiler. More formally, I want to get the value from the following enum:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">enum&lt;/span> &lt;span class="n">JitVersion&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Mono&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">MsX86&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">MsX64&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">RyuJit&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is easy to detect Mono by existing of the &lt;code>Mono.Runtime&lt;/code> class. Otherwise, we can assume that we work with Microsoft JIT implementation. It is easy to detect JIT-x86 with help of &lt;code>IntPtr.Size == 4&lt;/code>. The challenge is to distinguish JIT-x64 and RyuJIT. Next, I will show how you can do it with help of the bug from my &lt;a href="http://aakinshin.net/en/blog/dotnet/subexpression-elimination-bug-in-jit-x64/">previous post&lt;/a>.&lt;/p></description></item><item><title>A bug story about JIT-x64</title><link>https://aakinshin.net/posts/subexpression-elimination-bug-in-jit-x64/</link><pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/subexpression-elimination-bug-in-jit-x64/</guid><description>&lt;p>Can you say, what will the following code display for &lt;code>step=1&lt;/code>?&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">++)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">bar&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="m">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="p">&amp;lt;&lt;/span> &lt;span class="m">2&lt;/span> &lt;span class="p">*&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="p">+=&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="m">10&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you think about specific numbers, you are wrong. The right answer: it depends. The post title suggests to us, the program can has a strange behavior for x64.&lt;/p></description></item><item><title>A story about JIT-x86 inlining and starg</title><link>https://aakinshin.net/posts/inlining-and-starg/</link><pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/inlining-and-starg/</guid><description>&lt;p>Sometimes you can learn a lot during reading source .NET. Let&amp;rsquo;s open the source code of a &lt;code>Decimal&lt;/code> constructor from .NET Reference Source (&lt;a href="http://referencesource.microsoft.com/#mscorlib/system/decimal.cs,158">mscorlib/system/decimal.cs,158&lt;/a>):&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="c1">// Constructs a Decimal from an integer value.
&lt;/span>&lt;span class="c1">//
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="k">public&lt;/span> &lt;span class="n">Decimal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="k">value&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="c1">// JIT today can&amp;#39;t inline methods that contains &amp;#34;starg&amp;#34; opcode.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="c1">// For more details, see DevDiv Bugs 81184: x86 JIT CQ: Removing the inline striction of &amp;#34;starg&amp;#34;.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">value_copy&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">value&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">value_copy&lt;/span> &lt;span class="p">&amp;gt;=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">flags&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">flags&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">SignMask&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">value_copy&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">-&lt;/span>&lt;span class="n">value_copy&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="n">lo&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">value_copy&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">mid&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">hi&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The comment states that JIT-x86 can&amp;rsquo;t apply the inlining optimization for a method that contains the &lt;a href="https://msdn.microsoft.com/library/system.reflection.emit.opcodes.starg.aspx">starg&lt;/a> IL-opcode. Curious, is not it?&lt;/p></description></item><item><title>About UTF-8 conversions in Mono</title><link>https://aakinshin.net/posts/mono-utf8-conversions/</link><pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/mono-utf8-conversions/</guid><description>&lt;p>This post is a logical continuation of the Jon Skeet&amp;rsquo;s blog post &lt;a href="http://codeblog.jonskeet.uk/2014/11/07/when-is-a-string-not-a-string">“When is a string not a string?”&lt;/a>. Jon showed very interesting things about behavior of ill-formed Unicode strings in .NET. I wondered about how similar examples will work on Mono. And I have got very interesting results.&lt;/p>
&lt;h3 id="experiment-1-compilation">Experiment 1: Compilation&lt;/h3>
&lt;p>Let&amp;rsquo;s take the Jon&amp;rsquo;s code with a small modification. We will just add &lt;code>text&lt;/code> null check in &lt;code>DumpString&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">using&lt;/span> &lt;span class="nn">System&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">using&lt;/span> &lt;span class="nn">System.ComponentModel&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">using&lt;/span> &lt;span class="nn">System.Text&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">using&lt;/span> &lt;span class="nn">System.Linq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="na">[Description(Value)]&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">Test&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">const&lt;/span> &lt;span class="kt">string&lt;/span> &lt;span class="n">Value&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="s">&amp;#34;X\ud800Y&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">static&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Main&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">description&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">DescriptionAttribute&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="k">typeof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Test&lt;/span>&lt;span class="p">).&lt;/span>
&lt;span class="n">GetCustomAttributes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">typeof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">DescriptionAttribute&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="k">true&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;span class="n">DumpString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Attribute&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">description&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Description&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">DumpString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;Constant&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Value&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">static&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">DumpString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">string&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">string&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;{0}: &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span> &lt;span class="p">!=&lt;/span> &lt;span class="k">null&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">utf16&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Select&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">((&lt;/span>&lt;span class="kt">uint&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="n">ToString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;x4&amp;#34;&lt;/span>&lt;span class="p">));&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">string&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Join&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">utf16&lt;/span>&lt;span class="p">));&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">else&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;null&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Happy Monday!</title><link>https://aakinshin.net/posts/happy-monday/</link><pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/happy-monday/</guid><description>&lt;p>Today I tell you a story about one tricky bug. The bug is a tricky one because it doesn&amp;rsquo;t allow me to debug my application on Mondays. I&amp;rsquo;m serious right now: the debug mode doesn&amp;rsquo;t work every Monday. Furthermore, the bug literally tell me: &amp;ldquo;Happy Monday!&amp;rdquo;.&lt;/p>
&lt;p>So, the story. It was a wonderful Sunday evening, no signs of trouble. We planned to release a new version of our software (a minor one, but it includes some useful features). Midnight on the clock. Suddenly, I came up with the idea that we have a minor bug that should be fixed. It requires a few lines of code and 10 minutes to do it. And I decided to write needed logic before I go to sleep. I open VisualStudio, lunch build, and wait. But something goes wrong, because I get the following error:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-txt" data-lang="txt">Error connecting to the pipe server.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hmm. It is a strange error.&lt;/p></description></item><item><title>To Refactor Or Not To Refactor?</title><link>https://aakinshin.net/posts/refactoring/</link><pubDate>Sat, 19 Jul 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/refactoring/</guid><description>&lt;p>I like refactoring. No, I love refactoring. No, not even like this. I awfully love refactoring.&lt;/p>
&lt;p>I hate bad code and bad architecture. I feel quite creepy when I design a new feature and the near-by class contains absolute mess. I just can’t look at the sadly-looking variables. Sometimes before falling asleep I close my eyes and imagine what could be improved in the project. Sometimes I wake up at 3:00AM and go to my computer to improve something. I want to have not just code, but a masterpiece that is pleasant to look at, that is pleasant to work with at any stage of the project.&lt;/p>
&lt;p>If you just a little bit share my feelings we have something to talk about. The matter is that over some time something inside me began to hint that it’s a bad idea to refactor all code, everywhere and all the time. Understand me correctly – code should be good (even better when it’s ideal), but in real life it’s not reasonable to improve code instantly. I formed some rules about the refactoring timeliness. If I am itching to improve something, I look at these rules and think “Is that the moment when I need to refactor the code?” So, let’s talk about when refactoring is necessary and when it’s inappropriate.&lt;/p></description></item><item><title>Strange behavior of FindElementsInHostCoordinates in WinRT</title><link>https://aakinshin.net/posts/findelementsinhostcoordinates/</link><pubDate>Tue, 29 Apr 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/findelementsinhostcoordinates/</guid><description>&lt;p>Silverlight features a splendid method: &lt;a href="http://msdn.microsoft.com/en-us/library/system.windows.media.visualtreehelper.findelementsinhostcoordinates(v=vs.95).aspx">VisualTreeHelper.FindElementsInHostCoordinates&lt;/a>. It allows the &lt;code>HitTest&lt;/code>, i.e. makes it possible for a point or rectangle to search for all visual sub-tree objects that intersect this rectangle or point. Formally the same method &lt;a href="http://msdn.microsoft.com/en-us/library/windows/apps/windows.ui.xaml.media.visualtreehelper.findelementsinhostcoordinates.aspx">VisualTreeHelper.FindElementsInHostCoordinates&lt;/a> is available in WinRT. And it seems the method looks in the same way, but there is a little nuance. It works differently in different versions of the platform. So, let’s see what’s going on.&lt;/p></description></item><item><title>About System.Drawing.Color and operator ==</title><link>https://aakinshin.net/posts/system-drawing-color-equals/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/system-drawing-color-equals/</guid><description>&lt;p>Operator &lt;code>==&lt;/code> that allows easy comparison of your objects is overridden for many standard structures in .NET. Unfortunately, not every developer really knows what is actually compared when working with this wonderful operator. This brief blog post will show the comparison logic based on a sample of &lt;code>System.Drawing.Color&lt;/code>. What do you think the following code will get:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">var&lt;/span> &lt;span class="n">redName&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">Color&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Red&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">var&lt;/span> &lt;span class="n">redArgb&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">Color&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">FromArgb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">255&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">255&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">0&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">redName&lt;/span> &lt;span class="p">==&lt;/span> &lt;span class="n">redArgb&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Setting up build configuration in .NET</title><link>https://aakinshin.net/posts/msbuild-configurations/</link><pubDate>Sat, 08 Feb 2014 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/msbuild-configurations/</guid><description>&lt;p>You get two default build configurations: Debug and Release, when creating a new project in Visual Studio. And it’s enough for most small projects. But there can appear a necessity to extend it with the additional configurations. It’s ok if you need to add just a couple of new settings, but what if there are tens of such settings? And what if your solution contains 20 projects that need setting up of these configurations? In this case it becomes quite difficult to manage and modify build parameters.&lt;/p>
&lt;p>In this article, we will review a way to make this process simpler by reducing description of the build configurations.&lt;/p></description></item><item><title>Jon Skeet's Quiz</title><link>https://aakinshin.net/posts/jon-skeet-quiz/</link><pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/jon-skeet-quiz/</guid><description>&lt;p>Jon Skeet was once asked to give three questions to check how well you know C#. He asked the &lt;a href="http://www.dotnetcurry.com/magazine/jon-skeet-quiz.aspx">following questions&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Q1.&lt;/strong> &lt;em>What constructor call can you write such that this prints True (at least on the Microsoft .NET implementation)?&lt;/em>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">object&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="cm">/* fill in code here */&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="kt">object&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="cm">/* fill in code here */&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">Console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">WriteLine&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="p">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Note that it’s just a constructor call, and you can’t change the type of the variables.&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Q2.&lt;/strong> &lt;em>How can you make this code compile such that it calls three different method overloads?&lt;/em>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">void&lt;/span> &lt;span class="n">Foo&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">EvilMethod&lt;/span>&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="kt">string&lt;/span>&lt;span class="p">&amp;gt;();&lt;/span>
&lt;span class="n">EvilMethod&lt;/span>&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">&amp;gt;();&lt;/span>
&lt;span class="n">EvilMethod&lt;/span>&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="kt">int?&lt;/span>&lt;span class="p">&amp;gt;();&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>Q3.&lt;/strong> &lt;em>With a local variable (so no changing the variable value cunningly), how can you make this code fail on the second line?&lt;/em>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="kt">string&lt;/span> &lt;span class="n">text&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ToString&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="c1">// No exception
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">Type&lt;/span> &lt;span class="n">type&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetType&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="c1">// Bang!
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>These questions seemed interesting to me, that is why I decided to discuss the solutions.&lt;/p></description></item><item><title>Perfect code and real projects</title><link>https://aakinshin.net/posts/perfect-code-and-real-projects/</link><pubDate>Wed, 28 Aug 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/perfect-code-and-real-projects/</guid><description>I’ve got a problem. I am a perfectionist. I like perfect code. This is not only the correct way to develop applications but also the real proficiency. I enjoy reading a good listing not less than reading a good book. Developing architecture of a big project is no simpler than designing architecture of a big building. In case the work is good the result is no less beautiful. I am sometimes fascinated by how elegantly the patterns are entwined in the perfect software system. I am delighted by the attention to details when every method is so simple and understandable that can be a classic sample of the perfect code.
But, unfortunately, this splendor is ruined by stern reality and real projects. If we talk about production project, users don’t care how beautiful your code is and how wonderful your architecture is, they care to have a properly working project. But I still think that in any case you need to strive for writing good code, but without getting stuck on this idea. After reading various holy-war discussions related to correct approaches to writing code I noticed a trend: everyone tries to apply the mentioned approaches not to programming in general, but to personal development experience, to their own projects. Many developers don’t understand that good practice is not an absolute rule that should be followed in 100% of scenarios. It’s just an advice on what to do in most cases. You can get a dozen of scenarios where the practice won’t work at all. But it doesn’t mean that the approach is not that good, it’s just used in the wrong environment.
There is another problem: some developers are not that good as they think. I often see the following situation: such developer got some idea (without getting deep into details) in the big article about the perfect code and he started to use it everywhere and the developer’s code became even worse.</description></item><item><title>To Add Comments or Not to Add?</title><link>https://aakinshin.net/posts/comments/</link><pubDate>Wed, 28 Aug 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/comments/</guid><description>&lt;p>&lt;em>A really good comment is the one you managed to avoid. (c) Uncle Bob&lt;/em>&lt;/p>
&lt;p>Lately, I’ve been feeling really tired of hot discussions on if it’s necessary to add comments in the code. As a rule, there are self-confident juniors with the indisputable statement as: “Why not to comment it, it will be unreadable without the comments!” on one side. And experienced seniors are on the other side. They understand that if it’s possible to go without the comments than “You better, damn it, do it in this way!” Probably, many developers got comment cravings since they’ve been students when professors made them comment every code line, “to make the student better understand it”. Real projects shouldn’t contain a lot of comments that only spoil the code. I don’t agitate for avoiding comments at all, but if you managed to write the code that doesn’t need comments, you can consider it your small victory. I would like to refer you to some good books that helped form my position. I like and respect these authors and completely share their opinion.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670">Steven C. McConnell, Code Complete&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882">Robert Martin, Clean Code: A Handbook of Agile Software Craftsmanship&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.amazon.com/The-Readable-Code-Theory-Practice/dp/0596802293">Dustin Boswell, Trevor Foucher, The Art of Readable Code (Theory in Practice)&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Unexpected area to collect garbage in .NET</title><link>https://aakinshin.net/posts/gc-native/</link><pubDate>Thu, 08 Aug 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/gc-native/</guid><description>&lt;p>The .NET framework provides an intelligent garbage collector that saves us a trouble of manual memory management. And in 95% of cases you can forget about memory and related issues. But the remaining 5% have some specific aspects connected to unmanaged resources, too big objects, etc. And it’s better to know how the garbage is collected. Otherwise, you can get surprises.&lt;/p>
&lt;p>Do you think GC is able to collect an object till its last method is complete? It appears it is. But it is necessary to run an application in release mode without debugging. In this case JIT compiler will perform optimizations that will make this situation possible. Of course, JIT compiler does it when the remaining method body doesn’t contain references to the object or its fields. It should seem a very harmless optimization. But it can lead to the problems if you work with the unmanaged resources: object compilation can be executed before the operation over the unmanaged resource is finished. And most likely it will result in the application crash.&lt;/p></description></item><item><title>Unobviousness in use of C# closures</title><link>https://aakinshin.net/posts/closures/</link><pubDate>Wed, 07 Aug 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/closures/</guid><description>&lt;p>C# gives us an ability to use closures. This is a powerful tool that allows anonymous methods and lambda-functions to capture unbound variables in their lexical scope. And many programmers in .NET world like using closures very much, but only few of them understand how they really work. Let’s start with a simple sample:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">e&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Nothing complicated happens here: we just captured a local variable &lt;code>e&lt;/code> in its lambda that is passed to some &lt;code>Foo&lt;/code> method. Let’s see how the compiler will expand such construction.*&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">void&lt;/span> &lt;span class="n">Run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="n">DisplayClass&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">DisplayClass&lt;/span>&lt;span class="p">();&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">e&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">Foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Action&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">private&lt;/span> &lt;span class="k">sealed&lt;/span> &lt;span class="k">class&lt;/span> &lt;span class="nc">DisplayClass&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">Action&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Wrapping C# class for use in COM</title><link>https://aakinshin.net/posts/wrap-cs-in-com/</link><pubDate>Mon, 03 Jun 2013 00:00:00 +0000</pubDate><guid>https://aakinshin.net/posts/wrap-cs-in-com/</guid><description>&lt;p>Let us have a C# class that makes something useful, for example:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-cs" data-lang="cs">&lt;span class="k">public&lt;/span> &lt;span class="k">class&lt;/span> &lt;span class="nc">Calculator&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">Sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="p">+&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let’s create a &lt;a href="http://ru.wikipedia.org/wiki/Component_Object_Model">COM&lt;/a> interface for this class to make it possible to use its functionality in other areas. At the end we will see how this class is used in Delphi environment.&lt;/p></description></item></channel></rss>