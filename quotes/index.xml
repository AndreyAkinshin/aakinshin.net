<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quotes on Andrey Akinshin</title><link>https://aakinshin.net/quotes/</link><description>Recent content in Quotes on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://aakinshin.net/quotes/index.xml" rel="self" type="application/rss+xml"/><item><title>A cumulative evidence base in psychology</title><link>https://aakinshin.net/library/quotes/7a99a4bd-15fa-4520-9bf6-dacdea581298/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/7a99a4bd-15fa-4520-9bf6-dacdea581298/</guid><description>Scrutiny of the details of previous work’s measures is necessary to both inform how we should interpret existing findings and to increase measures’ future reuse potential. Transparency about the fine grain details of our measures allows others to reuse them with fidelity, and allows for the fidelity of measures to be checked between studies. These aspects of transparency and their scientific benefits have yet to be tapped by our field. If we want to build a cumulative evidence base in psychology, we need to standardise our measures and protocols. Psychologists need to stop remixing and recycling, and start reusing (measures, not toothbrushes).</description></item><item><title>A jingle-jangle of labels</title><link>https://aakinshin.net/library/quotes/9b23bf30-c334-4f03-b399-a336fe8fe1cb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9b23bf30-c334-4f03-b399-a336fe8fe1cb/</guid><description>A jingle-jangle of labels
Some measures actually quantify different things, but share similar labels (or even identical ones: In APA PsycTests, no less than 19 different tests go by “theory of planned behavior ques- tionnaire”, 15 by “job satisfaction scale”, and 11 by “self-efficacy scale”). Other measures quantify the same thing as existing measures but under a different label. Known as the Jingle and Jangle fallacies, these are common and well-documented threats to the replicability and validity of psychological research, e.g. in studies on emotion . They involve a nominal fallacy: that a measure’s name tells you about its contents or what it measures.
Undisclosed flexibility
Even when authors profess using the same measure of the same construct, all is not yet well because disclosed and undisclosed measurement flexibility, i.e. changes to a measure with known or unknown psychometric consequences, is common. Dropping, adding, and altering items in self-report scales, aggregating total scores in various ways in laboratory tasks, or varying stimuli and trial durations all occur while researchers not only refer to the same construct, but actually to the same nominal instrument. Even when all decisions are disclosed, only a methodological literature review will reveal that many studies used, for instance, unique aggregation algorithms, scoring strategies, or items, often with unknown psychometric consequences.</description></item><item><title>Addressing False Positives in Self-Report Surveys</title><link>https://aakinshin.net/library/quotes/6c23bac3-f06b-4234-8b0d-eb4c29388360/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/6c23bac3-f06b-4234-8b0d-eb4c29388360/</guid><description>Self-report surveys of rare events easily lead to huge overestimates of the true incidence of such events, particularly if the event in question has some potential social desirability. Researchers who claim that such survey incidence data are accurate must show how they have eliminated the enormous problem of false positives.</description></item><item><title>Addressing Pseudoreplication in Experimental Studies</title><link>https://aakinshin.net/library/quotes/4c2a9774-48bc-42db-a035-c416e710ed01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/4c2a9774-48bc-42db-a035-c416e710ed01/</guid><description>We believe that good investigators who are aware of the issue will find that eliminating pseudoreplication from their experiments is a relatively straightforward matter. The problems of pseudoreplication that we typically encounter in published studies are easily solvable, and we see no reason that reviewers and editors should accept studies that fail to eliminate pseudoreplication. We certainly do not believe that absence of pseudoreplication should be the sole, or even the main, criterion for evaluating experiments. Rather, we see the absence of pseudoreplication as representing a minimum requirement that should be met before the merits of an experiment are evaluated.</description></item><item><title>An enormous threat</title><link>https://aakinshin.net/library/quotes/d5b1dc5f-2f56-41fa-ac2a-a4bcd86e4aa4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/d5b1dc5f-2f56-41fa-ac2a-a4bcd86e4aa4/</guid><description>Hence, (a) the lack of strong empirical or procedural norms in measurement, (b) the lack of transparency in reporting, and (c) the lack of common referents (i.e., test norms) in measurement are an enormous threat to meaningful evidence cumulation and research synthesis.</description></item><item><title>Analysis of Variance in Genetic Associations and Health Care Interventions</title><link>https://aakinshin.net/library/quotes/c84cec6e-c9dd-4ea2-b87f-516d09824fbf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c84cec6e-c9dd-4ea2-b87f-516d09824fbf/</guid><description>The maximal between-study variance was more likely to be recorded early in the 44 eligible meta-analyses of genetic associations than in the 37 meta-analyses of health care interventions (P = .013). At the time of the first heterogeneity assessment, the most favorable-ever result in support of a specific association was more likely to appear than the least favorable-ever result (22 vs. 10, P = .017); the opposite was seen at the second heterogeneity assessment (15 vs. 5, P = .031). Such a sequence of extreme opposite results was not seen in the clinical trials meta-analyses. The estimated between-study variance decreased over time in genetic association studies (P = .010), but not in clinical trials (P = .30).</description></item><item><title>Analytically calculating power can be difﬁcult or downright impossible</title><link>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</guid><description>Math is another possible explanation for why power calculations are so uncommon: analytically calculating power can be difﬁcult or downright impossible. Techniques for calculating power are not frequently taught in intro statistics courses. And some commercially available statistical software does not come with power calculation functions. It is possible to avoid hairy mathematics by simply simulating thousands of artiﬁcial datasets with the effect size you expect and running your statistical tests on the simulated data. The power is simply the fraction of datasets for which you obtain a statistically signiﬁcant result. But this approach requires programming experience, and simulating realistic data can be tricky.</description></item><item><title>Balancing Realism and Purity</title><link>https://aakinshin.net/library/quotes/c940f25b-3f02-40ea-8f2c-8934efe03226/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c940f25b-3f02-40ea-8f2c-8934efe03226/</guid><description>It is difficult to give an objective assessment of the validity of a statistical analysis. Rather like medicine, statistics is an art, and there is rarely a unique correct approach. Rather judgement is necessary to select one of a number of possible analyses, each with their own advantages and limitations. In my review I encountered many analyses which I would have tackled differently, but where in my judgement the analysis as presented was perfectly acceptable. In writing this article I have attempted to confine my criticisms to points where I believe that the vast majority of statisticians would agree that the approach adopted was not acceptable, and indeed many statisticians would probably take a harder line than I have. My statistical philosophy leans towards being a realist rather than a purist, and my research interests lie in the area of how to obtain the least biased results possible in areas where perhaps for ethical or practical reasons randomization is not possible, or where missing data abound.</description></item><item><title>Black and white medical conclusions</title><link>https://aakinshin.net/library/quotes/61d2c118-762a-4f38-8710-231759813a08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/61d2c118-762a-4f38-8710-231759813a08/</guid><description>It appears that medical authors feel the need to make black and white conclusions when their data almost never allows for such dichotomous statements. This is particularly true when comparing results to similar studies with largely overlapping CIs. Virtually all of the conclusion confusion discussed in this paper can be linked to slavish adherence to an arbitrary threshold for statistical significance. Even if the threshold is reasonable, it still cannot be used to make dichotomous conclusions.</description></item><item><title>Challenges and Pitfalls in Spatial Data Mapping</title><link>https://aakinshin.net/library/quotes/454084e3-99c3-415b-8a06-25a8b5318e21/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/454084e3-99c3-415b-8a06-25a8b5318e21/</guid><description>Mapping raw data can lead to spurious spatial features. For example, regions can appear highly variable because of small sample sizes in spatial sub-units (as in the radon example) or small populations (as in the cancer example), and these apparently variable regions contain a disproportionate number of very high (or low) observed parameter values. Mapping posterior means leads to the reverse problems: areas that appear too uniform because of small sample sizes or populations.</description></item><item><title>Conclusions on journal impact factor</title><link>https://aakinshin.net/library/quotes/2526af27-e9d4-4a82-a5d3-e95b8af58146/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/2526af27-e9d4-4a82-a5d3-e95b8af58146/</guid><description> While at this point it seems impossible to quantify the relative contributions of the different factors influencing the reliability of scientific publications, the current empirical literature on the effects of journal rank provides evidence supporting the following four conclusions:
Journal rank is a weak to moderate predictor of utility and perceived importance; Journal rank is a moderate to strong predictor of both intentional and unintentional scientific unreliability Journal rank is expensive, delays science and frustrates researchers; Journal rank as established by IF violates even the most basic scientific standards, but predicts subjective judgments of journal quality.</description></item><item><title>Confidence Intervals</title><link>https://aakinshin.net/library/quotes/8a403d69-3240-4581-9710-cde46af21603/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8a403d69-3240-4581-9710-cde46af21603/</guid><description>We have argued that the excessive use of hypothesis testing at the expense of more informative approaches to data interpretation is an unsatisfactory way of assessing and presenting statistical findings from medical studies. We prefer the use of confidence intervals, which present the results directly on the scale of data measurement. We have also suggested a notation for confidence intervals which is intended to force clarity of meaning. Confidence intervals, which also have a link to the outcome of hypothesis tests, should become the standard method for presenting the statistical results of major findings.</description></item><item><title>Convenient p=0.05</title><link>https://aakinshin.net/library/quotes/9306030d-dc0f-42cd-ae4f-8c77d9dbd656/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9306030d-dc0f-42cd-ae4f-8c77d9dbd656/</guid><description>The value for which P = 0.05, or 1 in 20, is 1'96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant.</description></item><item><title>Correspondence between confidence intervals and hypothesis tests</title><link>https://aakinshin.net/library/quotes/426c7b3f-1b39-443c-9f6f-25dfaa973005/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/426c7b3f-1b39-443c-9f6f-25dfaa973005/</guid><description>Although for quantitative data and means there is a direct correspondence between the confidence interval approach and a t test ofthe null hypothesis at the associated level of statistical significance, this is not exactly so for qualitative data and proportions. The reason is related to the use of different estimates of the standard error for the usual tests of the null hypothesis from those given here for constructing confidence intervals. The lack of direct correspondence is small and should not result in changes of interpretation. In addition, more accurate confidence intervals can sometimes be obtained by using estimates of the standard error of the sample statistic at the confidence limits themselves-such as derived by Cornfield for relative risks.</description></item><item><title>Counterintuitiveness of significance testing</title><link>https://aakinshin.net/library/quotes/87d5d946-bcbf-4c7d-921f-d7d2f680a5d4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/87d5d946-bcbf-4c7d-921f-d7d2f680a5d4/</guid><description>This is a counterintuitive feature of significance testing: if you want to prove that your drug works, you do so by showing the data is inconsistent with the drug not working.</description></item><item><title>Critique of McClintock's Study of Human Menstrual Cycles</title><link>https://aakinshin.net/library/quotes/79121729-78ab-4ef3-a3da-409b3596273c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/79121729-78ab-4ef3-a3da-409b3596273c/</guid><description>McClintock’s study of human menstrual cycles went something like this:
Find groups of women who live together in close contactfor instance, college students in dormitories. Every month or so, ask each woman when her last menstrual period began and to list the other women with whom she spent the most time. Use these lists to split the women into groups that tend to spend time together. For each group of women, see how far the average woman’s period start date deviates from the average. Small deviations would mean the women’s cycles were aligned, all starting at around the same time. Then the researchers tested whether the deviations decreased over time, which would indicate that the women were synchronizing. To do this, they checked the mean deviation at ﬁve different points throughout the study, testing whether the deviation decreased more than could be expected by chance.
Unfortunately, the statistical test they used assumed that if there was no synchronization, the deviations would randomly increase and decrease from one period to another. But imagine two women in the study who start with aligned cycles. One has an average gap of 28 days between periods and the other a gap of roughly 30 days. Their cycles will diverge consistently over the course of the study, starting two days apart, then four days, and so on, with only a bit of random variation because periods are not perfectly timed. Similarly, two women can start the study not aligned but gradually align.
For comparison, if you’ve ever been stuck in traffic, you’ve probably seen how two turn signals blinking at different rates will gradually synchronize and then go out of phase again. If you’re stuck at the intersection long enough, you’ll see this happen multiple times. But to the best of my knowledge, there are no turn signal pheromones.</description></item><item><title>Decline of power in psychology (1962..1984)</title><link>https://aakinshin.net/library/quotes/9eaab253-ea71-4eb4-add2-fc061c448725/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9eaab253-ea71-4eb4-add2-fc061c448725/</guid><description>The long-term impact of studies of statistical power is investigated using J. Cohen&amp;rsquo;s (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases.</description></item><item><title>Dichotomization of 2 continuous independent variables</title><link>https://aakinshin.net/library/quotes/ac013725-23bd-4d83-b98c-4b791a382e70/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ac013725-23bd-4d83-b98c-4b791a382e70/</guid><description>Despite pleas from methodologists, researchers often continue to dichotomize continuous predictor variables. The primary argument against this practice has been that it underestimates the strength of relationships and reduces statistical power. Although this argument is correct for relationships involving a single predictor, a different problem can arise when multiple predictors are involved. Specifically, dichotomizing 2 continuous independent variables can lead to false statistical significance. As a result, the typical justification for using a median split as long as results continue to be statistically significant is invalid, because such results may in fact be spurious. Thus, researchers who dichotomize multiple continuous predictor variables not only may lose power to detect true predictor-criterion relationships in some situations but also may dramatically increase the probability of Type I errors in other situations.</description></item><item><title>Dichotomization should be avoided in most cases</title><link>https://aakinshin.net/library/quotes/8cb30d74-c4bb-470b-b52f-eab706477a22/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8cb30d74-c4bb-470b-b52f-eab706477a22/</guid><description>The knowledge of losing information from dichotomizing a continuous outcome is nothing new. However, many previous writings report on the optimal choice of cut points, which depends upon the parameters we wish to estimate. If we are lucky, the chosen cut point is near the optimal point, but the consequences of dichotomizing become more dire as we deviate from the optimal point. We focus our study on the evaluation of losses caused by dichotomization given cut points. While the analysis of dichotomized outcomes may be easier, there are no benefits to this approach when the true outcomes can be observed and the ‘working’ model is flexible enough to describe the population at hand. Thus, dichotomization should be avoided in most cases. Only when we wish to estimate a CDF value, our working model poorly approximates reality, and our sample size is large will the biasedness of model-based estimators overpower the improvement in variance. In this case, the dichotomized estimator may lead to better results, but further study-specific consideration is needed. We also want to emphasize that while analysis should be done using actual outcomes, some aspects of this analysis can be reported on a dichotomized scale.</description></item><item><title>Distributions are never normal</title><link>https://aakinshin.net/library/quotes/8c71defc-488a-4b1e-a360-f0b37f77de12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8c71defc-488a-4b1e-a360-f0b37f77de12/</guid><description>To begin, distributions are never normal. For some this seems obvious, hardly worth mentioning, but an aphorism given by Cramér (1946) and attributed to the mathematician Poincaré remains relevant: “Everyone believes in the [normal] law of errors, the experimenters because they think it is a mathematical theorem, the mathematicians because they think it is an experimental fact.”</description></item><item><title>Eager-beaver researchers</title><link>https://aakinshin.net/library/quotes/5a3b8f36-edc5-4e56-bd51-d6fe8c1e0fee/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/5a3b8f36-edc5-4e56-bd51-d6fe8c1e0fee/</guid><description>Meanwhile our eager-beaver researcher, undismayed by logic-of-science considerations and relying blissfully on the “exactitude” of modem statistical hypothesis-testing, has produced a long publication list and been promoted to a full professorship. In terms of his contribution to the enduring body of psychological knowledge, he has done hardly anything. His true position is that of a potent-but-sterile intellectual rake, who leaves in his merry path a long train of ravished maidens but no viable scientific offspring.
First, I found this quote in the paper itself, next rediscovered it in [reinhart-statistics-done-wrong].</description></item><item><title>Early stopping of RCTs</title><link>https://aakinshin.net/library/quotes/7c744647-3299-4d41-b73f-85556e2d6e91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/7c744647-3299-4d41-b73f-85556e2d6e91/</guid><description>RCTs stopped early for benefit are becoming more common, often fail to adequately report relevant information about the decision to stop early, and show implausibly large treatment effects, particularly when the number of events is small. These findings suggest clinicians should view the results of such trials with skepticism.</description></item><item><title>Efficiency loss of dichotomization</title><link>https://aakinshin.net/library/quotes/834262d6-ba81-46ea-a08a-952fafecc22c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/834262d6-ba81-46ea-a08a-952fafecc22c/</guid><description>Dichotomization is the transformation of a continuous outcome (response) to a binary outcome. This approach, while somewhat common, is harmful from the viewpoint of statistical estimation and hypothesis testing. We show that this leads to loss of information, which can be large. For normally distributed data, this loss in terms of Fisher’s information is at least $1-2/\pi$ (or 36%). In other words, 100 continuous observations are statistically equivalent to 158 dichotomized observations. The amount of information lost depends greatly on the prior choice of cut points, with the optimal cut point depending upon the unknown parameters. The loss of information leads to loss of power or conversely a sample size increase to maintain power. Only in certain cases, for instance, in estimating a value of the cumulative distribution function and when the assumed model is very different from the true model, can the use of dichotomized outcomes be considered a reasonable approach.</description></item><item><title>Embarrassignly large confidence intervals</title><link>https://aakinshin.net/library/quotes/a2e73f66-cacd-4065-94d2-b541952903ab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a2e73f66-cacd-4065-94d2-b541952903ab/</guid><description>&amp;ldquo;Everyone knows&amp;rdquo; that confidence intervals contain all the information to be found in significance tests and much more. They not only reveal the status of the trivial nil hypothesis but also about the status of non-nil null hypotheses and thus help remind researchers about the possible operation of the crud factor. Yet they are rarely to be found in the literature. I suspect that the main reason they are not reported is that they are so embarrassingly large!</description></item><item><title>Evaluating the Reliability of Highly Cited Clinical Research Studies</title><link>https://aakinshin.net/library/quotes/3862f143-7b54-472a-b201-83407f62ac6c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3862f143-7b54-472a-b201-83407f62ac6c/</guid><description>Of 49 highly cited original clinical research studies, 45 claimed that the intervention was effective. Of these, 7 (16%) were contradicted by subsequent studies, 7 others (16%) had found effects that were stronger than those of subsequent studies, 20 (44%) were replicated, and 11 (24%) remained largely unchallenged. Five of 6 highlycited nonrandomized studies had been contradicted or had found stronger effects vs 9 of 39 randomized controlled trials (P=.008). Among randomized trials, studies with contradicted or stronger effects were smaller (P=.009) than replicated or unchallenged studies although there was no statistically significant difference in their early or overall citation impact. Matched control studies did not have a significantly different share of refuted results than highly cited studies, but they included more studies with “negative” results.</description></item><item><title>Falsifiability and reality</title><link>https://aakinshin.net/library/quotes/a755755a-4bf6-4dc8-81ba-b1d76438eb62/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a755755a-4bf6-4dc8-81ba-b1d76438eb62/</guid><description>In so far as a scientific statement speaks about reality, it must be falsifiable: and in so far as it is not falsifiable, it does not speak about reality.</description></item><item><title>Five percieved probability values</title><link>https://aakinshin.net/library/quotes/68b00b34-b893-406a-963c-6775b11a9147/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/68b00b34-b893-406a-963c-6775b11a9147/</guid><description>There are only five probabilities the average human can handle: 99 percent, one percent, 100 percent, zero, and 50-50. That’s it.
Source: https://twitter.com/ProbFact/status/1559569468979908608</description></item><item><title>Guidelines for reviewers</title><link>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</guid><description>We propose the following four guidelines for reviewers.
Reviewers should ensure that authors follow the requirements. Review teams are the gatekeepers of the scientific community, and they should encourage authors not only to rule out alternative explanations, but also to more convincingly demonstrate that their findings are not due to chance alone. This means prioritizing transparency over tidiness; if a wonderful study is partially marred by a peculiar exclusion or an inconsistent condition, those imperfections should be retained. If reviewers require authors to follow these requirements, they will. Reviewers should be more tolerant of imperfections in results. One reason researchers exploit researcher degrees of freedom is the unreasonable expectation we often impose as reviewers for every data pattern to be (significantly) as predicted. Underpowered studies with perfect results are the ones that should invite extra scrutiny. Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions. Even if authors follow all of our guidelines, they will necessarily still face arbitrary decisions. For example, should they subtract the baseline measure of the dependent variable from the final result or should they use the baseline measure as a covariate? When there is no obviously correct way to answer questions like this, the reviewer should ask for alternatives. For example, reviewer reports might include questions such as, “Do the results also hold if the baseline measure is instead used as a covariate?” Similarly, reviewers should ensure that arbitrary decisions are used consistently across studies (e.g., “Do the results hold for Study 3 if gender is entered as a covariate, as was done in Study 2?”).5 If a result holds only for one arbitrary specification, then everyone involved has learned a great deal about the robustness (or lack thereof) of the effect. If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication.</description></item><item><title>How to Lie with Smoking Statistics</title><link>https://aakinshin.net/library/quotes/c7d9734a-45b7-4537-a692-93abb79a4e25/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c7d9734a-45b7-4537-a692-93abb79a4e25/</guid><description>Attempting to capitalize on Huff’s respected status, the tobacco industry commissioned him to testify before Congress and then to write a book, tentatively titled &amp;lsquo;How to Lie with Smoking Statistics&amp;rsquo;, covering the many statistical and logical errors alleged to be found in the surgeon general’s report. Huff completed a manuscript, for which he was paid more than $9,000 > (roughly $60,000 in 2014 dollars) by tobacco companies and which was positively reviewed by University of Chicago statistician (and paid tobacco industry consultant) K.A. Brownlee. Although it was never published, it’s likely that Huff’s friendly, accessible style would have made a strong impression on the public, providing talking points for watercooler arguments.
Read more: [reinhart2014].</description></item><item><title>Incorrect Statistical Procedures in Neuroscience</title><link>https://aakinshin.net/library/quotes/4e6b0a2d-9a6c-4b97-9c15-b7009ef063a5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/4e6b0a2d-9a6c-4b97-9c15-b7009ef063a5/</guid><description>In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P &amp;lt; 0.05) but the other is not (P &amp;gt; 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. An additional analysis suggests that incorrect analyses of interactions are even more common in cellular and molecular neuroscience.</description></item><item><title>Investigating the Probability of Rejecting Null Hypotheses in Abnormal-Social Research</title><link>https://aakinshin.net/library/quotes/fcecf1bc-d282-46ec-b32a-86dc4b053695/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/fcecf1bc-d282-46ec-b32a-86dc4b053695/</guid><description>The results indicate that the investigators contributing to Volume 61 of the Journal of Abnormal and Social Psychology had, on the average, a relatively (or even absolutely) poor chance of rejecting their major null hypotheses, unless the effect they sought was large. This surprising (and discouraging) finding needs some further consideration to be seen in full perspective.
First, it may be noted that with few exceptions, the 70 studies did have significant results. This may then suggest that perhaps the definitions of size of effect were too severe, or perhaps, accepting the definitions, one might seek to conclude that the investigators were operating under circumstances wherein the effects were actually large, hence their success. Perhaps, then, research in the abnormal-social area is not as &amp;ldquo;weak&amp;rdquo; as the above results suggest. But this argument rests on the implicit assumption that the research which is published is representative of the research undertaken in this area. It seems obvious that investigators are less likely to submit for publication unsuccessful than successful research, to say nothing of a similar editorial bias in accepting research for publication. Consider this paradigm: 100 investigations are undertaken in which, in fact, there is actually a medium population effect. From the above findings, about SO get positive results and are likely to come to publication; the other SO fail to reject their (assumed false) null hypotheses and are unlikely to come to publication. Thus, the general success of the articles in the volume under review does not successfully argue for their antecedent probabilities of success being any higher than the results of the analysis suggest, or, equivalently, that the criteria for size of effect used were overly stringent.&amp;quot;</description></item><item><title>It is unacceptably easy to publish “statistically significant” evidence consistent with any hypothesis</title><link>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</guid><description>Our job as scientists is to discover truths about the world. We generate hypotheses, collect data, and examine whether or not the data are consistent with those hypotheses. Although we aspire to always be accurate, errors are inevitable. Perhaps the most costly error is a false positive, the incorrect rejection of a null hypothesis. First, once they appear in the literature, false positives are particularly persistent. Because null results have many possible causes, failures to replicate previous findings are never conclusive. Furthermore, because it is uncommon for prestigious journals to publish null findings or exact replications, researchers have little incentive to even attempt them. Second, false positives waste resources: They inspire investment in fruitless research programs and can lead to ineffective policy changes. Finally, a field known for publishing false positives risks losing its credibility. In this article, we show that despite the nominal endorsement of a maximum false-positive rate of 5% (i.e., p ≤ .05), current standards for disclosing details of data collection and analyses make false positives vastly more likely. In fact, it is unacceptably easy to publish “statistically significant” evidence consistent with any hypothesis.
(Emphasis is mine.)</description></item><item><title>Joint P and E usage</title><link>https://aakinshin.net/library/quotes/80ed236b-ff1e-4214-9995-6e5f12948167/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/80ed236b-ff1e-4214-9995-6e5f12948167/</guid><description>Consider the P value and the E value jointly; if the P value is small and the E value is substantial, then a real effect is obtained.</description></item><item><title>Journal’s impact factor and overestimation of effect sizes</title><link>https://aakinshin.net/library/quotes/544f89c6-188a-488a-a13a-585caf62ef81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/544f89c6-188a-488a-a13a-585caf62ef81/</guid><description>Some evidence suggests a correlation between a journal’s impact factor (a rough measure of its prominence and importance) and the factor by which its studies overestimate effect sizes. Studies that produce less “exciting” results are closer to the truth but less interesting to a major journal editor.</description></item><item><title>Look elsewhere effect in physics</title><link>https://aakinshin.net/library/quotes/ce332bef-224c-42b5-8235-9a412e36de23/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ce332bef-224c-42b5-8235-9a412e36de23/</guid><description>When searching for a new resonance somewhere in a possible mass range, the significance of observing a local excess of events must take into account the probability of observing such an excess anywhere in the range. This is the so called “look elsewhere effect”. The effect can be quantified in terms of a trial factor, which is the ratio between the probability of observing the excess at some fixed mass point, to the probability of observing it anywhere in the range.</description></item><item><title>Low statistical power</title><link>https://aakinshin.net/library/quotes/982229dc-9c49-4800-8039-c08ca40858e3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/982229dc-9c49-4800-8039-c08ca40858e3/</guid><description>Objective. To describe the pattern over time in the level of statistical power and the reporting of sample size calculations in published randomized controlled trials (RCTs) with negative results.
Design. Ourstudy was a descriptive survey. Power to detect 25% and 50% relative differences was calculated for the subset of trials with negative results in which a simple two-group parallel design was used. Criteria were developed both to classify trial results as positive or negative and to identify the primary outcomes. Power calculations were based on results from the primary outcomes reported in the trials.
Population. We reviewed all 383 RCTs published in JAMA, Lancet, and the New England Journal of Medicine in 1975, 1980, 1985, and 1990.
Results. Twenty-sevenpercent of the 383 RCTs (n=102) were classified as having negative results. The number of published RCTs more than doubled from 1975 to 1990, with the proportion of trials with negative results remaining fairly stable. Of the simple two-group parallel design trials having negative results with dichotomous or continuous primary outcomes (n=70), only 16% and 36% had sufficient statistical power (80%) to detect a 25% or 50% relative difference, respectively. These percentages did not consistently increase overtime. Overall, only 32% of the trials with negative results reported sample size calculations, but the percentage doing so has improved over time from 0% in 1975 to 43% in 1990. Only 20 of the 102 reports made any statement related to the clinical significance of the observed differences.
Conclusions. Most trials with negative results did not have large enough sample sizes to detect a 25% or a 50% relative difference. This result has not changed over time. Few trials discussed whether the observed differences were clinically important. There are important reasons to change this practice. The reporting of statistical power and sample size also needs to be improved.</description></item><item><title>Misunderstanding of Confidence Intervals and Standard Error Bars</title><link>https://aakinshin.net/library/quotes/22a80e19-4456-4510-8a14-7415b537a7da/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/22a80e19-4456-4510-8a14-7415b537a7da/</guid><description>Little is known about researchers’ understanding of confidence intervals (CIs) and standard error (SE) bars. Authors of journal articles in psychology, behavioral neuroscience, and medicine were invited to visit a Web site where they adjusted a figure until they judged 2 means, with error bars, to be just statistically significantly different (p &amp;lt; .05). Results from 473 respondents suggest that many leading researchers have severe misconceptions about how error bars relate to statistical significance, do not adequately distinguish CIs and SE bars, and do not appreciate the importance of whether the 2 means are independent or come from a repeated measures design. Better guidelines for researchers and less ambiguous graphical conventions are needed before the advantages of CIs for research communication can be realized.</description></item><item><title>Monte Carlo methods</title><link>https://aakinshin.net/library/quotes/cdce4d2e-fb19-44fb-997f-4d1981e471a2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/cdce4d2e-fb19-44fb-997f-4d1981e471a2/</guid><description>Monte Carlo is an extremely bad method; it should only be used when all alternative methods are worse.</description></item><item><title>Neyman-Pearson vs. Fisher</title><link>https://aakinshin.net/library/quotes/e9011bc6-0eaf-4e76-8090-0331bd513617/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/e9011bc6-0eaf-4e76-8090-0331bd513617/</guid><description>It is not generally appreciated that the p value, as conceived by R. A. Fisher, is not compatible with the Neyman-Pearson hypothesis test in which it has become embedded. The p value was meant to be a flexible inferential measure, whereas the hypothesis test was a rule for behavior, not inference. The combination of the two methods has led to a reinterpretation of the p value simultaneously as an &amp;ldquo;observed error rate&amp;rdquo; and as a measure of evidence. Both of these interpretations are problematic, and their combination has obscured the important differences between Neyman and Fisher on the nature of the scientific method and inhibited our understanding of the philosophic implications of the basic methods in use today. An analysis using another method promoted by Fisher, mathematical likelihood, shows that the p value substantially overstates the evidence against the null hypothesis. Likelihood makes clearer the distinction between error rates and inferential evidence and is a quantitative tool for expressing evidential strength that is more appropriate for the purposes of epidemiology than the p value.</description></item><item><title>Neyman–Pearson vs. Fisher</title><link>https://aakinshin.net/library/quotes/510b2ea5-0e1c-4aa9-8e13-28240daf157a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/510b2ea5-0e1c-4aa9-8e13-28240daf157a/</guid><description>Confusion surrounding the reporting and interpretation of results of classical statistical tests is widespread among applied researchers, most of whom erroneously believe that such tests are prescribed by a single coherent theory of statistical inference. This is not the case: Classical statistical testing is an anonymous hybrid of the competing and frequently contradictory approaches formulated by R. A. Fisher on the one hand, and Jerzy Neyman and Egon Pearson on the other. In particular, there is a widespread failure to appreciate the incompatibility of Fisher’s evidential p value with the Type I error rate, α, of Neyman–Pearson statistical orthodoxy. The distinction between evidence (p’s) and error (α ’s) is not trivial. Instead, it reflects the fundamental differences between Fisher’s ideas on significance testing and inductive inference, and Neyman–Pearson’s views on hypothesis testing and inductive behavior. The emphasis of the article is to expose this incompatibility, but we also briefly note a possible reconciliation.</description></item><item><title>NHST</title><link>https://aakinshin.net/library/quotes/0dff54f2-c974-483a-a6b6-81eb944d4882/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/0dff54f2-c974-483a-a6b6-81eb944d4882/</guid><description>What&amp;rsquo;s wrong with NHST? Well, among many other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does!</description></item><item><title>NHST</title><link>https://aakinshin.net/library/quotes/f75b2f1f-b183-45f5-a015-f0cf16344a19/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/f75b2f1f-b183-45f5-a015-f0cf16344a19/</guid><description>We are inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis.
But we may look at the purpose of tests from another view-point. Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong.</description></item><item><title>Noise and real effects</title><link>https://aakinshin.net/library/quotes/ca6fb0ae-cfb4-406b-b47d-603f05d13cd6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ca6fb0ae-cfb4-406b-b47d-603f05d13cd6/</guid><description>A statistically insignificant difference could be nothing but noise, or it could represent a real effect that can be pinned down only with more data.</description></item><item><title>Normal subjects</title><link>https://aakinshin.net/library/quotes/ca964333-044c-4fec-8d89-145921696525/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ca964333-044c-4fec-8d89-145921696525/</guid><description>Normal or approximately normal subjects are less useful objects of research than their pathological counterparts.</description></item><item><title>Occult effects</title><link>https://aakinshin.net/library/quotes/a2c994b2-6063-4a15-b7a4-0d14f4ea5eee/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a2c994b2-6063-4a15-b7a4-0d14f4ea5eee/</guid><description>Every experimental physicist knows those surprising and inexplicable apparent ‘effects’ which in his laboratory can perhaps even be reproduced for some time, but which finally disappear without trace. Of course, no physicist would say in such a case that he had made a scientific discovery (though he might try to rearrange his experiments so as to make the effect reproducible). Indeed the scientifically significant physical effect may be defined as that which can be regularly reproduced by anyone who carries out the appropriate experiment in the way prescribed. No serious physicist would offer for publication, as a scientific discovery, any such ‘occult effect’, as I propose to call it — one for whose reproduction he could give no instructions. The ‘discovery’ would be only too soon rejected as chimerical, simply because attempts to test it would lead to negative results. (It follows that any controversy over the question whether events which are in principle unrepeatable and unique ever do occur cannot be decided by science: it would be a metaphysical controversy.)</description></item><item><title>On The Triumph of Mediocrity in Business./</title><link>https://aakinshin.net/library/quotes/44145d2a-d5dc-41ea-8376-174d942ed091/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/44145d2a-d5dc-41ea-8376-174d942ed091/</guid><description>A final, famous example dates back to 1933, when the field of mathematical statistics was in its infancy. Horace Secrist, a statistics professor at Northwestern University, published The Triumph of Mediocrity in Business, which argued that unusually successful businesses tend to become less successful and unsuccessful businesses tend to become more successful: proof that businesses trend toward mediocrity. This was not a statistical artifact, he argued, but a result of competitive market forces. Secrist supported his argument with reams of data and numerous charts and graphs and even cited some of Galton’s work in regression to the mean. Evidently, Secrist did not understand Galton’s point.</description></item><item><title>p values like mosquitos</title><link>https://aakinshin.net/library/quotes/5b71be63-c543-4d94-8c3a-aafff77ca666/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/5b71be63-c543-4d94-8c3a-aafff77ca666/</guid><description>Perhaps p values are like mosquitos. They have an evolutionary niche somewhere and no amount of scratching, swatting, or spraying will dislodge them</description></item><item><title>Post mortem examination</title><link>https://aakinshin.net/library/quotes/e361cd8b-796d-4edd-8b8a-1f5325c9801c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/e361cd8b-796d-4edd-8b8a-1f5325c9801c/</guid><description>To consult the statistician after an experiment is fnished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.</description></item><item><title>Pseudoreplication in Experimental Studies</title><link>https://aakinshin.net/library/quotes/c5d9190f-aaaa-4d7b-96f6-84186076c145/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c5d9190f-aaaa-4d7b-96f6-84186076c145/</guid><description>Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27% of them, or 48% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals.</description></item><item><title>Pseudoreplication in Neuroscience Research:</title><link>https://aakinshin.net/library/quotes/e9ebe56a-db58-4982-9d18-d15926814ecc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/e9ebe56a-db58-4982-9d18-d15926814ecc/</guid><description>Of the nineteen papers published in the August 2008 issue of Nature Neuroscience, seventeen papers (89%) used inferential statistics; of these, only three (18%) had sufficient information to assess whether there was pseudoreplication. Of these three, two appeared to have pseudoreplication. Of the fourteen papers that used inferential statistics but did not provide sufficient information, five (36%) were suspected of having pseudoreplication, but it was not possible to determine for certain.</description></item><item><title>Questionable practices</title><link>https://aakinshin.net/library/quotes/a40b9984-ae39-4f04-821b-42f8fabf5719/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a40b9984-ae39-4f04-821b-42f8fabf5719/</guid><description>Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.</description></item><item><title>Recommendations on presenting of statistical results in medical literature</title><link>https://aakinshin.net/library/quotes/10d26f39-ca23-4d67-8a99-baa4b9c0211f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/10d26f39-ca23-4d67-8a99-baa4b9c0211f/</guid><description>We encourage authors to avoid statements like “X has no effect on mortality” as they are likely to be both untrue and misleading. This is especially true as results get “close” to being statistically significant. Results should speak for themselves. For that to happen, readers (clinicians and science reporters) need to understand the language of statistics and approach authors’ conclusions with a critical eye. We are not trying to say that the reader should not review the abstract but when authors’ conclusions differ from others, readers must examine and compare the actual results. In fact, all but one of the meta-analyses provided point estimates and CIs in the abstracts. This facilitates quick comparisons to other studies reported to be “completely different,” and to determine if the CIs demonstrate clinically important differences. The problem lies in the authors’ conclusions, which often have little to do with their results but rather what they want the results to show. We encourage journal editors to challenge authors’ conclusions, particularly when they argue they have found something unique or different than other researchers but the difference is based solely on tiny variations in CIs or p-value (statistically significant or not).
We are not suggesting the elimination of statistical testing or statistical significance, but rather that all people (authors, publishers, regulators etc.) who write about medical interventions use common sense and good judgment when presenting results that differ from others and not be so beholden to the “magical” statistical significance level of 0.05. We urge them to consider the degree to which the results of the “differing” study overlap with their own, the true difference in the point estimates and range of possible effects, where the preponderance of the effect lies and how clinicians might apply the evidence.
It appears that readers of the papers discussed here would be better served by reviewing the actual results than reading the authors’ conclusions.</description></item><item><title>Requirements for authors</title><link>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</guid><description>We propose the following six requirements for authors.
Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. Following this requirement may mean reporting the outcome of power calculations or disclosing arbitrary rules, such as “we decided to collect 100 observations” or “we decided to collect as many observations as we could before the end of the semester.” The rule itself is secondary, but it must be determined ex ante and be reported. Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification. This requirement offers extra protection for the first requirement. Samples smaller than 20 per cell are simply not powerful enough to detect most effects, and so there is usually no good reason to decide in advance to collect such a small number of observations. Smaller samples, it follows, are much more likely to reflect interim data analysis and a flexible termination rule. In addition, as Figure 1 shows, larger minimum sample sizes can lessen the impact of violating Requirement 1. Authors must list all variables collected in a study. This requirement prevents researchers from reporting only a convenient subset of the many measures that were collected, allowing readers and reviewers to easily identify possible researcher degrees of freedom. Because authors are required to just list those variables rather than describe them in detail, this requirement increases the length of an article by only a few words per otherwise shrouded variable. We encourage authors to begin the list with “only,” to assure readers that the list is exhaustive (e.g., “participants reported only their age and gender”). Authors must report all experimental conditions, including failed manipulations. This requirement prevents authors from selectively choosing only to report the condition comparisons that yield results that are consistent with their hypothesis.</description></item><item><title>Risk reduction in truncated RCTs</title><link>https://aakinshin.net/library/quotes/cfcf0003-6df7-4c36-a140-1297e5f8954f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/cfcf0003-6df7-4c36-a140-1297e5f8954f/</guid><description>Nontruncated RCTs with no evidence of benefit &amp;hellip; would on average be associated with a 29% relative risk reduction in truncated RCTs addressing the same question.</description></item><item><title>Rules of smaple size planning</title><link>https://aakinshin.net/library/quotes/63edb163-442a-4336-a5c4-fc51eb0e94e7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/63edb163-442a-4336-a5c4-fc51eb0e94e7/</guid><description> Sample size planning is important to enhance cumulative knowledge in the discipline as well as for the individual researcher. Sample size planning can be based on a goal of achieving adequate statistical power, or accurate parameter estimates, or both. Researchers are actively involved in developing methods for sample size planning, especially for complex designs and analyses. Sample sizes necessary to achieve accurate parameter estimates will often be larger than sample sizes necessary to detect even a small effect. Sample sizes necessary to obtain accurate parameter estimates or power to detect small effects may often require resources prohibitive to the individual researcher, thus suggesting the desirability of study registries accompanied by meta-analytic methods.</description></item><item><title>Same Data, Different Results</title><link>https://aakinshin.net/library/quotes/6e3a83e1-2574-4f7f-a436-579ec133c468/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/6e3a83e1-2574-4f7f-a436-579ec133c468/</guid><description>Most published reports of clinical studies begin with an abstract – likely the first and perhaps only thing many clinicians, the media and patients will read. Within that abstract, authors/investigators typically provide a brief summary of the results and a 1–2 sentence conclusion. At times, the conclusion of one study will be different, even diametrically opposed, to another despite the authors looking at similar data. In these cases, readers may assume that these individual authors somehow found dramatically different results. While these reported differences may be true some of the time, radically diverse conclusions and ensuing controversies may simply be due to tiny differences in confidence intervals combined with an over-reliance and misunderstanding of a “statistically significant difference.” Unfortunately, this misunderstanding can lead to therapeutic uncertainty for front-line clinicians when in fact the overall data on a particular issue is remarkably consistent.</description></item><item><title>Scientific method</title><link>https://aakinshin.net/library/quotes/cd4d7cb3-1f80-4fa7-be94-9cb8181e7851/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/cd4d7cb3-1f80-4fa7-be94-9cb8181e7851/</guid><description>The originators of the statistical frameworks that underlie modern epidemiologic studies recognized that their methods could not be interpreted properly without an understanding of their philosophical underpinnings. Neyman held that inductive reasoning was an illusion and that the only meaningful parameters of importance in an experiment were constraints on the number of statistical &amp;ldquo;errors&amp;rdquo; we would make, defined before an experiment. Fisher rejected mechanistic approaches to inference, believing in a more flexible, inductive approach to science. One of Fisher&amp;rsquo;s developments, mathematical likelihood, fit into such an approach. The p value, which Fisher wanted used in a similar manner, invited misinterpretation because it occupied a peculiar middle ground. Because of its resemblance to the pretrial a error, it was absorbed into the hypothesis test framework. This created two illusions: that an &amp;ldquo;error rate&amp;rdquo; could be measured after an experiment and that this posttrial &amp;ldquo;error rate&amp;rdquo; could be regarded as a measure of inductive evidence. Even though Fisher, Neyman, and many others have recognized these as fallacies, their perpetuation has been encouraged by the manner in which we use the p value today. One consequence is that we overestimate the evidence for associations, particularly with p values in the range of 0.001-0.05, creating misleading impressions of their plausibility. Another result is that we minimize the importance of judgment in inference, because its role is unclear when postexperiment evidential strength is thought to be measurable with preexperiment &amp;ldquo;error-rates.&amp;rdquo; Many experienced epidemiologists have tried to correct these problems by offering guidelines about how p values should be used. We may be more effective if, in the spirts of Fisher and Neyman, we instead focus on clarifying what p values mean, and on what we mean by the &amp;ldquo;scientific method.&amp;rdquo;</description></item><item><title>Sir. Ronald Fisher</title><link>https://aakinshin.net/library/quotes/01015429-2154-45e1-b36d-9e11757643ca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/01015429-2154-45e1-b36d-9e11757643ca/</guid><description>You may say, “But, Meehl, R. A. Fisher was a genius, and we all know how valuable his stuff has been in agronomy. Why shouldn’t it work for soft psychology?” Well, I am not intimidated by Fisher’s genius, because my complaint is not in the field of mathematical statistics, and as regards inductive logic and philosophy of science, it is well-known that Sir Ronald permitted himself a great deal of dogmatism. I remember my amazement when the late Rudolf Carnap said to me, the first time I met him, “But, of course, on this subject Fisher is just mistaken: surely you must know that.” My statistician friends tell me that it is not clear just how useful the significance test has been in biological science either, but I set that aside as beyond my competence to discuss.</description></item><item><title>Spherical horse moving through a vacuum</title><link>https://aakinshin.net/library/quotes/345b1b6e-c75a-4164-9999-c9519f1a309c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/345b1b6e-c75a-4164-9999-c9519f1a309c/</guid><description>A multimillionaire offered a prize for predicting the outcome of a horse race to a stockbreeder, a geneticist, and a physicist. The stockbreeder said there were too many variables, the geneticist could not make a prediction about any particular horse, but the physicist claimed the prize, saying he could make the prediction to many decimal places—provided it were a perfectly spherical horse moving through a vacuum.</description></item><item><title>Sphygmomanometer</title><link>https://aakinshin.net/library/quotes/a2525cc5-5f08-47f3-a5af-da84d9e94d9b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a2525cc5-5f08-47f3-a5af-da84d9e94d9b/</guid><description> Or perhaps I’m worried that my sphygmomanometers are not perfectly calibrated, so I measure with a different one each day.
I just wanted an excuse to use the word sphygmomanometer.</description></item><item><title>Statistical literacy training for obstetrics-gynecology residents</title><link>https://aakinshin.net/library/quotes/58d4e333-4344-4fef-be8a-61e319433780/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/58d4e333-4344-4fef-be8a-61e319433780/</guid><description>Methods In 2011 we surveyed US obstetrics-gynecology residents participating in the Council for Resident Education in Obstetrics and Gynecology In-Training Examination about their statistical literacy and statistical literacy training.
Results Our response rate was 95% (4713 of 4961). About two-thirds (2980 of 4713) of the residents rated their statistical literacy training as adequate. Female respondents were more likely to rate their statistical literacy training poorly, with 25% (897 of 3575) indicating inadequate literacy compared with 17% (141 of 806) of the male respondents (P &amp;lt; .001). Respondents performed poorly on 2 statistical literacy questions, with only 26% (1222 of 4713) correctly answering a positive predictive value question and 42% (1989 of 4173) correctly defining a P value. A total of 51% (2391 of 4713) of respondents reported receiving statistical literacy training through a journal club, 29% (1359 of 4713) said they had informal training, 15% (711 of 4713) said that they had statistical literacy training as part of a course, and 11% (527 of 4713) said that they had no training.</description></item><item><title>Statistical misleadingness of error bars</title><link>https://aakinshin.net/library/quotes/e42a2cde-94ae-44f9-b9bf-3f2c1eee39c9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/e42a2cde-94ae-44f9-b9bf-3f2c1eee39c9/</guid><description>Climate studies often involve comparisons between estimates of some parameter derived from different observed and/or model-generated datasets. It is common practice to present estimates of two or more statistical quantities with error bars about each representing a confidence interval. If the error bars do not overlap, it is presumed that there is a statistically significant difference between them. In general, such a procedure is not valid and usually results in declaring statistical significance too infrequently. Simple examples that demonstrate the nature of this pitfall, along with some formulations, are presented. It is recommended that practitioners use standard hypothesis testing techniques that have been derived from statistical theory rather than the ad hoc approach involving error bars.</description></item><item><title>Statistical practices in high-impact journals</title><link>https://aakinshin.net/library/quotes/8120cd40-f648-47d8-ac65-45509e4262af/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8120cd40-f648-47d8-ac65-45509e4262af/</guid><description>What are the statistical practices of articles published in journals with a high impact factor? Are there differences compared with articles published in journals with a somewhat lower impact factor that have adopted editorial policies to reduce the impact of limitations of Null Hypothesis Significance Testing? To investigate these questions, the current study analyzed all articles related to psychological, neuropsychological and medical issues, published in 2011 in four journals with high impact factors: Science, Nature, The New England Journal of Medicine and The Lancet, and three journals with relatively lower impact factors: Neuropsychology, Journal of Experimental Psychology-Applied and the American Journal of Public Health. Results show that Null Hypothesis Significance Testing without any use of confidence intervals, effect size, prospective power and model estimation, is the prevalent statistical practice used in articles published in Nature, 89%, followed by articles published in Science, 42%.</description></item><item><title>Statistical reforms</title><link>https://aakinshin.net/library/quotes/efcae53d-3220-4ef9-9a3d-2d2925a667c2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/efcae53d-3220-4ef9-9a3d-2d2925a667c2/</guid><description>In recent years there have been many advocates for statistical reform, and naturally there is disagreement among them on the best method to address these problems. Some insist that p values, which I will show are frequently misleading and confusing, should be abandoned altogether; others advocate a “new statistics” based on confidence intervals. Still others suggest a switch to new Bayesian methods that give more-interpretable results, while others believe statistics as it’s currently taught is just fine but used poorly.</description></item><item><title>Tentative exploratory findings</title><link>https://aakinshin.net/library/quotes/976fa15f-840e-4f3c-a764-76f2fc7dd657/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/976fa15f-840e-4f3c-a764-76f2fc7dd657/</guid><description>But aimlessly exploring data means a lot of opportunities for false positives and truth inflation. If in your explorations you find an interesting correlation, the standard procedure is to collect a new dataset and test the hypothesis again. Testing an independent dataset will filter out false positives and leave any legitimate discoveries standing. (Of course, you’ll need to ensure your test dataset is sufficiently powered to replicate your findings.) And so exploratory findings should be considered tentative until confirmed.
If you don’t collect a new dataset or your new dataset is strongly related to the old one, truth inflation will come back to bite you in the butt.</description></item><item><title>The 3% usage of power analysis</title><link>https://aakinshin.net/library/quotes/b6157da6-8408-4226-a073-c1a0ac73fdb4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/b6157da6-8408-4226-a073-c1a0ac73fdb4/</guid><description>Over-reliance on significance testing has been heavily criticized in psychology. Therefore the American Psychological Association recommended supplementing the p value with additional elements such as effect sizes, confidence intervals, and considering statistical power seriously. This article elaborates the conclusions that can be drawn when these measures accompany the p value. An analysis of over 30 summary papers (including over 6,000 articles) reveals that, if at all, only effect sizes are reported in addition to p’s (38%). Only every 10th article provides a confidence interval and statistical power is reported in only 3% of articles.</description></item><item><title>The dipsy-doodle</title><link>https://aakinshin.net/library/quotes/e7194b0a-bece-4058-8687-cb7b6d077289/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/e7194b0a-bece-4058-8687-cb7b6d077289/</guid><description>Schunn and his colleagues were interested in how scientists deal with unexpected results, or anomalies. In one study, he videotaped two astronomers interacting over a new set of data concerning the formation of ring galaxies. Schunn found that these researchers noticed anomalies as much as expected results, but paid more attention to the anomalies. The researchers developed hypotheses about the anomalies and elaborated on them visually, whereas they used theory to elaborate on expected results. When the two astronomers discussed the anomalies, they used terms like ‘the funky thing’ and ‘the dipsy-doodle’, staying at a perceptual rather than a theoretical level. Schunn’s astronomers were working neither in the hypothesis nor experimental space; instead, they were working in a space of possible visualizations dependent on their domain-specific experience.</description></item><item><title>The first Feynman's principle</title><link>https://aakinshin.net/library/quotes/fb1f3887-c6eb-4bbe-9275-6cf0424e276f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/fb1f3887-c6eb-4bbe-9275-6cf0424e276f/</guid><description>The first principle is that you must not fool yourself — and you are the easiest person to fool.</description></item><item><title>The game of science</title><link>https://aakinshin.net/library/quotes/14ae475c-edf6-46a8-9bd4-e49f792adb71/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/14ae475c-edf6-46a8-9bd4-e49f792adb71/</guid><description>The game of science is, in principle, without end. He who decides one day that scientific statements do not call for any further test, and that they can be regarded as finally verified, retires from the game.</description></item><item><title>The illusion of attaining improbability</title><link>https://aakinshin.net/library/quotes/7a038512-b3c5-496a-b7ac-e5c900fca23b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/7a038512-b3c5-496a-b7ac-e5c900fca23b/</guid><description>One problem arises from a misapplication of deductive syllogistic reasoning. Falk and Greenbaum (in press) called this the &amp;ldquo;illusion of probabilistic proof by contradiction&amp;rdquo; or the &amp;ldquo;illusion of attaining improbability.&amp;rdquo; Gigerenzer (1993) called it the &amp;ldquo;permanent illusion&amp;rdquo; and the &amp;ldquo;Bayesian Id&amp;rsquo;s wishful thinking,&amp;rdquo; part of the &amp;ldquo;hybrid logic&amp;rdquo; of contemporary statistical inference—a mishmash of Fisher and Neyman-Pearson, with invalid Bayesian interpretation.</description></item><item><title>The Importance of Effect Magnitude Over Statistical Hypotheses</title><link>https://aakinshin.net/library/quotes/c6000a61-924a-47b7-805c-2fc024a131bb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c6000a61-924a-47b7-805c-2fc024a131bb/</guid><description>Instead of asking “how many more crashes?” and the authors chose to ask “are we sufficiently sure that the effect was not zero?” This substitution of questions led to all the subsequent entanglements. These can all be avoided by not testing statistical hypotheses when the research question is about the effect of some treatment; by returning to common sense and the mainstream of science and providing estimates of effect magnitude and its standard error instead.</description></item><item><title>The importance of the critical attitude</title><link>https://aakinshin.net/library/quotes/b9b10859-3a7a-49d0-9a3a-cfb9d9a16c3c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/b9b10859-3a7a-49d0-9a3a-cfb9d9a16c3c/</guid><description>I have italicized the words ‘rational discussion’ and ‘critically’ in order to stress that I equate the rational attitude and the critical attitude. The point is that, whenever we propose a solution to a problem, we ought to try as hard as we can to overthrow our solution, rather than defend it. Few of us, unfortunately, practise this precept; but other people, fortunately, will supply the criticism for us if we fail to supply it ourselves. Yet criticism will be fruitful only if we state our problem as clearly as we can and put our solution in a sufficiently definite form — a form in which it can be critically discussed.</description></item><item><title>The inappropriateness of significance tests</title><link>https://aakinshin.net/library/quotes/441a516a-9f37-46bc-afb1-fd6165503b80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/441a516a-9f37-46bc-afb1-fd6165503b80/</guid><description>All references to statistical hypothesis testing and statistical significance should be removed from the paper. I ask that you delete p values as well as comments about statistical significance. If you do not agree with my standards (concerning the inappropriateness of significance tests), you should feel free to argue the point, or simply ignore what you may consider to be my misguided view, by publishing elsewher.</description></item><item><title>The Influence of Journal Prestige on the Inflation of Effect Sizes in Small Trials</title><link>https://aakinshin.net/library/quotes/79991435-cde7-4963-8f62-2bf812a39fda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/79991435-cde7-4963-8f62-2bf812a39fda/</guid><description>We found that small trials published in NEJM, JAMA and Lancet were more likely to display more favourable results for experimental interventions compared with trials in other publication venues. Inflated effect sizes were seen primarily for early small trials in these prominent journals. Therefore, the results of small trials with spectacular early promises for large treatment effects should be seen with great caution. Conversely, for large trials, effect estimates are likely to be more reliable. Small-study effects have been previously documented in the randomized trials literature. However, the results of our study provide further insight suggesting the possibility of a specific interaction with further exaggerated effects when the limited evidence from small trials appears in the most prestigious journals. Also, the inflation of effects in early randomized trials on particular interventions appears to be quite specific to these most prestigious journals. Some modest heterogeneity was seen in the two tertiles with higher events, but heterogeneity is difficult to determine in the tertile with lower events, because of the wide uncertainty in the ROR for single topics, when there is limited evidence.</description></item><item><title>The neuroscience of low statistical power</title><link>https://aakinshin.net/library/quotes/76c951e9-c936-4b30-919f-d3d3fd7f7227/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/76c951e9-c936-4b30-919f-d3d3fd7f7227/</guid><description>A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.</description></item><item><title>The null field</title><link>https://aakinshin.net/library/quotes/f05e3cd0-fb35-4654-b121-421c58c5f5e4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/f05e3cd0-fb35-4654-b121-421c58c5f5e4/</guid><description>Let us suppose that in a research field there are no true findings at all to be discovered. History of science teaches us that scientific endeavor has often in the past wasted effort in fields with absolutely no yield of true scientific information, at least based on our current understanding. In such a “null field,” one would ideally expect all observed effect sizes to vary by chance around the null in the absence of bias. The extent that observed findings deviate from what is expected by chance alone would be simply a pure measure of the prevailing bias.</description></item><item><title>The null hypothesis is always false</title><link>https://aakinshin.net/library/quotes/9d1c1cff-0bdb-4fd7-b146-da704a7caa82/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9d1c1cff-0bdb-4fd7-b146-da704a7caa82/</guid><description>I believe is generally recognized by statisticians today and by thoughtful social scientists, the null hypothesis, taken literally, is always false.</description></item><item><title>The null hypothesis ritual</title><link>https://aakinshin.net/library/quotes/af7c74d8-e8ac-4736-958a-ef3bd67c4385/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/af7c74d8-e8ac-4736-958a-ef3bd67c4385/</guid><description>After 4 decades of severe criticism, the ritual of null hypothesis significance testing — mechanical dichotomous decisions around a sacred .05 criterion — still persists.</description></item><item><title>The overlap method</title><link>https://aakinshin.net/library/quotes/eb3039e9-e45a-411d-ac91-4885d0049c0b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/eb3039e9-e45a-411d-ac91-4885d0049c0b/</guid><description>The overlap method is simple, and it is convenient when lists or graphs of confidence intervals are presented. It can be useful as a quick and relatively rough method for exploratory data analysis. It should not be regarded as an optimal method for significance testing, however, given its conservatism and low power relative to the standard method in the common situation that we have considered. Thus, the overlap method should not be used for formal significance testing unless the data analyst is aware of its deficiencies and unless the information needed to carry out a more appropriate procedure is unavailable.</description></item><item><title>The P value fallacy</title><link>https://aakinshin.net/library/quotes/aeb58f62-484b-49a5-853c-8f72e49c82e3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/aeb58f62-484b-49a5-853c-8f72e49c82e3/</guid><description>An important problem exists in the interpretation of modern medical research data: Biological understanding and previous research play little formal role in the interpretation of quantitative results. This phenomenon is manifest in the discussion sections of research articles and ultimately can affect the reliability of conclusions. The standard statistical approach has created this situation by promoting the illusion that conclusions can be produced with certain “error rates,” without consideration of information from outside the experiment. This statistical approach, the key components of which are P values and hypothesis tests, is widely perceived as a mathematically coherent approach to inference. There is little appreciation in the medical community that the methodology is an amalgam of incompatible elements, whose utility for scientific inference has been the subject of intense debate among statisticians for almost 70 years. This article introduces some of the key elements of that debate and traces the appeal and adverse impact of this methodology to the P value fallacy, the mistaken idea that a single number can capture both the long-run outcomes of an experiment and the evidential meaning of a single result. This argument is made as a prelude to the suggestion that another measure of evidence should be used—the Bayes factor, which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings.</description></item><item><title>The riddle of the world</title><link>https://aakinshin.net/library/quotes/3e2b099c-520a-40a2-b99b-f11d4ddad259/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3e2b099c-520a-40a2-b99b-f11d4ddad259/</guid><description>For myself, I am interested in science and in philosophy only because I want to learn something about the riddle of the world in which we live, and the riddle of man’s knowledge of that world. And I believe that only a revival of interest in these riddles can save the sciences and philosophy from narrow specialization and from an obscurantist faith in the expert’s special skill, and in his personal knowledge and authority; a faith that so well fits our ‘post-rationalist’ and ‘post-critical’ age, proudly dedicated to the destruction of the tradition of rational philosophy, and of rational thought itself.</description></item><item><title>The statistical accessibility of medical papers</title><link>https://aakinshin.net/library/quotes/cca63eb6-34c7-451e-baaf-1c35e7c1d7d8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/cca63eb6-34c7-451e-baaf-1c35e7c1d7d8/</guid><description>Similarly, if a reader had knowledge of t-tests, contingency tables, nonparametric tests, epidemiologic statistics, Pearson’s correlation, simple linear regression, analysis of variance, transformations, and nonparametric correlation (topics typically included in introductory statistics courses), then 21 percent of the articles would be accessible.</description></item><item><title>The toothbrush problem</title><link>https://aakinshin.net/library/quotes/3e2e8840-784d-41ed-8c01-5a880bd614c2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3e2e8840-784d-41ed-8c01-5a880bd614c2/</guid><description>Taxonomies and frameworks are like toothbrushes — no one wants to use anyone else’s.
The attribution of this quotation is ambiguous. A nice historical overview of the quote can be found in A useless history of the “Frameworks are Like Toothbrushes” quote. While [Spherical Horses and Shared Toothbrushes: Lessons Learned from a Workshop on Scientific and Technological Thinking] is one of the first appearances of the quote, this metaphor was especially popularized in The Toothbrush Problem by Walter Mischel. Occasionally, it is used in other works (e.g., see [Psychological Measures Aren’t Toothbrushes]).</description></item><item><title>The wrong question</title><link>https://aakinshin.net/library/quotes/7566960c-ae77-4add-9c00-b9865d8f1cac/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/7566960c-ae77-4add-9c00-b9865d8f1cac/</guid><description>Statisticians classically asked the wrong question — and were willing to answer with a lie, one that was often a downright lie. They asked &amp;ldquo;Are the effects of A and B different?&amp;rdquo; and they were willing to answer &amp;ldquo;no.&amp;rdquo;
All we know about the world teaches us that the effects of A and B are always difference — in some decimal place — for any A and B. Thus asking &amp;ldquo;Are the effects different?&amp;rdquo; is foolish.
What we should be answering first is &amp;ldquo;Can we tell the direction in which the effects of A differ from the effects of B?&amp;rdquo; In other words, can we be confident about the direction from A to B? Is it &amp;ldquo;up,&amp;rdquo; &amp;ldquo;down&amp;rdquo; or &amp;ldquo;uncertain&amp;rdquo;?
The third answer to this first question is that we are &amp;ldquo;uncertain about the direction&amp;rdquo; — it is not, and never should be, that we &amp;ldquo;accept the null hypothesis.&amp;rdquo;</description></item><item><title>Three dangerous statistical equations</title><link>https://aakinshin.net/library/quotes/9d9f8cca-5477-4108-9b60-6520cd897505/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9d9f8cca-5477-4108-9b60-6520cd897505/</guid><description>Supporting ignorance is not, however, the direction I wish to pursue—indeed it is quite the antithesis of my message. Instead I am interested in equations that unleash their danger not when we know about them, but rather when we do not. Kept close at hand, these equations allow us to understand things clearly, but their absence leaves us dangerously ignorant.
There are many plausible candidates, and I have identified three prime examples: Kelley&amp;rsquo;s equation, which indicates that the truth is estimated best when its observed value is regressed toward the mean of the group that it came from; the standard linear regression equation; and the equation that provides us with the standard deviation of the sampling distribution of the mean—what might be called de Moivre&amp;rsquo;s equation: $\sigma_{\bar{x}} = \sigma/\sqrt{n}$, where $\sigma_{\bar{x}}$ is the standard error of the mean, $s$ is the standard deviation of the sample and $n$ is the size of the sample. (Note the square root symbol, which will be a key to at least one of the misunderstandings of variation.) De Moivre&amp;rsquo;s equation was derived by the French mathematician Abraham de Moivre, who described it in his 1730 exploration of the binomial distribution, Miscellanea Analytica.</description></item><item><title>Truth inflation</title><link>https://aakinshin.net/library/quotes/48424c6e-3e04-4281-87fd-2b2ca1ef057d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/48424c6e-3e04-4281-87fd-2b2ca1ef057d/</guid><description>This effect, known as truth inflation, type M error (M for magnitude), or the winner’s curse, occurs in fields where many researchers conduct similar experiments and compete to publish the most “exciting” results: pharmacological trials, epidemiological studies, gene association studies (“gene A causes condition B”), and psychological studies often show symptoms, along with some of the most-cited papers in the medical literature.</description></item><item><title>Truth Inflation and groundbreaking effect sizes in top-ranked journals</title><link>https://aakinshin.net/library/quotes/3107d5e7-d81b-48f9-926e-591261332ef5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3107d5e7-d81b-48f9-926e-591261332ef5/</guid><description>Consider also that top-ranked journals, such as Nature and Science, prefer to publish studies with groundbreaking results—meaning large effect sizes in novel fields with little prior research. This is a perfect combination for chronic truth inflation. Some evidence suggests a correlation between a journal’s impact factor (a rough measure of its prominence and importance) and the factor by which its studies overestimate effect sizes. Studies that produce less “exciting” results are closer to the truth but less interesting to a major journal editor. [brembs2013] [siontis2011]</description></item><item><title>Twelve P-Value Misconceptions</title><link>https://aakinshin.net/library/quotes/ac262184-cdb6-46f8-b7ac-5107d66d8314/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ac262184-cdb6-46f8-b7ac-5107d66d8314/</guid><description> Twelve P-Value Misconceptions:
If P =.05, the null hypothesis has only a 5% chance of being true. A nonsignificant difference (eg, P &amp;gt; .05) means there is no difference between groups. A statistically significant finding is clinically important. Studies with P values on opposite sides of .05 are conflicting. Studies with the same P value provide the same evidence against the null hypothesis. P = .05 means that we have observed data that would occur only 5% of the time under the null hypothesis. P = .05 and P &amp;lt;= .05 mean the same thing. P values are properly written as inequalities (eg, “P &amp;lt; .02” when P = .015) P = .05 means that if you reject the null hypothesis, the probability of a type I error is only 5%. With a P = .05 threshold for significance, the chance of a type I error will be 5%. You should use a one-sided P value when you don’t care about a result in one direction, or a difference in that direction is impossible. A scientific conclusion or treatment policy should be based on whether or not the P value is significant.</description></item><item><title>Type II error should be as essential a requirement for publication</title><link>https://aakinshin.net/library/quotes/c7ba1497-c183-41ab-9d9e-a4cffb4d2c84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c7ba1497-c183-41ab-9d9e-a4cffb4d2c84/</guid><description>Failure to achieve statistical significance between interventions does not prove the absence of any difference. Proper planning of a clinical trial with attention to beta error and sample size determination allows the critical investigator to acknowledge the probability of a type II error and therefore the probability of detecting a clinically meaningful difference if one exists. The reader and clinician must be aware that negative trials may in fact be falsely negative, and should look for specific reporting of alpha, beta, and $\Delta_c$ error to provide the details of the reliability of such conclusions. Type II error should be as essential a requirement for publication, and as rigorously analyzed, as the traditional and far more common type I (alpha) error.</description></item><item><title>Type M and Type S Errors</title><link>https://aakinshin.net/library/quotes/8359ad42-cd60-4f97-b32a-1f81df61f486/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/8359ad42-cd60-4f97-b32a-1f81df61f486/</guid><description>This is a Type M (magnitude) error (Gelman and Tuerlinckx, 2000): the study is constructed in such a way that any statistically-significant finding will almost certainly be a huge overestimate of the true effect. In addition there will be Type S (sign) errors, in which the estimate will be in the opposite direction as the true effect.</description></item><item><title>Underpowered negative clinical trials</title><link>https://aakinshin.net/library/quotes/10189519-eb48-41f5-a1ca-adac8092bf8e/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/10189519-eb48-41f5-a1ca-adac8092bf8e/</guid><description>Our survey of 423 negative clinical trials indicates that 55% of trials had too few participants to detect a medium effect size in favor of the experimental over the standard treatment arm for their primary end point with at least 80% statistical power. Although underpowered negative clinical trials have been widely reported in the general medical and subspecialty literature, there are few reports relating to trials evaluating treatment of cancer. A review of 22 negative randomized oncology trials published in major general medical or oncology journals during a 1-year period found that 16 trials (73%) lacked adequate statistical power to detect a 50% improvement in median survival in favor of the experimental arm.</description></item><item><title>Unimpressive theories in psychology</title><link>https://aakinshin.net/library/quotes/a877a820-1fcb-4944-99ad-5f888320ea8d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a877a820-1fcb-4944-99ad-5f888320ea8d/</guid><description>I consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless.</description></item><item><title>White swans and universal statements</title><link>https://aakinshin.net/library/quotes/a6d7b386-604a-4b5d-afaf-cc570da1ec87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a6d7b386-604a-4b5d-afaf-cc570da1ec87/</guid><description>Now it is far from obvious, from a logical point of view, that we are justified in inferring universal statements from singular ones, no matter how numerous; for any conclusion drawn in this way may always turn out to be false: no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white.</description></item><item><title>Your eyeball</title><link>https://aakinshin.net/library/quotes/36f7123e-0f36-4dc6-9699-12bcf8a85455/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/36f7123e-0f36-4dc6-9699-12bcf8a85455/</guid><description>Your eyeball is not a well-deﬁned statistical procedure.</description></item></channel></rss>