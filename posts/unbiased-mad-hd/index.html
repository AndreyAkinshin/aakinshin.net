<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.103.0-DEV"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content="Mathematics,Statistics,Research,Quantile,MAD,Harrell-Davis quantile estimator,Research: Unbiased median absolute deviation"><title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator | Andrey Akinshin</title><meta name=description content="The finite-sample bias-correction factors for the median absolute deviation which make it a consistent estimator for the standard deviation (improved versi..."><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=https://aakinshin.net/sass/bootstrap-light.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=https://aakinshin.net/sass/bootstrap-dark.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.d7f6578e23ad3f0c33c589f9bcb34f995b45121f1f3ce888869c86b26826c845.js></script>
<script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.9e68d431432744c07a43381a8a833ffe0921cc0e006d84ad97f0b0d43c5cd82d.mjs></script>
<link href=https://aakinshin.net/css/fontawesome-all.min.178e95bba6ea2e9a90838cd646658f4bf6667a6ceaa057cb587bf7e6eb8412d3.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.60773ab7266ecc6e9625306f57c0a82a219b011dc3ea2d83763d1ecdf11c86d2.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.7e186db80d8c431d196b3e0718afb0636b82a269a8c92521c11a55267a24fc27.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZVB6MXSX32")</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-41419012-5"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-41419012-5")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(28700916,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-expanded=false>Blog</a><ul class="dropdown-menu bg-primary"><li><a class=dropdown-item href=https://aakinshin.net/posts/>All Posts</a></li><li><a class=dropdown-item href=https://aakinshin.net/tags/statistics/>Posts about Statistics</a></li><li></li><a class=dropdown-item href=https://aakinshin.net/tags/>All Tags</a></li></ul></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li></ul><ul class="navbar-nav ms-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class="blog-post table-sm"><h1 class=blog-post-title id=post-title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</h1><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-16>February 16, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a>
<a href=https://aakinshin.net/tags/research-unbiased-mad/ class="badge badge-info">Research: Unbiased median absolute deviation</a></span><br><br><strong>Update: this blog post is a part of research that aimed to build an unbiased median absolute deviation estimator based on various quantile estimators. A <a href=/posts/preprint-mad-factors/>preprint with final results</a> is available on arXiv: <a href=https://arxiv.org/abs/2207.12005>arXiv:2207.12005 [stat.ME]</a>. Some information in this blog post can be obsolete: please, use the preprint as the primary reference.</strong><br><br><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
In the previous post, I <a href=https://aakinshin.net/posts/unbiased-mad/>showed</a>
how to use the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>unbiased</a>
version of the <span class="math inline">\(\textrm{MAD}\)</span> estimator
as a robust alternative to the standard deviation.
&ldquo;Unbiasedness&rdquo; means that such estimator&rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the <a href=https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>biasâ€“variance tradeoff</a>:
when we remove the bias of the <span class="math inline">\(\textrm{MAD}\)</span> estimator,
we increase its variance and mean squared error (<span class="math inline">\(\textrm{MSE}\)</span>).</p><p>In this post, I want to suggest a more <a href=https://en.wikipedia.org/wiki/Efficiency_(statistics)>efficient</a>
unbiased <span class="math inline">\(\textrm{MAD}\)</span> estimator.
It&rsquo;s also a consistent estimator for the standard deviation, but it has smaller <span class="math inline">\(\textrm{MSE}\)</span>.
To build this estimator,
we should replace the classic &ldquo;straightforward&rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&rsquo;s discuss this approach in detail.</p><h3 id=introduction>Introduction</h3><p>Let&rsquo;s consider a sample <span class="math inline">\(x = \{ x_1, x_2, \ldots, x_n \}\)</span>.
Its median absolute deviation <span class="math inline">\(\textrm{MAD}_n\)</span> can be defined as follows:</p><p><span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]</span></p><p>where <span class="math inline">\(\textrm{median}\)</span> is a median estimator, <span class="math inline">\(C_n\)</span> is a scale factor (or consistency constant).</p><p>Typically, <span class="math inline">\(\textrm{median}\)</span> assumes the classic &ldquo;straightforward&rdquo; median estimator:</p><ul><li>If <span class="math inline">\(n\)</span> is odd, the median is the middle element of the sorted sample</li><li>If <span class="math inline">\(n\)</span> is even, the median is the arithmetic average of the two middle elements of the sorted sample</li></ul><p>However, we can use other median estimators.
Let&rsquo;s consider <span class="math inline">\(\textrm{median}_{\textrm{HD}}\)</span> which calculates the median using the Harrell-Davis quantile estimator (see <a href=#Harrell1982>[Harrell1982]</a>):</p><p><span class="math display">\[\textrm{median}_{\textrm{HD}}(x) = \sum_{i=1}^n W_i x_i, \quad
W_i = I_{i/n} \bigg( \frac{n+1}{2}, \frac{n+1}{2} \bigg) -
I_{(i-1)/n} \bigg( \frac{n+1}{2}, \frac{n+1}{2} \bigg)
\]</span></p><p>where <span class="math inline">\(I_t(a, b)\)</span> denotes the <a href=https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function>regularized incomplete beta function</a>.</p><p>When <span class="math inline">\(n \to \infty\)</span>, we have the exact value of <span class="math inline">\(C_n\)</span> which makes <span class="math inline">\(\textrm{MAD}_n\)</span> an unbiased estimator for the standard deviation:</p><p><span class="math display">\[C_n = C_\infty = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056
\]</span></p><p>For finite values of <span class="math inline">\(n\)</span>, we should use adjusted <span class="math inline">\(C_n\)</span> values.
We already discussed these values for the straightforward median estimator in
the <a href=https://aakinshin.net/posts/unbiased-mad/>previous post</a>
(this problem is well-covered in <a href=#Park2020>[Park2020]</a>).
For <span class="math inline">\(\textrm{median}_{\textrm{HD}}\)</span>, we should use another set of <span class="math inline">\(C_n\)</span> values.
We reuse the approach from <a href=#Hayes2014>[Hayes2014]</a> with the following notation:</p><p><span class="math display">\[C_n = \dfrac{1}{\hat{a}_n}
\]</span></p><h3 id=factors-for-small-n>Factors for small n</h3><p>Factors for the small values of <span class="math inline">\(n\)</span> can be obtained using numerical experiments.
Here is the scheme for the performed simulation:</p><ul><li>Enumerate <span class="math inline">\(n\)</span> from <span class="math inline">\(2\)</span> to <span class="math inline">\(100\)</span></li><li>For each <span class="math inline">\(n\)</span>, generate <span class="math inline">\(2\cdot 10^8\)</span> samples from the normal distribution of size <span class="math inline">\(n\)</span>
(the <a href=https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform>Boxâ€“Muller transform</a> is used, see <a href=#Box1958>[Box1958]</a>)</li><li>For each sample, estimate <span class="math inline">\(\textrm{MAD}_n\)</span> using <span class="math inline">\(C_n = 1\)</span></li><li>Calculate the arithmetic average of all <span class="math inline">\(\textrm{MAD}_n\)</span> values for the same <span class="math inline">\(n\)</span>: the result is the value of <span class="math inline">\(\hat{a}_n\)</span></li></ul><p>As the result of this simulation, I got the following table with <span class="math inline">\(\hat{a}_n\)</span> and <span class="math inline">\(C_n\)</span> values for <span class="math inline">\(n \in \{ 2..100 \}\)</span>.</p><table><thead><tr><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th><th align=right><span class="math inline">\(C_n\)</span></th><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th><th align=right><span class="math inline">\(C_n\)</span></th></tr></thead><tbody><tr><td align=right>1</td><td align=right>NA</td><td align=right>NA</td><td align=right>51</td><td align=right>0.66649</td><td align=right>1.50039</td></tr><tr><td align=right>2</td><td align=right>0.56417</td><td align=right>1.77250</td><td align=right>52</td><td align=right>0.66668</td><td align=right>1.49998</td></tr><tr><td align=right>3</td><td align=right>0.63769</td><td align=right>1.56816</td><td align=right>53</td><td align=right>0.66682</td><td align=right>1.49966</td></tr><tr><td align=right>4</td><td align=right>0.62661</td><td align=right>1.59589</td><td align=right>54</td><td align=right>0.66699</td><td align=right>1.49926</td></tr><tr><td align=right>5</td><td align=right>0.63853</td><td align=right>1.56611</td><td align=right>55</td><td align=right>0.66713</td><td align=right>1.49895</td></tr><tr><td align=right>6</td><td align=right>0.63834</td><td align=right>1.56656</td><td align=right>56</td><td align=right>0.66728</td><td align=right>1.49863</td></tr><tr><td align=right>7</td><td align=right>0.63915</td><td align=right>1.56458</td><td align=right>57</td><td align=right>0.66741</td><td align=right>1.49833</td></tr><tr><td align=right>8</td><td align=right>0.64141</td><td align=right>1.55908</td><td align=right>58</td><td align=right>0.66753</td><td align=right>1.49805</td></tr><tr><td align=right>9</td><td align=right>0.64237</td><td align=right>1.55675</td><td align=right>59</td><td align=right>0.66767</td><td align=right>1.49774</td></tr><tr><td align=right>10</td><td align=right>0.64397</td><td align=right>1.55288</td><td align=right>60</td><td align=right>0.66780</td><td align=right>1.49746</td></tr><tr><td align=right>11</td><td align=right>0.64535</td><td align=right>1.54955</td><td align=right>61</td><td align=right>0.66791</td><td align=right>1.49720</td></tr><tr><td align=right>12</td><td align=right>0.64662</td><td align=right>1.54651</td><td align=right>62</td><td align=right>0.66803</td><td align=right>1.49694</td></tr><tr><td align=right>13</td><td align=right>0.64790</td><td align=right>1.54346</td><td align=right>63</td><td align=right>0.66815</td><td align=right>1.49667</td></tr><tr><td align=right>14</td><td align=right>0.64908</td><td align=right>1.54064</td><td align=right>64</td><td align=right>0.66825</td><td align=right>1.49644</td></tr><tr><td align=right>15</td><td align=right>0.65018</td><td align=right>1.53803</td><td align=right>65</td><td align=right>0.66836</td><td align=right>1.49621</td></tr><tr><td align=right>16</td><td align=right>0.65125</td><td align=right>1.53552</td><td align=right>66</td><td align=right>0.66846</td><td align=right>1.49597</td></tr><tr><td align=right>17</td><td align=right>0.65226</td><td align=right>1.53313</td><td align=right>67</td><td align=right>0.66857</td><td align=right>1.49574</td></tr><tr><td align=right>18</td><td align=right>0.65317</td><td align=right>1.53101</td><td align=right>68</td><td align=right>0.66865</td><td align=right>1.49555</td></tr><tr><td align=right>19</td><td align=right>0.65404</td><td align=right>1.52896</td><td align=right>69</td><td align=right>0.66876</td><td align=right>1.49531</td></tr><tr><td align=right>20</td><td align=right>0.65489</td><td align=right>1.52698</td><td align=right>70</td><td align=right>0.66883</td><td align=right>1.49514</td></tr><tr><td align=right>21</td><td align=right>0.65565</td><td align=right>1.52520</td><td align=right>71</td><td align=right>0.66893</td><td align=right>1.49493</td></tr><tr><td align=right>22</td><td align=right>0.65638</td><td align=right>1.52351</td><td align=right>72</td><td align=right>0.66901</td><td align=right>1.49475</td></tr><tr><td align=right>23</td><td align=right>0.65708</td><td align=right>1.52190</td><td align=right>73</td><td align=right>0.66910</td><td align=right>1.49456</td></tr><tr><td align=right>24</td><td align=right>0.65771</td><td align=right>1.52043</td><td align=right>74</td><td align=right>0.66918</td><td align=right>1.49437</td></tr><tr><td align=right>25</td><td align=right>0.65832</td><td align=right>1.51902</td><td align=right>75</td><td align=right>0.66925</td><td align=right>1.49422</td></tr><tr><td align=right>26</td><td align=right>0.65888</td><td align=right>1.51772</td><td align=right>76</td><td align=right>0.66933</td><td align=right>1.49402</td></tr><tr><td align=right>27</td><td align=right>0.65943</td><td align=right>1.51647</td><td align=right>77</td><td align=right>0.66940</td><td align=right>1.49387</td></tr><tr><td align=right>28</td><td align=right>0.65991</td><td align=right>1.51536</td><td align=right>78</td><td align=right>0.66948</td><td align=right>1.49370</td></tr><tr><td align=right>29</td><td align=right>0.66036</td><td align=right>1.51433</td><td align=right>79</td><td align=right>0.66955</td><td align=right>1.49354</td></tr><tr><td align=right>30</td><td align=right>0.66082</td><td align=right>1.51328</td><td align=right>80</td><td align=right>0.66962</td><td align=right>1.49339</td></tr><tr><td align=right>31</td><td align=right>0.66123</td><td align=right>1.51233</td><td align=right>81</td><td align=right>0.66968</td><td align=right>1.49325</td></tr><tr><td align=right>32</td><td align=right>0.66161</td><td align=right>1.51146</td><td align=right>82</td><td align=right>0.66974</td><td align=right>1.49312</td></tr><tr><td align=right>33</td><td align=right>0.66200</td><td align=right>1.51057</td><td align=right>83</td><td align=right>0.66980</td><td align=right>1.49298</td></tr><tr><td align=right>34</td><td align=right>0.66235</td><td align=right>1.50977</td><td align=right>84</td><td align=right>0.66988</td><td align=right>1.49281</td></tr><tr><td align=right>35</td><td align=right>0.66270</td><td align=right>1.50899</td><td align=right>85</td><td align=right>0.66993</td><td align=right>1.49270</td></tr><tr><td align=right>36</td><td align=right>0.66302</td><td align=right>1.50824</td><td align=right>86</td><td align=right>0.66999</td><td align=right>1.49257</td></tr><tr><td align=right>37</td><td align=right>0.66334</td><td align=right>1.50753</td><td align=right>87</td><td align=right>0.67005</td><td align=right>1.49244</td></tr><tr><td align=right>38</td><td align=right>0.66362</td><td align=right>1.50688</td><td align=right>88</td><td align=right>0.67009</td><td align=right>1.49233</td></tr><tr><td align=right>39</td><td align=right>0.66391</td><td align=right>1.50623</td><td align=right>89</td><td align=right>0.67016</td><td align=right>1.49219</td></tr><tr><td align=right>40</td><td align=right>0.66417</td><td align=right>1.50563</td><td align=right>90</td><td align=right>0.67021</td><td align=right>1.49207</td></tr><tr><td align=right>41</td><td align=right>0.66443</td><td align=right>1.50504</td><td align=right>91</td><td align=right>0.67026</td><td align=right>1.49196</td></tr><tr><td align=right>42</td><td align=right>0.66469</td><td align=right>1.50447</td><td align=right>92</td><td align=right>0.67031</td><td align=right>1.49185</td></tr><tr><td align=right>43</td><td align=right>0.66493</td><td align=right>1.50393</td><td align=right>93</td><td align=right>0.67036</td><td align=right>1.49174</td></tr><tr><td align=right>44</td><td align=right>0.66515</td><td align=right>1.50341</td><td align=right>94</td><td align=right>0.67041</td><td align=right>1.49161</td></tr><tr><td align=right>45</td><td align=right>0.66539</td><td align=right>1.50289</td><td align=right>95</td><td align=right>0.67046</td><td align=right>1.49152</td></tr><tr><td align=right>46</td><td align=right>0.66557</td><td align=right>1.50246</td><td align=right>96</td><td align=right>0.67049</td><td align=right>1.49144</td></tr><tr><td align=right>47</td><td align=right>0.66578</td><td align=right>1.50200</td><td align=right>97</td><td align=right>0.67055</td><td align=right>1.49131</td></tr><tr><td align=right>48</td><td align=right>0.66598</td><td align=right>1.50155</td><td align=right>98</td><td align=right>0.67060</td><td align=right>1.49121</td></tr><tr><td align=right>49</td><td align=right>0.66616</td><td align=right>1.50115</td><td align=right>99</td><td align=right>0.67063</td><td align=right>1.49114</td></tr><tr><td align=right>50</td><td align=right>0.66633</td><td align=right>1.50076</td><td align=right>100</td><td align=right>0.67068</td><td align=right>1.49102</td></tr></tbody></table><p>Here is a visualization of this table:</p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/simulation100-light.png target=_blank class=imgldlink alt=simulation100><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/simulation100-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/simulation100-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/simulation100-light.png></picture></a></div></div><br><h3 id=factors-for-huge-n>Factors for huge n</h3><p>To build the equation for <span class="math inline">\(n > 100\)</span>, we also continue the approach suggested in <a href=#Hayes2014>[Hayes2014]</a> (pages 2208-2209)
and use the prediction equation of the following form:</p><p><span class="math display">\[\hat{a}_n = \Phi^{-1}(3/4) \Bigg( 1 - \dfrac{\alpha}{n} - \dfrac{\beta}{n^2} \Bigg).
\]</span></p><p>We simulated approximations of <span class="math inline">\(\hat{a}_n\)</span> for some large <span class="math inline">\(n\)</span> values:</p><table><thead><tr><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th></tr></thead><tbody><tr><td align=right>100</td><td align=right>0.6707</td></tr><tr><td align=right>110</td><td align=right>0.6711</td></tr><tr><td align=right>120</td><td align=right>0.6714</td></tr><tr><td align=right>130</td><td align=right>0.6716</td></tr><tr><td align=right>140</td><td align=right>0.6719</td></tr><tr><td align=right>150</td><td align=right>0.6720</td></tr><tr><td align=right>200</td><td align=right>0.6727</td></tr><tr><td align=right>250</td><td align=right>0.6731</td></tr><tr><td align=right>300</td><td align=right>0.6733</td></tr><tr><td align=right>350</td><td align=right>0.6735</td></tr><tr><td align=right>400</td><td align=right>0.6736</td></tr><tr><td align=right>450</td><td align=right>0.6737</td></tr><tr><td align=right>500</td><td align=right>0.6738</td></tr><tr><td align=right>1000</td><td align=right>0.6742</td></tr><tr><td align=right>1500</td><td align=right>0.6743</td></tr><tr><td align=right>2000</td><td align=right>0.6743</td></tr></tbody></table><p>Next, we fitted these values using the multiple linear regression.
The dependent variable is <span class="math inline">\(y = 1 - \hat{a}_n / \Phi^{-1}(3/4)\)</span>;
the independent variables are <span class="math inline">\(n^{-1}\)</span> and <span class="math inline">\(n^{-2}\)</span>;
the intercept is zero:</p><p><span class="math display">\[y = \alpha n^{-1} + \beta n^{-2}.
\]</span></p><p>The fitted model was adjusted a little bit to get nice-looking values (keeping the good accuracy level):</p><p><span class="math display">\[\alpha = 0.5,\quad \beta = 6.5.
\]</span></p><p>These values give pretty accurate estimation of <span class="math inline">\(\hat{a}_n\)</span> for <span class="math inline">\(n > 100\)</span> (the accuracy is less than <span class="math inline">\(10^{-4}\)</span>).</p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/simulation-light.png target=_blank class=imgldlink alt=simulation><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/simulation-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/simulation-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/simulation-light.png></picture></a></div></div><br><h3 id=the-mean-squared-error>The mean squared error</h3><p>It&rsquo;s time to compare the straightforward and the Harrell-Davis-based <span class="math inline">\(\textrm{MAD}\)</span> estimators
in terms of the mean squared error.
To estimate the <span class="math inline">\(\textrm{MSE}\)</span>, we also use numerical simulations.
Here are the results for some <span class="math inline">\(n\)</span> values:</p><table><thead><tr><th align=right>n</th><th align=right>Straightforward</th><th align=right>Harrell-Davis</th></tr></thead><tbody><tr><td align=right>3</td><td align=right>0.690</td><td align=right>0.272</td></tr><tr><td align=right>4</td><td align=right>0.327</td><td align=right>0.205</td></tr><tr><td align=right>5</td><td align=right>0.341</td><td align=right>0.181</td></tr><tr><td align=right>10</td><td align=right>0.136</td><td align=right>0.100</td></tr><tr><td align=right>20</td><td align=right>0.069</td><td align=right>0.055</td></tr><tr><td align=right>30</td><td align=right>0.045</td><td align=right>0.038</td></tr><tr><td align=right>40</td><td align=right>0.034</td><td align=right>0.029</td></tr><tr><td align=right>50</td><td align=right>0.027</td><td align=right>0.024</td></tr><tr><td align=right>100</td><td align=right>0.014</td><td align=right>0.012</td></tr></tbody></table><p>As we can see, the difference between estimators for small samples is tangible.
For example, for <span class="math inline">\(n = 3\)</span>, we got <span class="math inline">\(\textrm{MSE} \approx 0.69\)</span> for the straightforward estimator
vs. <span class="math inline">\(\textrm{MSE} \approx 0.27\)</span> for the Harrell-Davis-based estimator.
Below you can see density plots of <span class="math inline">\(\textrm{MSE}\)</span> for the above <span class="math inline">\(n\)</span> values.</p><p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse3-light.png target=_blank class=imgldlink alt=mse3><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse3-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse3-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse3-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse4-light.png target=_blank class=imgldlink alt=mse4><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse4-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse4-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse4-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse5-light.png target=_blank class=imgldlink alt=mse5><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse5-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse5-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse5-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse10-light.png target=_blank class=imgldlink alt=mse10><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse10-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse10-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse10-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse20-light.png target=_blank class=imgldlink alt=mse20><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse20-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse20-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse20-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse30-light.png target=_blank class=imgldlink alt=mse30><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse30-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse30-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse30-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse40-light.png target=_blank class=imgldlink alt=mse40><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse40-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse40-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse40-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse50-light.png target=_blank class=imgldlink alt=mse50><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse50-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse50-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse50-light.png></picture></a></div></div><br><div class=row><div class=mx-auto><a href=/posts/unbiased-mad-hd/img/mse100-light.png target=_blank class=imgldlink alt=mse100><picture><source theme=dark srcset=/posts/unbiased-mad-hd/img/mse100-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad-hd/img/mse100-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad-hd/img/mse100-light.png></picture></a></div></div><br></p><h3 id=conclusion>Conclusion</h3><p>In this post, we discussed the unbiased <span class="math inline">\(\textrm{MAD}_n\)</span> estimator which is based on the Harrell-Davis quantile estimator
instead of the straightforward median estimator.
The suggested estimator is much more efficient for small samples because it has smaller <span class="math inline">\(\textrm{MSE}\)</span>.
It allows using <span class="math inline">\(\textrm{MAD}_n\)</span> as a efficient alternative to the standard deviation under the normal distribution
with higher accuracy.</p><h3 id=references>References</h3><ul><li><b id=Harrell1982>[Harrell1982]</b><br>Harrell, F.E. and Davis, C.E., 1982. A new distribution-free quantile estimator.
<em>Biometrika</em>, 69(3), pp.635-640.<br><a href=https://doi.org/10.2307/2335999>https://doi.org/10.2307/2335999</a></li><li><b id=Hayes2014>[Hayes2014]</b><br>Hayes, Kevin. &ldquo;Finite-sample bias-correction factors for the median absolute deviation.&rdquo; Communications in Statistics-Simulation and Computation 43, no. 10 (2014): 2205-2212.<br><a href=https://doi.org/10.1080/03610918.2012.748913>https://doi.org/10.1080/03610918.2012.748913</a></li><li><b id=Park2020>[Park2020]</b><br>Park, Chanseok, Haewon Kim, and Min Wang. &ldquo;Investigation of finite-sample properties of robust location and scale estimators.&rdquo; Communications in Statistics-Simulation and Computation (2020): 1-27.<br><a href=https://doi.org/10.1080/03610918.2019.1699114>https://doi.org/10.1080/03610918.2019.1699114</a></li><li><b id=Box1958>[Box1958]</b><br>Box, George EP. &ldquo;A note on the generation of random normal deviates.&rdquo; Ann. Math. Statist. 29 (1958): 610-611.<br><a href=https://doi.org/10.1214/aoms/1177706645>https://doi.org/10.1214/aoms/1177706645</a></li></ul><br><br><div class=row><div class="justify-content-center share-block"><div class=share-title>Share:</div><div class=share-button><a href="https://www.reddit.com/submit?url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad-hd%2f&title=Unbiased%20median%20absolute%20deviation%20based%20on%20the%20Harrell-Davis%20quantile%20estimator" target=_blank title="Share on Reddit"><i class="fab fa-reddit fa-2x"></i></a></div><div class=share-button><a href="https://twitter.com/intent/tweet?text=Unbiased%20median%20absolute%20deviation%20based%20on%20the%20Harrell-Davis%20quantile%20estimator&url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad-hd%2f&via=andrey_akinshin&related=andrey_akinshin" rel=nofollow target=_blank title="Share on Twitter"><i class="fab fa-twitter fa-2x"></i></a></div><div class=share-button><a href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad-hd%2f" target=_blank title="Share on HackerNews"><i class="fab fa-hacker-news fa-2x"></i></a></div><div class=share-button><a href="https://getpocket.com/save?url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad-hd%2f&title=Unbiased%20median%20absolute%20deviation%20based%20on%20the%20Harrell-Davis%20quantile%20estimator" target=_blank title="Add to Pocket"><i class="fab fa-get-pocket fa-2x"></i></a></div></div></div></div><hr></div></div></main></div><footer class=blog-footer><div class=container><p>&copy; 2013&mdash;2022 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub></i></a>
<a href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter></i></a>
<a href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS></i></a>
|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.c26550621ac13c2221d7f6f5e6fe862345d3919723c50d730893e3e4856ecf35.js></script>
<script src=/js/bootstrap.bundle.min.js></script>
<script src=/js/anchor.min.js></script>
<script src=https://aakinshin.net/js/custom.min.f6e5d13fe39f305056cc743ee4cb8d117c1aba26176e58c82c79a480d02fe4b7.js></script>
<script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>