<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.103.0-DEV"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content="Mathematics,Statistics,Research,Quantile,MAD,Harrell-Davis quantile estimator,Research: Unbiased median absolute deviation"><title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator | Andrey Akinshin</title><meta name=description content="The finite-sample bias-correction factors for the median absolute deviation which make it a consistent estimator for the standard deviation (improved version based on the Harrell-Davis quantile estimator)"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link href=https://aakinshin.net/css/main.min.fe1c45a5bb1462c47792274eba884723afa28a87c7a92405ed21b423927e7fbf.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body><nav><div class="container flex flex-wrap items-center justify-between mx-auto max-w-6xl h-12 px-6"><div class="flex items-center justify-start justify-self-start"><div class="flex nav-item h-12 w-12 items-center justify-center"><a class=text-white href=https://aakinshin.net/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></div><div class="flex nav-item h-12 items-center"><a class="px-3 text-white" href=https://aakinshin.net/posts/>Posts</a></div><div class="flex nav-item h-12 items-center"><a class="px-3 text-white" href=https://aakinshin.net/research/>Research</a></div><div class="flex nav-item h-12 items-center"><a class="px-3 text-white" href=https://aakinshin.net/about/>About</a></div></div><button id=theme-toggle type=button class="nav-item p-3 content-center justify-self-end"><svg id="theme-toggle-dark-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></div></nav><div class="main container mx-auto max-w-6xl px-6"><div class=main-post><h1 class=blog-post-title id=post-title>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</h1><div class="flex flex-wrap justify-start items-center"><svg class="fai text-accent-l dark:text-front-d"><title>Date</title><use xlink:href="/img/fa/all.svg#calendar-days"/></svg><time datetime=2021-02-16>February 16, 2021</time><svg class="fai ml-3 text-accent-l dark:text-front-d"><title>Tags</title><use xlink:href="/img/fa/all.svg#tag"/></svg><div class="flex flex-wrap gap-y-1"><a class=label-link href=https://aakinshin.net/tags/mathematics/>Mathematics</a>
<a class=label-link href=https://aakinshin.net/tags/statistics/>Statistics</a>
<a class=label-link href=https://aakinshin.net/tags/research/>Research</a>
<a class=label-link href=https://aakinshin.net/tags/quantile/>Quantile</a>
<a class=label-link href=https://aakinshin.net/tags/mad/>MAD</a>
<a class=label-link href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/>Harrell-Davis quantile estimator</a>
<a class=label-link href=https://aakinshin.net/tags/research-unbiased-mad/>Research: Unbiased median absolute deviation</a></div></div><br><div class="main-content mb-3 px-2 py-1 rounded border text-alert-text-l border-alert-frame-l dark:text-alert-text-d dark:border-alert-frame-d">Update: this blog post is a part of research that aimed to build an unbiased median absolute deviation estimator based on various quantile estimators. A <a href=/posts/preprint-mad-factors/>preprint with final results</a> is available on arXiv: <a href=https://arxiv.org/abs/2207.12005>arXiv:2207.12005 [stat.ME]</a>. Some information in this blog post can be obsolete: please, use the preprint as the primary reference.</div><div class=main-content><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
In the previous post, I <a href=https://aakinshin.net/posts/unbiased-mad/>showed</a>
how to use the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>unbiased</a>
version of the <span class="math inline">\(\textrm{MAD}\)</span> estimator
as a robust alternative to the standard deviation.
&ldquo;Unbiasedness&rdquo; means that such estimator&rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the <a href=https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>bias–variance tradeoff</a>:
when we remove the bias of the <span class="math inline">\(\textrm{MAD}\)</span> estimator,
we increase its variance and mean squared error (<span class="math inline">\(\textrm{MSE}\)</span>).</p><p>In this post, I want to suggest a more <a href=https://en.wikipedia.org/wiki/Efficiency_(statistics)>efficient</a>
unbiased <span class="math inline">\(\textrm{MAD}\)</span> estimator.
It&rsquo;s also a consistent estimator for the standard deviation, but it has smaller <span class="math inline">\(\textrm{MSE}\)</span>.
To build this estimator,
we should replace the classic &ldquo;straightforward&rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&rsquo;s discuss this approach in detail.</p><h3 id=introduction>Introduction</h3><p>Let&rsquo;s consider a sample <span class="math inline">\(x = \{ x_1, x_2, \ldots, x_n \}\)</span>.
Its median absolute deviation <span class="math inline">\(\textrm{MAD}_n\)</span> can be defined as follows:</p><p><span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]</span></p><p>where <span class="math inline">\(\textrm{median}\)</span> is a median estimator, <span class="math inline">\(C_n\)</span> is a scale factor (or consistency constant).</p><p>Typically, <span class="math inline">\(\textrm{median}\)</span> assumes the classic &ldquo;straightforward&rdquo; median estimator:</p><ul><li>If <span class="math inline">\(n\)</span> is odd, the median is the middle element of the sorted sample</li><li>If <span class="math inline">\(n\)</span> is even, the median is the arithmetic average of the two middle elements of the sorted sample</li></ul><p>However, we can use other median estimators.
Let&rsquo;s consider <span class="math inline">\(\textrm{median}_{\textrm{HD}}\)</span> which calculates the median using the Harrell-Davis quantile estimator (see <a href=#Harrell1982>[Harrell1982]</a>):</p><p><span class="math display">\[\textrm{median}_{\textrm{HD}}(x) = \sum_{i=1}^n W_i x_i, \quad
W_i = I_{i/n} \bigg( \frac{n+1}{2}, \frac{n+1}{2} \bigg) -
I_{(i-1)/n} \bigg( \frac{n+1}{2}, \frac{n+1}{2} \bigg)
\]</span></p><p>where <span class="math inline">\(I_t(a, b)\)</span> denotes the <a href=https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function>regularized incomplete beta function</a>.</p><p>When <span class="math inline">\(n \to \infty\)</span>, we have the exact value of <span class="math inline">\(C_n\)</span> which makes <span class="math inline">\(\textrm{MAD}_n\)</span> an unbiased estimator for the standard deviation:</p><p><span class="math display">\[C_n = C_\infty = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056
\]</span></p><p>For finite values of <span class="math inline">\(n\)</span>, we should use adjusted <span class="math inline">\(C_n\)</span> values.
We already discussed these values for the straightforward median estimator in
the <a href=https://aakinshin.net/posts/unbiased-mad/>previous post</a>
(this problem is well-covered in <a href=#Park2020>[Park2020]</a>).
For <span class="math inline">\(\textrm{median}_{\textrm{HD}}\)</span>, we should use another set of <span class="math inline">\(C_n\)</span> values.
We reuse the approach from <a href=#Hayes2014>[Hayes2014]</a> with the following notation:</p><p><span class="math display">\[C_n = \dfrac{1}{\hat{a}_n}
\]</span></p><h3 id=factors-for-small-n>Factors for small n</h3><p>Factors for the small values of <span class="math inline">\(n\)</span> can be obtained using numerical experiments.
Here is the scheme for the performed simulation:</p><ul><li>Enumerate <span class="math inline">\(n\)</span> from <span class="math inline">\(2\)</span> to <span class="math inline">\(100\)</span></li><li>For each <span class="math inline">\(n\)</span>, generate <span class="math inline">\(2\cdot 10^8\)</span> samples from the normal distribution of size <span class="math inline">\(n\)</span>
(the <a href=https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform>Box–Muller transform</a> is used, see <a href=#Box1958>[Box1958]</a>)</li><li>For each sample, estimate <span class="math inline">\(\textrm{MAD}_n\)</span> using <span class="math inline">\(C_n = 1\)</span></li><li>Calculate the arithmetic average of all <span class="math inline">\(\textrm{MAD}_n\)</span> values for the same <span class="math inline">\(n\)</span>: the result is the value of <span class="math inline">\(\hat{a}_n\)</span></li></ul><p>As the result of this simulation, I got the following table with <span class="math inline">\(\hat{a}_n\)</span> and <span class="math inline">\(C_n\)</span> values for <span class="math inline">\(n \in \{ 2..100 \}\)</span>.</p><table><thead><tr><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th><th align=right><span class="math inline">\(C_n\)</span></th><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th><th align=right><span class="math inline">\(C_n\)</span></th></tr></thead><tbody><tr><td align=right>1</td><td align=right>NA</td><td align=right>NA</td><td align=right>51</td><td align=right>0.66649</td><td align=right>1.50039</td></tr><tr><td align=right>2</td><td align=right>0.56417</td><td align=right>1.77250</td><td align=right>52</td><td align=right>0.66668</td><td align=right>1.49998</td></tr><tr><td align=right>3</td><td align=right>0.63769</td><td align=right>1.56816</td><td align=right>53</td><td align=right>0.66682</td><td align=right>1.49966</td></tr><tr><td align=right>4</td><td align=right>0.62661</td><td align=right>1.59589</td><td align=right>54</td><td align=right>0.66699</td><td align=right>1.49926</td></tr><tr><td align=right>5</td><td align=right>0.63853</td><td align=right>1.56611</td><td align=right>55</td><td align=right>0.66713</td><td align=right>1.49895</td></tr><tr><td align=right>6</td><td align=right>0.63834</td><td align=right>1.56656</td><td align=right>56</td><td align=right>0.66728</td><td align=right>1.49863</td></tr><tr><td align=right>7</td><td align=right>0.63915</td><td align=right>1.56458</td><td align=right>57</td><td align=right>0.66741</td><td align=right>1.49833</td></tr><tr><td align=right>8</td><td align=right>0.64141</td><td align=right>1.55908</td><td align=right>58</td><td align=right>0.66753</td><td align=right>1.49805</td></tr><tr><td align=right>9</td><td align=right>0.64237</td><td align=right>1.55675</td><td align=right>59</td><td align=right>0.66767</td><td align=right>1.49774</td></tr><tr><td align=right>10</td><td align=right>0.64397</td><td align=right>1.55288</td><td align=right>60</td><td align=right>0.66780</td><td align=right>1.49746</td></tr><tr><td align=right>11</td><td align=right>0.64535</td><td align=right>1.54955</td><td align=right>61</td><td align=right>0.66791</td><td align=right>1.49720</td></tr><tr><td align=right>12</td><td align=right>0.64662</td><td align=right>1.54651</td><td align=right>62</td><td align=right>0.66803</td><td align=right>1.49694</td></tr><tr><td align=right>13</td><td align=right>0.64790</td><td align=right>1.54346</td><td align=right>63</td><td align=right>0.66815</td><td align=right>1.49667</td></tr><tr><td align=right>14</td><td align=right>0.64908</td><td align=right>1.54064</td><td align=right>64</td><td align=right>0.66825</td><td align=right>1.49644</td></tr><tr><td align=right>15</td><td align=right>0.65018</td><td align=right>1.53803</td><td align=right>65</td><td align=right>0.66836</td><td align=right>1.49621</td></tr><tr><td align=right>16</td><td align=right>0.65125</td><td align=right>1.53552</td><td align=right>66</td><td align=right>0.66846</td><td align=right>1.49597</td></tr><tr><td align=right>17</td><td align=right>0.65226</td><td align=right>1.53313</td><td align=right>67</td><td align=right>0.66857</td><td align=right>1.49574</td></tr><tr><td align=right>18</td><td align=right>0.65317</td><td align=right>1.53101</td><td align=right>68</td><td align=right>0.66865</td><td align=right>1.49555</td></tr><tr><td align=right>19</td><td align=right>0.65404</td><td align=right>1.52896</td><td align=right>69</td><td align=right>0.66876</td><td align=right>1.49531</td></tr><tr><td align=right>20</td><td align=right>0.65489</td><td align=right>1.52698</td><td align=right>70</td><td align=right>0.66883</td><td align=right>1.49514</td></tr><tr><td align=right>21</td><td align=right>0.65565</td><td align=right>1.52520</td><td align=right>71</td><td align=right>0.66893</td><td align=right>1.49493</td></tr><tr><td align=right>22</td><td align=right>0.65638</td><td align=right>1.52351</td><td align=right>72</td><td align=right>0.66901</td><td align=right>1.49475</td></tr><tr><td align=right>23</td><td align=right>0.65708</td><td align=right>1.52190</td><td align=right>73</td><td align=right>0.66910</td><td align=right>1.49456</td></tr><tr><td align=right>24</td><td align=right>0.65771</td><td align=right>1.52043</td><td align=right>74</td><td align=right>0.66918</td><td align=right>1.49437</td></tr><tr><td align=right>25</td><td align=right>0.65832</td><td align=right>1.51902</td><td align=right>75</td><td align=right>0.66925</td><td align=right>1.49422</td></tr><tr><td align=right>26</td><td align=right>0.65888</td><td align=right>1.51772</td><td align=right>76</td><td align=right>0.66933</td><td align=right>1.49402</td></tr><tr><td align=right>27</td><td align=right>0.65943</td><td align=right>1.51647</td><td align=right>77</td><td align=right>0.66940</td><td align=right>1.49387</td></tr><tr><td align=right>28</td><td align=right>0.65991</td><td align=right>1.51536</td><td align=right>78</td><td align=right>0.66948</td><td align=right>1.49370</td></tr><tr><td align=right>29</td><td align=right>0.66036</td><td align=right>1.51433</td><td align=right>79</td><td align=right>0.66955</td><td align=right>1.49354</td></tr><tr><td align=right>30</td><td align=right>0.66082</td><td align=right>1.51328</td><td align=right>80</td><td align=right>0.66962</td><td align=right>1.49339</td></tr><tr><td align=right>31</td><td align=right>0.66123</td><td align=right>1.51233</td><td align=right>81</td><td align=right>0.66968</td><td align=right>1.49325</td></tr><tr><td align=right>32</td><td align=right>0.66161</td><td align=right>1.51146</td><td align=right>82</td><td align=right>0.66974</td><td align=right>1.49312</td></tr><tr><td align=right>33</td><td align=right>0.66200</td><td align=right>1.51057</td><td align=right>83</td><td align=right>0.66980</td><td align=right>1.49298</td></tr><tr><td align=right>34</td><td align=right>0.66235</td><td align=right>1.50977</td><td align=right>84</td><td align=right>0.66988</td><td align=right>1.49281</td></tr><tr><td align=right>35</td><td align=right>0.66270</td><td align=right>1.50899</td><td align=right>85</td><td align=right>0.66993</td><td align=right>1.49270</td></tr><tr><td align=right>36</td><td align=right>0.66302</td><td align=right>1.50824</td><td align=right>86</td><td align=right>0.66999</td><td align=right>1.49257</td></tr><tr><td align=right>37</td><td align=right>0.66334</td><td align=right>1.50753</td><td align=right>87</td><td align=right>0.67005</td><td align=right>1.49244</td></tr><tr><td align=right>38</td><td align=right>0.66362</td><td align=right>1.50688</td><td align=right>88</td><td align=right>0.67009</td><td align=right>1.49233</td></tr><tr><td align=right>39</td><td align=right>0.66391</td><td align=right>1.50623</td><td align=right>89</td><td align=right>0.67016</td><td align=right>1.49219</td></tr><tr><td align=right>40</td><td align=right>0.66417</td><td align=right>1.50563</td><td align=right>90</td><td align=right>0.67021</td><td align=right>1.49207</td></tr><tr><td align=right>41</td><td align=right>0.66443</td><td align=right>1.50504</td><td align=right>91</td><td align=right>0.67026</td><td align=right>1.49196</td></tr><tr><td align=right>42</td><td align=right>0.66469</td><td align=right>1.50447</td><td align=right>92</td><td align=right>0.67031</td><td align=right>1.49185</td></tr><tr><td align=right>43</td><td align=right>0.66493</td><td align=right>1.50393</td><td align=right>93</td><td align=right>0.67036</td><td align=right>1.49174</td></tr><tr><td align=right>44</td><td align=right>0.66515</td><td align=right>1.50341</td><td align=right>94</td><td align=right>0.67041</td><td align=right>1.49161</td></tr><tr><td align=right>45</td><td align=right>0.66539</td><td align=right>1.50289</td><td align=right>95</td><td align=right>0.67046</td><td align=right>1.49152</td></tr><tr><td align=right>46</td><td align=right>0.66557</td><td align=right>1.50246</td><td align=right>96</td><td align=right>0.67049</td><td align=right>1.49144</td></tr><tr><td align=right>47</td><td align=right>0.66578</td><td align=right>1.50200</td><td align=right>97</td><td align=right>0.67055</td><td align=right>1.49131</td></tr><tr><td align=right>48</td><td align=right>0.66598</td><td align=right>1.50155</td><td align=right>98</td><td align=right>0.67060</td><td align=right>1.49121</td></tr><tr><td align=right>49</td><td align=right>0.66616</td><td align=right>1.50115</td><td align=right>99</td><td align=right>0.67063</td><td align=right>1.49114</td></tr><tr><td align=right>50</td><td align=right>0.66633</td><td align=right>1.50076</td><td align=right>100</td><td align=right>0.67068</td><td align=right>1.49102</td></tr></tbody></table><p>Here is a visualization of this table:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/simulation100-light.png target=_blank alt=simulation100><img src=/posts/unbiased-mad-hd/img/simulation100-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/simulation100-dark.png target=_blank alt=simulation100><img src=/posts/unbiased-mad-hd/img/simulation100-dark.png width=800></a></div><h3 id=factors-for-huge-n>Factors for huge n</h3><p>To build the equation for <span class="math inline">\(n > 100\)</span>, we also continue the approach suggested in <a href=#Hayes2014>[Hayes2014]</a> (pages 2208-2209)
and use the prediction equation of the following form:</p><p><span class="math display">\[\hat{a}_n = \Phi^{-1}(3/4) \Bigg( 1 - \dfrac{\alpha}{n} - \dfrac{\beta}{n^2} \Bigg).
\]</span></p><p>We simulated approximations of <span class="math inline">\(\hat{a}_n\)</span> for some large <span class="math inline">\(n\)</span> values:</p><table><thead><tr><th align=right><span class="math inline">\(n\)</span></th><th align=right><span class="math inline">\(\hat{a}_n\)</span></th></tr></thead><tbody><tr><td align=right>100</td><td align=right>0.6707</td></tr><tr><td align=right>110</td><td align=right>0.6711</td></tr><tr><td align=right>120</td><td align=right>0.6714</td></tr><tr><td align=right>130</td><td align=right>0.6716</td></tr><tr><td align=right>140</td><td align=right>0.6719</td></tr><tr><td align=right>150</td><td align=right>0.6720</td></tr><tr><td align=right>200</td><td align=right>0.6727</td></tr><tr><td align=right>250</td><td align=right>0.6731</td></tr><tr><td align=right>300</td><td align=right>0.6733</td></tr><tr><td align=right>350</td><td align=right>0.6735</td></tr><tr><td align=right>400</td><td align=right>0.6736</td></tr><tr><td align=right>450</td><td align=right>0.6737</td></tr><tr><td align=right>500</td><td align=right>0.6738</td></tr><tr><td align=right>1000</td><td align=right>0.6742</td></tr><tr><td align=right>1500</td><td align=right>0.6743</td></tr><tr><td align=right>2000</td><td align=right>0.6743</td></tr></tbody></table><p>Next, we fitted these values using the multiple linear regression.
The dependent variable is <span class="math inline">\(y = 1 - \hat{a}_n / \Phi^{-1}(3/4)\)</span>;
the independent variables are <span class="math inline">\(n^{-1}\)</span> and <span class="math inline">\(n^{-2}\)</span>;
the intercept is zero:</p><p><span class="math display">\[y = \alpha n^{-1} + \beta n^{-2}.
\]</span></p><p>The fitted model was adjusted a little bit to get nice-looking values (keeping the good accuracy level):</p><p><span class="math display">\[\alpha = 0.5,\quad \beta = 6.5.
\]</span></p><p>These values give pretty accurate estimation of <span class="math inline">\(\hat{a}_n\)</span> for <span class="math inline">\(n > 100\)</span> (the accuracy is less than <span class="math inline">\(10^{-4}\)</span>).</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/simulation-light.png target=_blank alt=simulation><img src=/posts/unbiased-mad-hd/img/simulation-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/simulation-dark.png target=_blank alt=simulation><img src=/posts/unbiased-mad-hd/img/simulation-dark.png width=800></a></div><h3 id=the-mean-squared-error>The mean squared error</h3><p>It&rsquo;s time to compare the straightforward and the Harrell-Davis-based <span class="math inline">\(\textrm{MAD}\)</span> estimators
in terms of the mean squared error.
To estimate the <span class="math inline">\(\textrm{MSE}\)</span>, we also use numerical simulations.
Here are the results for some <span class="math inline">\(n\)</span> values:</p><table><thead><tr><th align=right>n</th><th align=right>Straightforward</th><th align=right>Harrell-Davis</th></tr></thead><tbody><tr><td align=right>3</td><td align=right>0.690</td><td align=right>0.272</td></tr><tr><td align=right>4</td><td align=right>0.327</td><td align=right>0.205</td></tr><tr><td align=right>5</td><td align=right>0.341</td><td align=right>0.181</td></tr><tr><td align=right>10</td><td align=right>0.136</td><td align=right>0.100</td></tr><tr><td align=right>20</td><td align=right>0.069</td><td align=right>0.055</td></tr><tr><td align=right>30</td><td align=right>0.045</td><td align=right>0.038</td></tr><tr><td align=right>40</td><td align=right>0.034</td><td align=right>0.029</td></tr><tr><td align=right>50</td><td align=right>0.027</td><td align=right>0.024</td></tr><tr><td align=right>100</td><td align=right>0.014</td><td align=right>0.012</td></tr></tbody></table><p>As we can see, the difference between estimators for small samples is tangible.
For example, for <span class="math inline">\(n = 3\)</span>, we got <span class="math inline">\(\textrm{MSE} \approx 0.69\)</span> for the straightforward estimator
vs. <span class="math inline">\(\textrm{MSE} \approx 0.27\)</span> for the Harrell-Davis-based estimator.
Below you can see density plots of <span class="math inline">\(\textrm{MSE}\)</span> for the above <span class="math inline">\(n\)</span> values.</p><p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse3-light.png target=_blank alt=mse3><img src=/posts/unbiased-mad-hd/img/mse3-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse3-dark.png target=_blank alt=mse3><img src=/posts/unbiased-mad-hd/img/mse3-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse4-light.png target=_blank alt=mse4><img src=/posts/unbiased-mad-hd/img/mse4-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse4-dark.png target=_blank alt=mse4><img src=/posts/unbiased-mad-hd/img/mse4-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse5-light.png target=_blank alt=mse5><img src=/posts/unbiased-mad-hd/img/mse5-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse5-dark.png target=_blank alt=mse5><img src=/posts/unbiased-mad-hd/img/mse5-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse10-light.png target=_blank alt=mse10><img src=/posts/unbiased-mad-hd/img/mse10-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse10-dark.png target=_blank alt=mse10><img src=/posts/unbiased-mad-hd/img/mse10-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse20-light.png target=_blank alt=mse20><img src=/posts/unbiased-mad-hd/img/mse20-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse20-dark.png target=_blank alt=mse20><img src=/posts/unbiased-mad-hd/img/mse20-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse30-light.png target=_blank alt=mse30><img src=/posts/unbiased-mad-hd/img/mse30-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse30-dark.png target=_blank alt=mse30><img src=/posts/unbiased-mad-hd/img/mse30-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse40-light.png target=_blank alt=mse40><img src=/posts/unbiased-mad-hd/img/mse40-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse40-dark.png target=_blank alt=mse40><img src=/posts/unbiased-mad-hd/img/mse40-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse50-light.png target=_blank alt=mse50><img src=/posts/unbiased-mad-hd/img/mse50-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse50-dark.png target=_blank alt=mse50><img src=/posts/unbiased-mad-hd/img/mse50-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/unbiased-mad-hd/img/mse100-light.png target=_blank alt=mse100><img src=/posts/unbiased-mad-hd/img/mse100-light.png width=800></a>
<a class="img-dark hidden" href=/posts/unbiased-mad-hd/img/mse100-dark.png target=_blank alt=mse100><img src=/posts/unbiased-mad-hd/img/mse100-dark.png width=800></a></div></p><h3 id=conclusion>Conclusion</h3><p>In this post, we discussed the unbiased <span class="math inline">\(\textrm{MAD}_n\)</span> estimator which is based on the Harrell-Davis quantile estimator
instead of the straightforward median estimator.
The suggested estimator is much more efficient for small samples because it has smaller <span class="math inline">\(\textrm{MSE}\)</span>.
It allows using <span class="math inline">\(\textrm{MAD}_n\)</span> as a efficient alternative to the standard deviation under the normal distribution
with higher accuracy.</p><h3 id=references>References</h3><ul><li><b id=Harrell1982>[Harrell1982]</b><br>Harrell, F.E. and Davis, C.E., 1982. A new distribution-free quantile estimator.
<em>Biometrika</em>, 69(3), pp.635-640.<br><a href=https://doi.org/10.2307/2335999>https://doi.org/10.2307/2335999</a></li><li><b id=Hayes2014>[Hayes2014]</b><br>Hayes, Kevin. &ldquo;Finite-sample bias-correction factors for the median absolute deviation.&rdquo; Communications in Statistics-Simulation and Computation 43, no. 10 (2014): 2205-2212.<br><a href=https://doi.org/10.1080/03610918.2012.748913>https://doi.org/10.1080/03610918.2012.748913</a></li><li><b id=Park2020>[Park2020]</b><br>Park, Chanseok, Haewon Kim, and Min Wang. &ldquo;Investigation of finite-sample properties of robust location and scale estimators.&rdquo; Communications in Statistics-Simulation and Computation (2020): 1-27.<br><a href=https://doi.org/10.1080/03610918.2019.1699114>https://doi.org/10.1080/03610918.2019.1699114</a></li><li><b id=Box1958>[Box1958]</b><br>Box, George EP. &ldquo;A note on the generation of random normal deviates.&rdquo; Ann. Math. Statist. 29 (1958): 610-611.<br><a href=https://doi.org/10.1214/aoms/1177706645>https://doi.org/10.1214/aoms/1177706645</a></li></ul></div><br><br></div></div><script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");const imagesDark=document.querySelectorAll(".img-dark"),imagesLight=document.querySelectorAll(".img-light");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?(themeToggleLightIcon.classList.remove("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")})):(themeToggleDarkIcon.classList.remove("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")})),themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),imagesLight.forEach(e=>{e.classList.toggle("hidden")}),imagesDark.forEach(e=>{e.classList.toggle("hidden")}),localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))})</script><footer class="z-20 p-4 md:p-6 mt-3 w-full border-t border-gray-200 shadow flex items-center justify-center dark:border-gray-600 bg-white dark:bg-zinc-800"><span class="dark:text-gray-400 text-center">© 2013—2023 <span class=whitespace-nowrap>Andrey Akinshin</span></span><ul class="flex flex-wrap items-center px-4 mb-1"><li><a href=https://github.com/AndreyAkinshin><svg class="fai fai-link w-5 h-5"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a></li><li><a href=https://twitter.com/andrey_akinshin><svg class="fai fai-link w-5 h-5"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></li><li><a href=https://aakinshin.net/posts/index.xml><svg class="fai fai-link w-5 h-5"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a></li></ul><div class=main-content><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></body></html>