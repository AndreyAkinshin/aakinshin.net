<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.103.0-DEV"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content="Mathematics,Statistics,Research,Research: Weighted quantile estimators"><title>The Huggins-Roy family of effective sample sizes | Andrey Akinshin</title><meta name=description content="When we work with weighted samples, it&amp;rsquo;s essential to introduce adjustments for the sample size. Indeed, let&amp;rsquo;s consider two following weighted ..."><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=https://aakinshin.net/sass/bootstrap-light.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=https://aakinshin.net/sass/bootstrap-dark.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.d7f6578e23ad3f0c33c589f9bcb34f995b45121f1f3ce888869c86b26826c845.js></script>
<script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.9e68d431432744c07a43381a8a833ffe0921cc0e006d84ad97f0b0d43c5cd82d.mjs></script>
<link href=https://aakinshin.net/css/fontawesome-all.min.178e95bba6ea2e9a90838cd646658f4bf6667a6ceaa057cb587bf7e6eb8412d3.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.60773ab7266ecc6e9625306f57c0a82a219b011dc3ea2d83763d1ecdf11c86d2.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.7e186db80d8c431d196b3e0718afb0636b82a269a8c92521c11a55267a24fc27.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script src=/js/jquery-3.3.1.slim.min.js></script></head><body class="d-flex flex-column min-vh-100"><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-expanded=false>Blog</a><ul class="dropdown-menu bg-primary"><li><a class=dropdown-item href=https://aakinshin.net/posts/>All Posts</a></li><li><a class=dropdown-item href=https://aakinshin.net/tags/statistics/>Posts about Statistics</a></li><li></li><a class=dropdown-item href=https://aakinshin.net/tags/>All Tags</a></li></ul></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li></ul><ul class="navbar-nav ms-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class="blog-post table-sm"><h1 class=blog-post-title id=post-title>The Huggins-Roy family of effective sample sizes</h1><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2022-09-13>September 13, 2022</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/research-wqe/ class="badge badge-info">Research: Weighted quantile estimators</a></span><br><br><p>When we work with weighted samples, it&rsquo;s essential to introduce adjustments for the sample size.
Indeed, let&rsquo;s consider two following weighted samples:</p><p><span class="math display">\[\mathbf{x}_1 = \{ x_1, x_2, \ldots, x_n \}, \quad \mathbf{w}_1 = \{ w_1, w_2, \ldots, w_n \},
\]</span></p><p><span class="math display">\[\mathbf{x}_2 = \{ x_1, x_2, \ldots, x_n, x_{n+1} \}, \quad \mathbf{w}_2 = \{ w_1, w_2, \ldots, w_n, 0 \}.
\]</span></p><p>Since the weight of <span class="math inline">\(x_{n+1}\)</span> in the second sample is zero,
it&rsquo;s natural to expect that both samples have the same set of properties.
However, there is a major difference between <span class="math inline">\(\mathbf{x}_1\)</span> and <span class="math inline">\(\mathbf{x}_2\)</span>: their sample sizes which are
<span class="math inline">\(n\)</span> and <span class="math inline">\(n+1\)</span>.
In order to eliminate this difference, we typically introduce the <em>effective sample size</em> (ESS)
which is estimated based on the list of weights.</p><p>There are various ways to estimate the ESS.
In this post, we briefly discuss the Huggins-Roy&rsquo;s family of ESS.</p><p>For a list of weights <span class="math inline">\(\mathbf{w} = \{ w_1, w_2, \ldots, w_n \}\)</span>, we can consider the corresponding normalized weights
(or standardized weights):</p><p><span class="math display">\[\overline{\mathbf{w}} = \frac{\mathbf{w}}{\sum_{i=1}^n w_i}.
\]</span></p><p>For any non-degenerate weighted sample, the sum of all weights is always positive so that
the normalized weights are defined.</p><p>The Huggins-Roy&rsquo;s family is given by:</p><p><span class="math display">\[\operatorname{ESS}_\beta(\overline{\mathbf{w}}) =
\Bigg( \frac{1}{\sum_{i=1}^n \overline{w}_i^\beta } \Bigg)^{\frac{1}{\beta - 1}} =
\Bigg( \sum_{i=1}^n \overline{w}_i^\beta \Bigg)^{\frac{1}{1 - \beta}}.
\]</span></p><p>This family is proposed in <a href=#Huggins2019>[Huggins2019]</a> and discussed in
<a href=#Elvira2021>[Elvira2021]</a> and <a href=#Elvira2022>[Elvira2022]</a>.</p><p>In order to understand this approach, let&rsquo;s consider several special cases.</p><ul><li><span class="math inline">\(\beta = 0\)</span>:</li></ul><p><span class="math display">\[\operatorname{ESS}_0(\overline{\mathbf{w}}) = n - n_z(\overline{\mathbf{w}}),
\]</span></p><p>where <span class="math inline">\(n_z(\overline{\mathbf{w}})\)</span> is the number of zeros in <span class="math inline">\(\overline{\mathbf{w}}\)</span>.
This approach is quite straightforward: we just omit elements with zero weights.</p><ul><li><span class="math inline">\(\beta = 1/2\)</span>:</li></ul><p><span class="math display">\[\operatorname{ESS}_{1/2}(\overline{\mathbf{w}}) = \Bigg( \sum_{i=1}^n \sqrt{\overline{w}_i} \Bigg)^2.
\]</span></p><ul><li><span class="math inline">\(\beta = 1\)</span>:</li></ul><p><span class="math display">\[\operatorname{ESS}_{1}(\overline{\mathbf{w}}) =
\operatorname{exp} \Bigg( -\sum_{i=1}^n \overline{w}_i \log \overline{w}_i \Bigg)^2.
\]</span></p><p>This approach is also known as <em>perplexity</em></p><ul><li><span class="math inline">\(\beta = 2\)</span>:</li></ul><p><span class="math display">\[\operatorname{ESS}_{2}(\overline{\mathbf{w}}) = \frac{1}{\sum_{i=1}^n \overline{w}_i^2 }.
\]</span></p><p>This approach is often referenced as the Kish&rsquo;s effective sample size (see <a href=#Kish1965>[Kish1965]</a>).</p><ul><li><span class="math inline">\(\beta = \infty\)</span>:</li></ul><p><span class="math display">\[\operatorname{ESS}_{\infty}(\overline{\mathbf{w}}) =
\frac{1}{\max [\overline{w}_1, \overline{w}_2, \ldots, \overline{w}_n] }.
\]</span></p><p>This approach is also popular and straightforward: we define the sample size based on the maximum weight.</p><h3 id=references>References</h3><ul><li><b id=Elvira2022>[Elvira2022]</b><br>Víctor Elvira, Luca Martino, and Christian P. Robert. &ldquo;Rethinking the effective sample size.&rdquo;
International Statistical Review (2022).<br><a href=https://arxiv.org/pdf/1809.04129.pdf>https://arxiv.org/pdf/1809.04129.pdf</a></li><li><b id=Elvira2021>[Elvira2021]</b><br>Víctor Elvira, Luca Martino. &ldquo;Effective sample size approximations as entropy measures.&rdquo; (2021)<br><a href=https://vixra.org/pdf/2111.0145v1.pdf>https://vixra.org/pdf/2111.0145v1.pdf</a></li><li><b id=Huggins2019>[Huggins2019]</b><br>Huggins, Jonathan H., and Daniel M. Roy.
&ldquo;Sequential Monte Carlo as approximate sampling: bounds, adaptive resampling via <span class="math inline">\(\infty\)</span>-ESS,
and an application to particle Gibbs.&rdquo; Bernoulli 25, no. 1 (2019): 584-622.<br><a href=https://arxiv.org/pdf/1503.00966.pdf>https://arxiv.org/pdf/1503.00966.pdf</a></li><li><b id=Kish1965>[Kish1965]</b><br>Kish, Leslie. Survey sampling. Chichester., 1965.<br><a href=https://doi.org/10.1002/bimj.19680100122>https://doi.org/10.1002/bimj.19680100122</a></li></ul><br><br><div class=row><div class="justify-content-center share-block"><div class=share-title>Share:</div><div class=share-button><a href="https://www.reddit.com/submit?url=https%3a%2f%2faakinshin.net%2fposts%2fhuggins-roy-ess%2f&title=The%20Huggins-Roy%20family%20of%20effective%20sample%20sizes" target=_blank title="Share on Reddit"><i class="fab fa-reddit fa-2x"></i></a></div><div class=share-button><a href="https://twitter.com/intent/tweet?text=The%20Huggins-Roy%20family%20of%20effective%20sample%20sizes&url=https%3a%2f%2faakinshin.net%2fposts%2fhuggins-roy-ess%2f&via=andrey_akinshin&related=andrey_akinshin" rel=nofollow target=_blank title="Share on Twitter"><i class="fab fa-twitter fa-2x"></i></a></div><div class=share-button><a href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faakinshin.net%2fposts%2fhuggins-roy-ess%2f" target=_blank title="Share on HackerNews"><i class="fab fa-hacker-news fa-2x"></i></a></div><div class=share-button><a href="https://getpocket.com/save?url=https%3a%2f%2faakinshin.net%2fposts%2fhuggins-roy-ess%2f&title=The%20Huggins-Roy%20family%20of%20effective%20sample%20sizes" target=_blank title="Add to Pocket"><i class="fab fa-get-pocket fa-2x"></i></a></div></div></div></div><hr></div></div></main></div><footer class="blog-footer mt-auto"><div class=container><p>&copy; 2013&mdash;2022 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub></i></a>
<a href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter></i></a>
<a href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS></i></a>
|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.c26550621ac13c2221d7f6f5e6fe862345d3919723c50d730893e3e4856ecf35.js></script>
<script src=/js/bootstrap.bundle.min.js></script>
<script src=/js/anchor.min.js></script>
<script src=https://aakinshin.net/js/custom.min.f6e5d13fe39f305056cc743ee4cb8d117c1aba26176e58c82c79a480d02fe4b7.js></script>
<script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>