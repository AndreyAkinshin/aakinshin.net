<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.103.0-DEV"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content="Mathematics,Statistics,Research,Quantile,MAD,Research: Unbiased median absolute deviation"><title>Unbiased median absolute deviation | Andrey Akinshin</title><meta name=description content="The finite-sample bias-correction factors for the median absolute deviation which make it a consistent estimator for the standard deviation"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=https://aakinshin.net/sass/bootstrap-light.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=https://aakinshin.net/sass/bootstrap-dark.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.d7f6578e23ad3f0c33c589f9bcb34f995b45121f1f3ce888869c86b26826c845.js></script>
<script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.9e68d431432744c07a43381a8a833ffe0921cc0e006d84ad97f0b0d43c5cd82d.mjs></script>
<link href=https://aakinshin.net/css/main.min.d3894e60e337fa8d9f2c81dcda5dcb9144502462aaabed3c6bad2fcf178a8410.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body class="d-flex flex-column min-vh-100"><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><svg class="fai"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></li><li class=nav-item><a class=nav-link id=nav-link-posts href=https://aakinshin.net/posts/>Posts</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li></ul><ul class="navbar-nav ms-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class="blog-post table-sm"><h1 class=blog-post-title id=post-title>Unbiased median absolute deviation</h1><span class=blog-post-meta><svg class="fai"><use xlink:href="/img/fa/all.svg#calendar-days"/></svg><time datetime=2021-02-09>February 9, 2021</time>
&nbsp;&nbsp;<svg class="fai"><use xlink:href="/img/fa/all.svg#tag"/></svg>
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/research-unbiased-mad/ class="badge badge-info">Research: Unbiased median absolute deviation</a></span><br><br><strong>Update: this blog post is a part of research that aimed to build an unbiased median absolute deviation estimator based on various quantile estimators. A <a href=/posts/preprint-mad-factors/>preprint with final results</a> is available on arXiv: <a href=https://arxiv.org/abs/2207.12005>arXiv:2207.12005 [stat.ME]</a>. Some information in this blog post can be obsolete: please, use the preprint as the primary reference.</strong><br><br><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
For distribution <span class="math inline">\(X\)</span>, it can be calculated as follows:</p><p><span class="math display">\[\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]</span></p><p>where <span class="math inline">\(C\)</span> is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the <span class="math inline">\(\textrm{MAD}\)</span> as a <a href=https://en.wikipedia.org/wiki/Consistent_estimator>consistent estimator</a>
for the standard deviation under the normal distribution,
we should set</p><p><span class="math display">\[C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
\]</span></p><p>where <span class="math inline">\(\Phi^{-1}\)</span> is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If <span class="math inline">\(X\)</span> is the normal distribution, we get <span class="math inline">\(\textrm{MAD} = \sigma\)</span> where <span class="math inline">\(\sigma\)</span> is the standard deviation.</p><p>Now let&rsquo;s consider a sample <span class="math inline">\(x = \{ x_1, x_2, \ldots x_n \}\)</span>.
Let&rsquo;s denote the median absolute deviation for a sample of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\textrm{MAD}_n\)</span>.
The corresponding equation looks similar to the definition of <span class="math inline">\(\textrm{MAD}\)</span> for a distribution:</p><p><span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
\]</span></p><p>Let&rsquo;s assume that <span class="math inline">\(\textrm{median}\)</span> is the straightforward definition of the median
(if <span class="math inline">\(n\)</span> is odd, the median is the middle element of the sorted sample,
if <span class="math inline">\(n\)</span> is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use <span class="math inline">\(C_n = C_{\infty}\)</span> for extremely large sample sizes.
However, for small <span class="math inline">\(n\)</span>, <span class="math inline">\(\textrm{MAD}_n\)</span> becomes a <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>biased estimator</a>.
If we want to get an unbiased version, we should adjust the value of <span class="math inline">\(C_n\)</span>.</p><p>In this post, we look at the possible approaches and learn the way to get the exact value of <span class="math inline">\(C_n\)</span>
that makes <span class="math inline">\(\textrm{MAD}_n\)</span> unbiased estimator of the median absolute deviation for any <span class="math inline">\(n\)</span>.</p><h3 id=the-bias>The bias</h3><p>Let&rsquo;s briefly discuss the impact of the bias on our measurements.
To illustrate the problem, we take <span class="math inline">\(100\,000\)</span> samples of size <span class="math inline">\(n = 5\)</span>
from the standard normal distribution and
calculate <span class="math inline">\(\textrm{MAD}_5\)</span> for each of them using <span class="math inline">\(C = 1\)</span>.
The obtained numbers form the following distribution:</p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad/img/bias-light.png target=_blank class=imgldlink alt=bias><picture><source theme=dark srcset=/posts/unbiased-mad/img/bias-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad/img/bias-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad/img/bias-light.png></picture></a></div></div><br><p>If we try to use <span class="math inline">\(\textrm{MAD}_5\)</span> with <span class="math inline">\(C = 1\)</span> as a standard deviation estimator,
it would be a <em>biased estimator</em>.
Indeed, the standard deviation equals <span class="math inline">\(1\)</span> (the true value),
but the expected value of <span class="math inline">\(\textrm{MAD}_5\)</span> is about
<span class="math inline">\(E[\textrm{MAD}_5] \approx 0.5542\)</span>.
In order to make it unbiased, we should set <span class="math inline">\(C_5 = 1 / 0.5542 \approx 1.804\)</span>.
If we repeat the experiment with the modified scale factor, we get a modified version of our distribution:</p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad/img/bias2-light.png target=_blank class=imgldlink alt=bias2><picture><source theme=dark srcset=/posts/unbiased-mad/img/bias2-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad/img/bias2-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad/img/bias2-light.png></picture></a></div></div><br><p>Now <span class="math inline">\(E[\textrm{MAD}_5] \approx 1\)</span> which makes <span class="math inline">\(\textrm{MAD}_5\)</span> <em>unbiased estimator</em>.</p><p>Note that <span class="math inline">\(C_5 = 1.804\)</span> differs from <span class="math inline">\(C_{\infty} \approx 1.4826\)</span> which is the proper scale factor for <span class="math inline">\(n \to \infty\)</span>.
Each sample size needs its own scale factor to make <span class="math inline">\(\textrm{MAD}_n\)</span> unbiased.
Let&rsquo;s review some papers and look at different approaches to find the optimal scale factor value.</p><h3 id=literature-overview>Literature overview</h3><p>One of the first mentions of the median absolute deviation can be found in <a href=#Hampel1974>[Hampel1974]</a>.
In this paper, Frank R Hampel introduced <span class="math inline">\(\textrm{MAD}\)</span> as a robust measure of scale
(attributed to Gauss).
I have found four papers that describe unbiased versions:
<a href=#Croux1992>[Croux1992]</a>, <a href=#Williams2011>[Williams2011]</a>, <a href=#Hayes2014>[Hayes2014]</a>, and <a href=#Park2020>[Park2020]</a>.
Let&rsquo;s briefly discuss approaches from these papers.</p><h4 id=the-croux-rousseeuw-approach>The Croux-Rousseeuw approach</h4><p>In <a href=#Croux1992>[Croux1992]</a>, Christophe Croux and Peter J. Rousseeuw
described an unbiased version of <span class="math inline">\(\textrm{MAD}\)</span>.
They suggested using the following equations:</p><p><span class="math display">\[C_n = \dfrac{b_n}{\Phi^{-1}(3/4)}.
\]</span></p><p>For <span class="math inline">\(n \leq 9\)</span>, the approximated values of <span class="math inline">\(b_n\)</span> were defined as follows:</p><table><thead><tr><th align=right>n</th><th align=right><span class="math inline">\(b_n\)</span></th></tr></thead><tbody><tr><td align=right>2</td><td align=right>1.196</td></tr><tr><td align=right>3</td><td align=right>1.495</td></tr><tr><td align=right>4</td><td align=right>1.363</td></tr><tr><td align=right>5</td><td align=right>1.206</td></tr><tr><td align=right>6</td><td align=right>1.200</td></tr><tr><td align=right>7</td><td align=right>1.140</td></tr><tr><td align=right>8</td><td align=right>1.129</td></tr><tr><td align=right>9</td><td align=right>1.107</td></tr></tbody></table><p>For <span class="math inline">\(n > 9\)</span>, they suggested to use the following equation:</p><p><span class="math display">\[b_n = \dfrac{n}{n-0.8}.
\]</span></p><h4 id=the-williams-approach>The Williams approach</h4><p>In <a href=#Williams2011>[Williams2011]</a>, Dennis C. Williams improved this approach.
Firstly, he provided updated <span class="math inline">\(b_n\)</span> values for small <span class="math inline">\(n\)</span>:</p><table><thead><tr><th align=right>n</th><th align=right><span class="math inline">\(b_n\)</span> by Croux</th><th align=right><span class="math inline">\(b_n\)</span> by Williams</th></tr></thead><tbody><tr><td align=right>2</td><td align=right>1.196</td><td align=right>1.197</td></tr><tr><td align=right>3</td><td align=right>1.495</td><td align=right>1.490</td></tr><tr><td align=right>4</td><td align=right>1.363</td><td align=right>1.360</td></tr><tr><td align=right>5</td><td align=right>1.206</td><td align=right>1.217</td></tr><tr><td align=right>6</td><td align=right>1.200</td><td align=right>1.189</td></tr><tr><td align=right>7</td><td align=right>1.140</td><td align=right>1.138</td></tr><tr><td align=right>8</td><td align=right>1.129</td><td align=right>1.127</td></tr><tr><td align=right>9</td><td align=right>1.107</td><td align=right>1.101</td></tr></tbody></table><p>Secondly, he also introduced a small correction for the general equation:</p><p><span class="math display">\[b_n = \dfrac{n}{n-0.801}.
\]</span></p><p>Also, he discussed another kind of approximation equation for such kind of bias-correction factors:</p><p><span class="math display">\[b_n \cong 1 + cn^{-d}.
\]</span></p><p>In his paper, he applied the above equation only to <em>Shorth</em>
(which is the smallest interval that contains at least half of the data points),
but this approach can also be applied to other measures of scale.</p><h4 id=the-hayes-approach>The Hayes approach</h4><p>Next, in <a href=#Hayes2014>[Hayes2014]</a>, Kevin Hayes suggested another kind of prediction equation for <span class="math inline">\(n \geq 9\)</span>:</p><p><span class="math display">\[C_n = \dfrac{1}{\hat{a}_n}
\]</span></p><p>where</p><p><span class="math display">\[\hat{a}_n = \Phi^{-1}(3/4) \Bigg( 1 - \dfrac{\alpha}{n} - \dfrac{\beta}{n^2} \Bigg).
\]</span></p><p>Here are the suggested constants:</p><table><thead><tr><th align=right>n</th><th align=right><span class="math inline">\(\alpha\)</span></th><th align=right><span class="math inline">\(\beta\)</span></th></tr></thead><tbody><tr><td align=right>odd</td><td align=right>0.7635</td><td align=right>0.565</td></tr><tr><td align=right>even</td><td align=right>0.7612</td><td align=right>1.123</td></tr></tbody></table><h4 id=the-park-kim-wang-approach>The Park-Kim-Wang approach</h4><p>Finally, in <a href=#Park2020>[Park2020]</a>, Chanseok Park, Haewon Kim, and Min Wang aggregated all of the previous results.
They used the following form of the main equation:</p><p><span class="math display">\[C_n = \dfrac{1}{\Phi^{-1}(3/4) \cdot (1+A_n)}
\]</span></p><p>For <span class="math inline">\(n > 100\)</span>, they suggested two approaches.
The first one is based on <a href=#Hayes2014>[Hayes2014]</a> (the same equation for both odd and even <span class="math inline">\(n\)</span> values):</p><p><span class="math display">\[A_n = -\dfrac{0.76213}{n} - \dfrac{0.86413}{n^2}
\]</span></p><p>The second one is based on <a href=#Williams2011>[Williams2011]</a>:</p><p><span class="math display">\[A_n = -0.804168866 \cdot n^{-1.008922}
\]</span></p><p>Both approaches produce almost identical results, so it doesn&rsquo;t actually matter which one to use.</p><p>For <span class="math inline">\(2 \leq n \leq 100\)</span>, they suggested to use predefined constants:
(the below values are based on Table A2 from <a href=#Park2020>[Park2020]</a>):</p><table><thead><tr><th align=right>n</th><th align=right><span class="math inline">\(C_n\)</span></th><th align=right>n</th><th align=right><span class="math inline">\(C_n\)</span></th></tr></thead><tbody><tr><td align=right>1</td><td align=right>NA</td><td align=right>51</td><td align=right>1.505611</td></tr><tr><td align=right>2</td><td align=right>1.772150</td><td align=right>52</td><td align=right>1.505172</td></tr><tr><td align=right>3</td><td align=right>2.204907</td><td align=right>53</td><td align=right>1.504575</td></tr><tr><td align=right>4</td><td align=right>2.016673</td><td align=right>54</td><td align=right>1.504417</td></tr><tr><td align=right>5</td><td align=right>1.803927</td><td align=right>55</td><td align=right>1.503713</td></tr><tr><td align=right>6</td><td align=right>1.763788</td><td align=right>56</td><td align=right>1.503604</td></tr><tr><td align=right>7</td><td align=right>1.686813</td><td align=right>57</td><td align=right>1.503095</td></tr><tr><td align=right>8</td><td align=right>1.671843</td><td align=right>58</td><td align=right>1.502864</td></tr><tr><td align=right>9</td><td align=right>1.632940</td><td align=right>59</td><td align=right>1.502253</td></tr><tr><td align=right>10</td><td align=right>1.624681</td><td align=right>60</td><td align=right>1.502085</td></tr><tr><td align=right>11</td><td align=right>1.601308</td><td align=right>61</td><td align=right>1.501611</td></tr><tr><td align=right>12</td><td align=right>1.596155</td><td align=right>62</td><td align=right>1.501460</td></tr><tr><td align=right>13</td><td align=right>1.580754</td><td align=right>63</td><td align=right>1.501019</td></tr><tr><td align=right>14</td><td align=right>1.577272</td><td align=right>64</td><td align=right>1.500841</td></tr><tr><td align=right>15</td><td align=right>1.566339</td><td align=right>65</td><td align=right>1.500331</td></tr><tr><td align=right>16</td><td align=right>1.563769</td><td align=right>66</td><td align=right>1.500343</td></tr><tr><td align=right>17</td><td align=right>1.555284</td><td align=right>67</td><td align=right>1.499877</td></tr><tr><td align=right>18</td><td align=right>1.553370</td><td align=right>68</td><td align=right>1.499772</td></tr><tr><td align=right>19</td><td align=right>1.547206</td><td align=right>69</td><td align=right>1.499291</td></tr><tr><td align=right>20</td><td align=right>1.545705</td><td align=right>70</td><td align=right>1.499216</td></tr><tr><td align=right>21</td><td align=right>1.540681</td><td align=right>71</td><td align=right>1.498922</td></tr><tr><td align=right>22</td><td align=right>1.539302</td><td align=right>72</td><td align=right>1.498838</td></tr><tr><td align=right>23</td><td align=right>1.535165</td><td align=right>73</td><td align=right>1.498491</td></tr><tr><td align=right>24</td><td align=right>1.534053</td><td align=right>74</td><td align=right>1.498399</td></tr><tr><td align=right>25</td><td align=right>1.530517</td><td align=right>75</td><td align=right>1.497917</td></tr><tr><td align=right>26</td><td align=right>1.529996</td><td align=right>76</td><td align=right>1.497901</td></tr><tr><td align=right>27</td><td align=right>1.526916</td><td align=right>77</td><td align=right>1.497489</td></tr><tr><td align=right>28</td><td align=right>1.526422</td><td align=right>78</td><td align=right>1.497544</td></tr><tr><td align=right>29</td><td align=right>1.523608</td><td align=right>79</td><td align=right>1.497248</td></tr><tr><td align=right>30</td><td align=right>1.523031</td><td align=right>80</td><td align=right>1.497185</td></tr><tr><td align=right>31</td><td align=right>1.520732</td><td align=right>81</td><td align=right>1.496797</td></tr><tr><td align=right>32</td><td align=right>1.520333</td><td align=right>82</td><td align=right>1.496779</td></tr><tr><td align=right>33</td><td align=right>1.518509</td><td align=right>83</td><td align=right>1.496428</td></tr><tr><td align=right>34</td><td align=right>1.517941</td><td align=right>84</td><td align=right>1.496501</td></tr><tr><td align=right>35</td><td align=right>1.516279</td><td align=right>85</td><td align=right>1.496295</td></tr><tr><td align=right>36</td><td align=right>1.516070</td><td align=right>86</td><td align=right>1.496089</td></tr><tr><td align=right>37</td><td align=right>1.514425</td><td align=right>87</td><td align=right>1.495794</td></tr><tr><td align=right>38</td><td align=right>1.513989</td><td align=right>88</td><td align=right>1.495796</td></tr><tr><td align=right>39</td><td align=right>1.512747</td><td align=right>89</td><td align=right>1.495557</td></tr><tr><td align=right>40</td><td align=right>1.512418</td><td align=right>90</td><td align=right>1.495420</td></tr><tr><td align=right>41</td><td align=right>1.511078</td><td align=right>91</td><td align=right>1.495270</td></tr><tr><td align=right>42</td><td align=right>1.511041</td><td align=right>92</td><td align=right>1.495141</td></tr><tr><td align=right>43</td><td align=right>1.509858</td><td align=right>93</td><td align=right>1.494944</td></tr><tr><td align=right>44</td><td align=right>1.509499</td><td align=right>94</td><td align=right>1.494958</td></tr><tr><td align=right>45</td><td align=right>1.508529</td><td align=right>95</td><td align=right>1.494706</td></tr><tr><td align=right>46</td><td align=right>1.508365</td><td align=right>96</td><td align=right>1.494665</td></tr><tr><td align=right>47</td><td align=right>1.507535</td><td align=right>97</td><td align=right>1.494379</td></tr><tr><td align=right>48</td><td align=right>1.507247</td><td align=right>98</td><td align=right>1.494331</td></tr><tr><td align=right>49</td><td align=right>1.506382</td><td align=right>99</td><td align=right>1.494113</td></tr><tr><td align=right>50</td><td align=right>1.506307</td><td align=right>100</td><td align=right>1.494199</td></tr></tbody></table><p>Here is the corresponding plot:</p><div class=row><div class=mx-auto><a href=/posts/unbiased-mad/img/factors-light.png target=_blank class=imgldlink alt=factors><picture><source theme=dark srcset=/posts/unbiased-mad/img/factors-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/unbiased-mad/img/factors-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/unbiased-mad/img/factors-light.png></picture></a></div></div><br><h3 id=conclusion>Conclusion</h3><p>Currently, my tool-of-choice is the approach from <a href=#Park2020>[Park2020]</a>].
I verified all the predefined constants and equations from the paper using numerical simulations.
I can confirm that the suggested approach produces
a reliable estimate of the unbiased median absolute deviation <span class="math inline">\(\textrm{MAD}_n\)</span>.</p><h3 id=references>References</h3><ul><li><b id=Hampel1974>[Hampel1974]</b><br>Hampel, Frank R. &ldquo;The influence curve and its role in robust estimation.&rdquo; Journal of the american statistical association 69, no. 346 (1974): 383-393.<br><a href=https://doi.org/10.2307/2285666>https://doi.org/10.2307/2285666</a></li><li><b id=Croux1992>[Croux1992]</b><br>Croux, Christophe, and Peter J. Rousseeuw. &ldquo;Time-efficient algorithms for two highly robust estimators of scale.&ldquo;In Computational statistics, pp. 411-428. Physica, Heidelberg, 1992.<br><a href=https://doi.org/10.1007/978-3-662-26811-7_58>https://doi.org/10.1007/978-3-662-26811-7_58</a></li><li><b id=Williams2011>[Williams2011]</b><br>Williams, Dennis C. &ldquo;Finite sample correction factors for several simple robust estimators of normal standard deviation.&rdquo; Journal of Statistical Computation and Simulation 81, no. 11 (2011): 1697-1702.<br><a href=https://doi.org/10.1080/00949655.2010.499516>https://doi.org/10.1080/00949655.2010.499516</a></li><li><b id=Hayes2014>[Hayes2014]</b><br>Hayes, Kevin. &ldquo;Finite-sample bias-correction factors for the median absolute deviation.&rdquo; Communications in Statistics-Simulation and Computation 43, no. 10 (2014): 2205-2212.<br><a href=https://doi.org/10.1080/03610918.2012.748913>https://doi.org/10.1080/03610918.2012.748913</a></li><li><b id=Park2020>[Park2020]</b><br>Park, Chanseok, Haewon Kim, and Min Wang. &ldquo;Investigation of finite-sample properties of robust location and scale estimators.&rdquo; Communications in Statistics-Simulation and Computation (2020): 1-27.<br><a href=https://doi.org/10.1080/03610918.2019.1699114>https://doi.org/10.1080/03610918.2019.1699114</a></li></ul><br><br><div class=row><div class="justify-content-center share-block"><div class=share-title>Share:</div><div class=share-button><a href="https://www.reddit.com/submit?url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad%2f&title=Unbiased%20median%20absolute%20deviation" target=_blank title="Share on Reddit"><svg class="fai"><use xlink:href="/img/fa/all.svg#reddit"/></svg></a></div><div class=share-button><a href="https://twitter.com/intent/tweet?text=Unbiased%20median%20absolute%20deviation&url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad%2f&via=andrey_akinshin&related=andrey_akinshin" rel=nofollow target=_blank title="Share on Twitter"><svg class="fai"><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></div><div class=share-button><a href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad%2f" target=_blank title="Share on HackerNews"><svg class="fai"><use xlink:href="/img/fa/all.svg#hacker-news"/></svg></a></div><div class=share-button><a href="https://getpocket.com/save?url=https%3a%2f%2faakinshin.net%2fposts%2funbiased-mad%2f&title=Unbiased%20median%20absolute%20deviation" target=_blank title="Add to Pocket"><svg class="fai"><use xlink:href="/img/fa/all.svg#get-pocket"/></svg></a></div></div></div></div><hr></div></div></main></div><footer class="blog-footer mt-auto"><div class=container><p>&copy; 2013&mdash;2022 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><svg class="fai"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a><a href=https://twitter.com/andrey_akinshin><svg class="fai"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a><a href=https://aakinshin.net/posts/index.xml><svg class="fai"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a>|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.3.1/anchor.min.js integrity="sha512-zPB79j2C+3sFS9zcA3vg/z6bVKzJVEyu9pY5w89akQRys76zpAT2t6S3wZKla3QQ14O5l/Yt0RUQ/DHXx82Y5g==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script src=https://aakinshin.net/js/theme-after.min.29520a8e1176193da7ea4e6cb56cf9b3e634b867c9979234d8dafb5ab61dd494.js></script>
<script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>