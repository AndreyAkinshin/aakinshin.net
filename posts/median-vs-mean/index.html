<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.123.3"><meta name=author content='Andrey Akinshin'><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content='Mathematics,Statistics,Research,Hodges-Lehmann estimator'><title>Understanding the pitfalls of preferring the median over the mean | Andrey Akinshin</title>
<meta name=description content="We discuss the problem of selecting the right measure of central tendency in statistical analysis, discussing the pros and cons of the mean, median, and Hodges-Lehmann estimator"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><meta name=og:image content="https://aakinshin.net/posts/median-vs-mean/img/sampling_norm1-dark.png"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link href=https://aakinshin.net/css/main.min.d5a0830628a332736b49276c6d88c4bc429b98950014058537288a38d360b7e2.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body><nav><div class="container flex flex-wrap items-center justify-between mx-auto max-w-6xl h-12 px-2 xs:px-6"><div class="flex items-center justify-start justify-self-start"><div class="flex nav-item h-12 w-12 items-center justify-center"><a class=text-white href=https://aakinshin.net/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/posts/>Posts</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/research/>Research</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/library/>Library</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/about/>About</a></div></div><button id=theme-toggle type=button title="Alt+Click to match OS color theme" class="nav-item p-3 content-center justify-self-end"><svg id="theme-toggle-dark-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></div></nav><div class="main container mx-auto max-w-6xl px-6"><div class=main-post><h1 class=blog-post-title id=post-title>Understanding the pitfalls of preferring the median over the mean</h1><div class="flex flex-wrap justify-start items-center"><svg class="fai text-accent-l dark:text-front-d"><title>Date</title><use xlink:href="/img/fa/all.svg#calendar-days"/></svg>
<time datetime=2023-06-20>June 20, 2023</time><svg class="fai ml-3 text-accent-l dark:text-front-d"><title>Tags</title><use xlink:href="/img/fa/all.svg#tag"/></svg><div class="flex flex-wrap gap-y-1"><a class=label-link href=https://aakinshin.net/tags/mathematics/>Mathematics</a>
<a class=label-link href=https://aakinshin.net/tags/statistics/>Statistics</a>
<a class=label-link href=https://aakinshin.net/tags/research/>Research</a>
<a class=label-link href=https://aakinshin.net/tags/hodges-lehmann/>Hodges-Lehmann estimator</a></div></div><br><div class=main-content><p>A common task in mathematical statistics is to aggregate a set of numbers $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$
to a single &ldquo;average&rdquo; value.
Such a value is usually called <em>central tendency</em>.
There are multiple measures of central tendency.
The most popular one is the <em>arithmetic average</em> or the <em>mean</em>:</p>$$
\overline{\mathbf{x}} = \left( x_1 + x_2 + \ldots + x_n \right) / n.
$$<p>The mean is so popular not only thanks to its simplicity but also
because it provides the best way to estimate the center of the perfect normal distribution.
Unfortunately, the mean is not a robust measure.
This means that a single extreme value $x_i$ may distort the mean estimation and
lead to a non-reproducible value that has nothing in common with the &ldquo;expected&rdquo; central tendency.
The actual real-life distributions are never normal.
They can be pretty close to the normal distribution, but only to a certain extent.
Even small deviations from normality may produce occasional extreme outliers,
which makes the mean an unreliable measure in the general case.</p><p>When people discover the danger of the mean, they start looking for a more robust measure of the central tendency.
And the first obvious alternative is the sample median $\tilde{\mathbf{x}}$.
The classic sample median is easy to calculate.
First, you have to sort the sample.
If the sample size $n$ is odd, the median is the middle element in the sorted sample.
If $n$ is even, the median is the arithmetic average of the two middle elements in the sorted sample.
The median is extremely robust: it provides a reasonable estimate
even if almost half of the sample elements are corrupted.</p><p>For symmetric distributions (including the normal one), the true values of the mean and the median are the same.
Once we discover the high robustness of the median, it may be tempting to always use the median instead of the mean.
The median is often perceived as &ldquo;something like the mean but with high resistance to outliers.&rdquo;
Indeed, what is the point of using the unreliable mean, if the median always provides a safer choice?
Should we make the median our default option for the central tendency?</p><p>The answer is no.
You should beware of any default options in mathematical statistics.
All the measures are just tools, and each tool has its limitations and areas of applicability.
A mindless transition from the mean to the median, regardless of the underlying distribution, is not a smart move.
When we are picking a measure of central tendency to use,
the first step should be reviewing the research goals:
why do we need a measure of central tendency, and what are we going to do with the result?
It&rsquo;s impossible to make a rational decision on the statistical methods used without a clear understanding of the goals.
Next, we should match the goals to the properties of available measures.</p><p>There are multiple practical issues with the median,
but the most noticeable problem in practice is about its <em>statistical efficiency</em>.
Understanding this problem reveals the price of advanced robustness of the median.
In this post, we discuss the concept of statistical efficiency,
estimate the statistical efficiency of the mean and the median under different distributions,
and consider the Hodges-Lehman estimator as a measure of central tendency
that provides a better trade-off between robustness and efficiency.</p><h3 id=sampling-distribution>Sampling distribution</h3><p>The easiest way to start exploring the properties of a measure is to build its sampling distribution.
In order to do this, we should set an assumption on the actual underlying distribution of our data.
For simplicity, we start with the standard normal (Gaussian) distribution $\mathcal{N}(0, 1)$.
Next, we should generate multiple random samples from this underlying distribution,
calculate the estimation for each sample,
and build a new distribution based on these estimations.</p><p>Here are the sampling distributions of the mean and the median under the normal distribution:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/sampling_norm1-light.png target=_blank alt=sampling_norm1><img src=/posts/median-vs-mean/img/sampling_norm1-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/sampling_norm1-dark.png target=_blank alt=sampling_norm1><img src=/posts/median-vs-mean/img/sampling_norm1-dark.png width=800></a></div><p>And here are the sampling distributions under the standard uniform distribution $\mathcal{U}(0, 1)$.</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/sampling_unif1-light.png target=_blank alt=sampling_unif1><img src=/posts/median-vs-mean/img/sampling_unif1-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/sampling_unif1-dark.png target=_blank alt=sampling_unif1><img src=/posts/median-vs-mean/img/sampling_unif1-dark.png width=800></a></div><p>As we can see, the median sampling distribution can be noticeably wider than the mean sampling distribution
(or it has larger dispersion).
It is not a desired property of an estimator.
The wider the sampling distribution, the less repeatable estimation we observe.
In order to express the relation between sampling distribution dispersions among estimators,
we have to calculate statistical efficiency.</p><h3 id=statistical-efficiency>Statistical efficiency</h3><p>A common measure of statistical dispersion is the <em>variance</em>.
The <em>relative statistical efficiency</em> between two estimators is defined as the ratio
between variances of the corresponding sampling distributions.
When the underlying distribution is the normal one,
the best estimator for the central tendency is the mean:
it has statistical efficiency of $100\%$.
The statistical efficiency of other estimators under normality (also known as <em>Gaussian efficiency</em>)
is typically calculated using the mean as the baseline.
For an estimator $T$ (e.g., for the sample median $T=\tilde{X}$), it is defined as follows:</p>$$
\operatorname{eff}(T) = \frac{\mathbb{V}[\operatorname{mean}]}{\mathbb{V}[T]},
$$<p>where $\mathbb{V}[\cdot]$ is the variance of the sampling distribution for the given estimator.</p><p>It is important to distinguish the finite-sample efficiency ($n < \infty$) and the asymptotic efficiency ($n = \infty$).
While the case of $n = \infty$ is practically impossible,
the asymptotic efficiency provides an acceptable efficiency approximation for large samples.
However, when we consider small samples, it is important to consider the finite-sample efficiency separately
since it can noticeably differ from its asymptotic limit.</p><p>For the sample median, the asymptotic Gaussian efficiency is $\approx 64\%$.
The sample median relative efficiency to the mean under the uniform distribution is $\approx 34\%$.
The finite-sample efficiency values are presented in the below charts
(the presented estimation are not precise and has some noise, but the common trend is clear):</p><p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/eff_norm1-light.png target=_blank alt=eff_norm1><img src=/posts/median-vs-mean/img/eff_norm1-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/eff_norm1-dark.png target=_blank alt=eff_norm1><img src=/posts/median-vs-mean/img/eff_norm1-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/eff_unif1-light.png target=_blank alt=eff_unif1><img src=/posts/median-vs-mean/img/eff_unif1-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/eff_unif1-dark.png target=_blank alt=eff_unif1><img src=/posts/median-vs-mean/img/eff_unif1-dark.png width=800></a></div></p><p>As we can see, the robustness of the median doesn&rsquo;t come for free:
it has a price in the form of reduced statistical efficiency.
Therefore, if the underlying distribution is light-tailed and no huge outliers are expected,
switching to the sample median as a measure of central tendency is not always a good move.</p><p>It is worth noting that there are other measures of central tendency with other properties.
Let us say that we expect some outliers, but not so many of them (definitely, much less than $50\%$ of the sample).
We cannot use the mean since it can be corrupted by these outliers.
However, we don&rsquo;t want to use the sample median either since we will get a huge efficiency loss.
What should we do?
One of the possible solutions is the Hodges-Lehmann location estimator.</p><h3 id=hodges-lehmann-location-estimator>Hodges-Lehmann location estimator</h3><p>The Hodges-Lehmann location estimator (also known as pseudo-median) is a robust, non-parametric statistic
used as a measure of the central tendency.
For a sample $\mathbf{x} = \{ x_1, x_2, \ldots, x_n \}$, it is defined as the median of the Walsh (pairwise) averages:</p>$$
\operatorname{HL}(\mathbf{x}) =
\underset{1 \leq i \leq j \leq n}{\operatorname{median}} \left(\frac{x_i + x_j}{2} \right).
$$<p>Now let us compare the sampling distribution and the statistical efficiency of the Hodges-Lehmann location estimator
against the median and the mean:</p><p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/sampling_norm2-light.png target=_blank alt=sampling_norm2><img src=/posts/median-vs-mean/img/sampling_norm2-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/sampling_norm2-dark.png target=_blank alt=sampling_norm2><img src=/posts/median-vs-mean/img/sampling_norm2-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/sampling_unif2-light.png target=_blank alt=sampling_unif2><img src=/posts/median-vs-mean/img/sampling_unif2-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/sampling_unif2-dark.png target=_blank alt=sampling_unif2><img src=/posts/median-vs-mean/img/sampling_unif2-dark.png width=800></a></div></p><p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/eff_norm2-light.png target=_blank alt=eff_norm2><img src=/posts/median-vs-mean/img/eff_norm2-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/eff_norm2-dark.png target=_blank alt=eff_norm2><img src=/posts/median-vs-mean/img/eff_norm2-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/median-vs-mean/img/eff_unif2-light.png target=_blank alt=eff_unif2><img src=/posts/median-vs-mean/img/eff_unif2-light.png width=800>
</a><a class="img-dark hidden" href=/posts/median-vs-mean/img/eff_unif2-dark.png target=_blank alt=eff_unif2><img src=/posts/median-vs-mean/img/eff_unif2-dark.png width=800></a></div></p><p>As we can see, the Hodges-Lehmann is much closer to the mean than the median.
To be more specific, the asymptotic Gaussian efficiency of $\operatorname{HL}$ is $\approx 96\%$,
which is much better than $\approx 64\%$ for the sample median.
For the uniform distribution, the Gaussian efficiency of $\operatorname{HL}$ is also close to $1.0$,
while the sample median has an efficiency of $\approx 34\%$.</p><p>Meanwhile, the Hodges-Lehmann location estimator has decent robustness:
its asymptotic breakdown point is $\approx 29\%$.
This means that even $\approx 29\%$ of the sample is corrupted by outliers,
$\operatorname{HL}$ will still provide a reasonable estimate.
This makes $\operatorname{HL}$ a decent alternative to the sample median.</p><h3 id=conclusion>Conclusion</h3><p>In summary, choosing a measure of central tendency in mathematical statistics requires
careful analysis of the dataset and the research goals.
While the mean is simple and efficient, it&rsquo;s sensitive to outliers.
On the other hand, the median is robust but lacks statistical efficiency.
Thus, neither should be a default choice.
Instead, depending on the situation, using other options like the Hodges-Lehmann location estimator
could be better because it balances robustness and efficiency.</p></div><br><br></div></div><script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");const imagesDark=document.querySelectorAll(".img-dark"),imagesLight=document.querySelectorAll(".img-light");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?(themeToggleLightIcon.classList.remove("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")})):(themeToggleDarkIcon.classList.remove("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}));function toggleTheme(e){e?(document.documentElement.classList.add("dark"),themeToggleLightIcon.classList.remove("hidden"),themeToggleDarkIcon.classList.add("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")}),imagesLight.forEach(e=>{e.classList.add("hidden")})):(document.documentElement.classList.remove("dark"),themeToggleDarkIcon.classList.remove("hidden"),themeToggleLightIcon.classList.add("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}),imagesDark.forEach(e=>{e.classList.add("hidden")}))}themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),imagesLight.forEach(e=>{e.classList.toggle("hidden")}),imagesDark.forEach(e=>{e.classList.toggle("hidden")}),event.altKey?(localStorage.removeItem("color-theme"),toggleTheme(window.matchMedia("(prefers-color-scheme: dark)").matches)):localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))}),window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{if(localStorage.getItem("color-theme")===null){const t=e.matches?"dark":"light";toggleTheme(t==="dark")}})</script><footer class="z-20 p-4 md:p-6 mt-3 w-full border-t border-gray-200 shadow flex items-center justify-center dark:border-gray-600 bg-white dark:bg-zinc-800"><span class="dark:text-gray-400 text-center">© 2013—2024 <span class=whitespace-nowrap>Andrey Akinshin</span></span><ul class="flex flex-wrap items-center px-4 mb-1"><li><a href=https://github.com/AndreyAkinshin><svg class="fai fai-link w-5 h-5"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a></li><li><a href=https://twitter.com/andrey_akinshin><svg class="fai fai-link w-5 h-5"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></li><li><a href=https://aakinshin.net/posts/index.xml><svg class="fai fai-link w-5 h-5"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a></li></ul><div class=main-content><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+=" has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></body></html>