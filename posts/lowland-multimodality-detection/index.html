<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.123.3"><meta name=author content='Andrey Akinshin'><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content='Mathematics,Statistics,Research,Multimodality,Quantile-Respectful Density Estimation,Harrell-Davis Quantile Estimator'><title>Lowland multimodality detection | Andrey Akinshin</title>
<meta name=description content="I came up with a new algorithm for multimodality detection. On my data sets, it works much better than all the other approaches I tried."><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link href=https://aakinshin.net/css/main.min.507552e362c73186c7b6d8d093247a479099e01f527bcc3432990c78c2e426d8.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body><nav><div class="container flex flex-wrap items-center justify-between mx-auto max-w-6xl h-12 px-2 xs:px-6"><div class="flex items-center justify-start justify-self-start"><div class="flex nav-item h-12 w-12 items-center justify-center"><a class=text-white href=https://aakinshin.net/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/posts/>Posts</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/research/>Research</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/library/>Library</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/about/>About</a></div></div><button id=theme-toggle type=button title="Alt+Click to match OS color theme" class="nav-item p-3 content-center justify-self-end"><svg id="theme-toggle-dark-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></div></nav><div class="main container mx-auto max-w-6xl px-6"><div class=main-post><h1 class=blog-post-title id=post-title>Lowland Multimodality Detection</h1><div class="flex flex-wrap justify-start items-center"><svg class="fai text-accent-l dark:text-front-d"><title>Date</title><use xlink:href="/img/fa/all.svg#calendar-days"/></svg>
<time datetime=2020-11-03>November 3, 2020</time><svg class="fai ml-3 text-accent-l dark:text-front-d"><title>Tags</title><use xlink:href="/img/fa/all.svg#tag"/></svg><div class="flex flex-wrap gap-y-1"><a class=label-link href=https://aakinshin.net/tags/mathematics/>Mathematics</a>
<a class=label-link href=https://aakinshin.net/tags/statistics/>Statistics</a>
<a class=label-link href=https://aakinshin.net/tags/research/>Research</a>
<a class=label-link href=https://aakinshin.net/tags/multimodality/>Multimodality</a>
<a class=label-link href=https://aakinshin.net/tags/qrde/>Quantile-Respectful Density Estimation</a>
<a class=label-link href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/>Harrell-Davis Quantile Estimator</a></div></div><br><div class=main-content><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data5-light.svg target=_blank alt=data5><img src=/posts/lowland-multimodality-detection/img/data5-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data5-dark.svg target=_blank alt=data5><img src=/posts/lowland-multimodality-detection/img/data5-dark.svg width=800></a></div><p>Multimodality is an essential feature of a distribution, which may create many troubles during automatic analysis.
One of the best ways to work with such distributions is to detect all the modes in advance based on the given samples.
Unfortunately, this problem is much harder than it looks like.</p><p>I tried many different approaches for multimodality detection, but none of them was good enough.
During the past several years, my approach of choice was the <a href=http://www.brendangregg.com/FrequencyTrails/modes.html>mvalue-based modal test</a> by Brendan Gregg.
It works nicely in simple cases, but I was constantly stumbling over noisy samples where this algorithm doesn&rsquo;t produce reliable results.
Also, it has some limitations that make it unapplicable to some corner cases.</p><p>So, I needed a better approach.
Here are my main requirements:</p><ul><li>It should detect the exact mode locations and ranges</li><li>It should provide reliable results even on noisy samples</li><li>It should be able to detect multimodality even when some modes are extremely close to each other</li><li>It should work out of the box without tricky parameter tuning for each specific distribution</li></ul><p>I failed to find such an algorithm anywhere, so I came up with my own!
The current working title is &ldquo;the lowland multimodality detector.&rdquo;
It takes an estimation of the probability density function (PDF) and tries to find &ldquo;lowlands&rdquo; (areas that are much lower than neighboring peaks).
Next, it splits the plot by these lowlands and detects modes between them.
For the PDF estimation, it uses the <a href=https://aakinshin.net/posts/qrde-hd/>quantile-respectful density estimation based on the Harrell-Davis quantile estimator</a> (QRDE-HD).
Let me explain how it works in detail.</p><h3 id=the-problem>The problem</h3><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/riddle-light.svg target=_blank alt=riddle><img src=/posts/lowland-multimodality-detection/img/riddle-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/riddle-dark.svg target=_blank alt=riddle><img src=/posts/lowland-multimodality-detection/img/riddle-dark.svg width=800></a></div><p>How many modes do you see in the above image?
It&rsquo;s a trick question because it depends on the &ldquo;mode&rdquo; definition and on the kind of plot we have.
Here are some possible options:</p><ul><li><em>The plot is an exact probability density function; the mode is the global <a href=https://en.wikipedia.org/wiki/Maxima_and_minima>maxima</a>.</em><br>In this case, we have only a single mode (0.02).</li><li><em>The plot is an exact probability density function; the mode is a local maxima.</em><br>In this case, we have seven modes (-2.38, -1.11, 0.02, 0.41, 0.91, 1.61, 1.99).</li><li><em>The plot is an estimated probability density function based on a sample; the mode is a local maxima on the underlying distribution.</em><br>In this case, it&rsquo;s impossible to answer the question based on the given plot.
Meanwhile, the correct answer is &ldquo;one&rdquo;: the samples were collected from the normal distribution $\mathcal{N}(0, 1^2)$ (the picture presents a kernel density estimation with bandwidth = 0.1 based on the normal kernel).</li></ul><p>In this post, we will work with the most practical case.
Let&rsquo;s say that we have a sample, and we want to provide an estimation of the modes in the underlying distribution.
There is an important nuance here.
If we define the mode as the local maxima, we can easily get bunches of noisy modes that are close to each other,
but have no clear separation.
Typically, the true density of real-life distributions is very noisy and wobbly.</p><p>However, we are not interested in most of &ldquo;close to each other noisy modes&rdquo; in practice.
When we work with such distributions, we tend to make the density smoother.
We tend to reduce the impact of noise patterns, even if such patterns are integral parts of the original distribution.
We tend to group each bunch of nearby local maxima to a single mode.</p><p>During basic distribution exploration,
we are interested only in &ldquo;major&rdquo; modes that make sense to us from the practical point of view.
Here I have some good news and some bad news.
The bad news: we don&rsquo;t have a strict mathematical definition, so we can&rsquo;t verify and compare different approaches.
The good news: we are free to choose our own definition, which will be optimal for our use cases.</p><p>During my research, I often ask my colleagues: &ldquo;How many modes do you see in this plot?&rdquo;
Typically, I get identical answers for the same plot, which makes me think that there is a common understanding of this concept.
In this post, we focus on such &ldquo;practically significant&rdquo; modes which often look obvious for most people.</p><h3 id=the-mvalue-approach>The mvalue approach</h3><p>To get a better understanding of the problem, we start with the analysis of the mvalue approach.
It&rsquo;s described by Brendan Gregg in <a href=/posts/lowland-multimodality-detection/#Gregg2015>[Gregg2015]</a>.
It worth reading the original post, but I briefly describe the main idea.
Basically, the mvalue is the <a href=https://en.wikipedia.org/wiki/Total_variation>total variation</a> of a histogram or a density plot normalized by the plot height.
The modal test compares the mvalue with a predefined threshold and makes a decision about multimodality.
The easiest way to understand the idea is to just look at some examples.</p><p>Here is a visualization of a perfect unimodal distribution:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue1-light.svg target=_blank alt=mvalue1><img src=/posts/lowland-multimodality-detection/img/mvalue1-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue1-dark.svg target=_blank alt=mvalue1><img src=/posts/lowland-multimodality-detection/img/mvalue1-dark.svg width=800></a></div><p>Here we have three local extrema: the left corner point, the central peak, and the right corner point.
For each extremum, we should calculate the density value normalized by the global maxima.
For the left and right corner points, this value equals zero.
For the central peak, it equals one (because it&rsquo;s the highest peak).
The total variation is the sum of absolute differences between consecutive values.
In this case, it equals
(1.0 for ascent from the left corner point to the central peak) +
(1.0 for descent from the central peak to the right corner point).
Thus, the mvalue equals 2.
It&rsquo;s the minimum possible value of mvalue.</p><p>Here is a visualization of a perfect bimodal distribution:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue2-light.svg target=_blank alt=mvalue2><img src=/posts/lowland-multimodality-detection/img/mvalue2-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue2-dark.svg target=_blank alt=mvalue2><img src=/posts/lowland-multimodality-detection/img/mvalue2-dark.svg width=800></a></div><p>The mvalue equals 4 (1 for ascent + 1 for descent + 1 for ascent + 1 for descent).
It&rsquo;s the default value of the etalon bimodal distribution.</p><p>Here is a visualization of a perfect trimodal distribution:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue3-light.svg target=_blank alt=mvalue3><img src=/posts/lowland-multimodality-detection/img/mvalue3-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue3-dark.svg target=_blank alt=mvalue3><img src=/posts/lowland-multimodality-detection/img/mvalue3-dark.svg width=800></a></div><p>The mvalue equals 6 (1 for ascent + 1 for descent + 1 for ascent + 1 for descent + 1 for ascent + 1 for descent).
It&rsquo;s the default value of the etalon trimodal distribution.</p><p>You may think that we can get the number of modes by dividing mvalue by 2.
Unfortunately, it&rsquo;s not so simple.
Could you guess the mvalue of the below plot?</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue4a-light.svg target=_blank alt=mvalue4a><img src=/posts/lowland-multimodality-detection/img/mvalue4a-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue4a-dark.svg target=_blank alt=mvalue4a><img src=/posts/lowland-multimodality-detection/img/mvalue4a-dark.svg width=800></a></div><p>It equals 4:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue4b-light.svg target=_blank alt=mvalue4b><img src=/posts/lowland-multimodality-detection/img/mvalue4b-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue4b-dark.svg target=_blank alt=mvalue4b><img src=/posts/lowland-multimodality-detection/img/mvalue4b-dark.svg width=800></a></div><p>It&rsquo;s hard to make an unequivocal conclusion about modality here, but it&rsquo;s definitely not a bimodal distribution.</p><p>And what could you say about the next plot?</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue5a-light.svg target=_blank alt=mvalue5a><img src=/posts/lowland-multimodality-detection/img/mvalue5a-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue5a-dark.svg target=_blank alt=mvalue5a><img src=/posts/lowland-multimodality-detection/img/mvalue5a-dark.svg width=800></a></div><p>It equals 6:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/mvalue5b-light.svg target=_blank alt=mvalue5b><img src=/posts/lowland-multimodality-detection/img/mvalue5b-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/mvalue5b-dark.svg target=_blank alt=mvalue5b><img src=/posts/lowland-multimodality-detection/img/mvalue5b-dark.svg width=800></a></div><p>This distribution is most probably unimodal.
But because of the noise, the mvalue-based modal test thinks that it&rsquo;s a trimodal distribution.
Although mvalues allows detecting multimodal distributions in simple cases,
they have some serious disadvantages:</p><ul><li><strong>High false-positive rate</strong><br>Many noisy distributions may be considered as multimodal.</li><li><strong>No clear mode separation</strong><br>We get only a single number that expresses the measure of multimodality.
But we don&rsquo;t have the exact number of modes and their locations and ranges.</li><li><strong>Tricky tuning</strong><br>This approach provides too many degrees of freedom:<ul><li>We should choose the basis for mvalues: a histogram or a density plot.</li><li>If we use a histogram, we should choose the bandwidth and offset values
(which is <a href=https://aakinshin.net/posts/misleading-histograms/>not easy</a>).
If we use a kernel density estimation, we should choose the kernel bandwidth
(which is also <a href=https://aakinshin.net/posts/kde-bw/>not easy</a>)</li><li>We should manually remove outliers, so we have to choose an appropriate outlier detection algorithm.</li><li>We should specify adequate thresholds.
Brendan Gregg recommends using 2.4 as a signal that it worth manually investigate the distribution.
In <a href=https://github.com/AndreyAkinshin/perfolizer>perfolizer</a>, I currently use 2.8 as a lower threshold for multimodal distribution to reduce the false-positive rate.
It&rsquo;s hard to say which value is the best one in the general case.</li></ul></li></ul><p>Despite all these problems, this approach works and can be used in practice.
I have been using it in <a href=https://github.com/dotnet/BenchmarkDotNet>BenchmarkDotNet</a> for the past two years (it&rsquo;s available since v0.10.14), but there are still cases when it doesn&rsquo;t help to automatically make a reliable conclusion about multimodality.
So, I decided to find another approach that will work better.</p><h3 id=introducing-lowlands>Introducing lowlands</h3><p>Firstly, we will use the <a href=https://aakinshin.net/posts/qrde-hd/>QRDE-HD</a> instead of histograms and kernel density estimations.
The approach provides a much better estimation of the shape of the <em>collected data</em>, and it doesn&rsquo;t require parameter tuning.
Now imagine that this function describes a mountain relief (side view):</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data1-light.svg target=_blank alt=data1><img src=/posts/lowland-multimodality-detection/img/data1-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data1-dark.svg target=_blank alt=data1><img src=/posts/lowland-multimodality-detection/img/data1-dark.svg width=800></a></div><p>Next, it&rsquo;s starting to rain<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.
The rain fills the mountain hollows with water and forms a bunch of ponds.
The area of the mountain strictly below the ponds saturated with water.
Now let&rsquo;s introduce the following definitions:</p><ul><li><strong>Groundwater</strong>: the area below a pond saturated with water</li><li><strong>Shallow Water</strong>: a pond with an area <em>smaller</em> than the area of the corresponding underwater</li><li><strong>Deep Water</strong>: a pond with an area <em>larger</em> than the area of the corresponding underwater</li><li><strong>Lowland</strong>: a part of the mountain that is covered by a deep water<br>(we also assume that there are hidden lowlands on the right and on the left side of the visible mountain)</li><li><strong>Peak</strong>: a local maxima of the mountain relief (a point that is higher than its neighbors)</li><li><strong>Mode</strong>: the highest peak between two deep water ponds (including hidden ponds)</li></ul><p>For better understanding, let&rsquo;s see how this classification works in the bimodal case:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data2-light.svg target=_blank alt=data2><img src=/posts/lowland-multimodality-detection/img/data2-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data2-dark.svg target=_blank alt=data2><img src=/posts/lowland-multimodality-detection/img/data2-dark.svg width=800></a></div><p>We can do the following observations:</p><ul><li>Here we have five ponds.</li><li>Only the second pond consists of deep water (because its area is larger than the area of the corresponding underwater). It&rsquo;s the only lowland that we see here.</li><li>Also, we have six peaks: two on the left side of the central lowland and four on the right side.</li><li>The second peak is the first mode because it&rsquo;s the highest peak between the central lowland and the left hidden lowland.</li><li>The third peak is the second mode because it&rsquo;s the highest peak between the central lowland and the right hidden lowland.</li><li>It&rsquo;s a bimodal distribution because we discovered exactly two modes.</li></ul><p>I hope that you got the idea.
Now it&rsquo;s time to formalize this schema.</p><h3 id=lowland-detection-algorithm>Lowland detection algorithm</h3><p>To find the number of modes and their locations for the given samples, we should do the following:</p><ol><li>Build the <a href=https://aakinshin.net/posts/qrde-hd/>QRDE-HD</a></li><li>Find all local maxima of the QRDE-HD except border points (peaks)</li><li>Enumerate all the segments between neighboring peaks in ascending order of size</li><li>For each segment, we try to fill it with &ldquo;water.&rdquo;
The water level is determined by the lowest peak.
If the water area is larger than the area under the water, we mark it as &ldquo;lowland.&rdquo;
Once we found a lowland, we don&rsquo;t touch it anymore: it can&rsquo;t be flooded by a larger pond.
If the water area is smaller than the area under the water, this pond can be merged with other ponds.</li><li>Once we found all the lowland areas, we split the whole plot by them to not-lowland areas.</li><li>In each not-lowland area, we choose the highest peek and mark them as a mode.</li></ol><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data3-light.svg target=_blank alt=data3><img src=/posts/lowland-multimodality-detection/img/data3-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data3-dark.svg target=_blank alt=data3><img src=/posts/lowland-multimodality-detection/img/data3-dark.svg width=800></a></div><h3 id=extreme-case>Extreme case</h3><p>Now consider a case of a bimodal distribution where two modes are very close to each other but still strictly separated.
Let&rsquo;s take two huge samples from the uniform distribution $\{x_{1..1000}\}, \{y_{1..1000}\} \in \mathcal{U}(0, 1)$ and build a bimodal sample of the following form: $\{ -\delta - x^3_{1..1000}, +\delta +y^3_{1..1000} \}$.
It contains two modes ($-\delta$ and $+\delta$), and it doesn&rsquo;t include any sample elements between them.
The distance between modes equals $2\delta$.</p><p>Let&rsquo;s check out how the lowland detector works in such a case.
We start with $\delta = 0.5$:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data-close-05-light.svg target=_blank alt=data-close-05><img src=/posts/lowland-multimodality-detection/img/data-close-05-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data-close-05-dark.svg target=_blank alt=data-close-05><img src=/posts/lowland-multimodality-detection/img/data-close-05-dark.svg width=800></a></div><p>It wasn&rsquo;t so hard to detect two modes here.
Let&rsquo;s try $\delta = 0.1$:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data-close-01-light.svg target=_blank alt=data-close-01><img src=/posts/lowland-multimodality-detection/img/data-close-01-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data-close-01-dark.svg target=_blank alt=data-close-01><img src=/posts/lowland-multimodality-detection/img/data-close-01-dark.svg width=800></a></div><p>Not bad.
Now let&rsquo;s try an extreme case: $\delta = 0.01$.</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data-close-001-light.svg target=_blank alt=data-close-001><img src=/posts/lowland-multimodality-detection/img/data-close-001-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data-close-001-dark.svg target=_blank alt=data-close-001><img src=/posts/lowland-multimodality-detection/img/data-close-001-dark.svg width=800></a></div><p>We are still able to detect bimodality here!
Note that it&rsquo;s only possible thanks to the <a href=https://aakinshin.net/posts/qrde-hd/>QRDE-HD</a>.
If we use classic histograms or kernel density estimations here, we will not be able to see multimodality:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data-close-001-comparison-light.svg target=_blank alt=data-close-001-comparison><img src=/posts/lowland-multimodality-detection/img/data-close-001-comparison-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data-close-001-comparison-dark.svg target=_blank alt=data-close-001-comparison><img src=/posts/lowland-multimodality-detection/img/data-close-001-comparison-dark.svg width=800></a></div><h3 id=more-examples>More examples</h3><p>Below you can find some additional cases of applying this algorithm to multimodal distributions.</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data4-light.svg target=_blank alt=data4><img src=/posts/lowland-multimodality-detection/img/data4-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data4-dark.svg target=_blank alt=data4><img src=/posts/lowland-multimodality-detection/img/data4-dark.svg width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data6-light.svg target=_blank alt=data6><img src=/posts/lowland-multimodality-detection/img/data6-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data6-dark.svg target=_blank alt=data6><img src=/posts/lowland-multimodality-detection/img/data6-dark.svg width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data7-light.svg target=_blank alt=data7><img src=/posts/lowland-multimodality-detection/img/data7-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data7-dark.svg target=_blank alt=data7><img src=/posts/lowland-multimodality-detection/img/data7-dark.svg width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data8-light.svg target=_blank alt=data8><img src=/posts/lowland-multimodality-detection/img/data8-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data8-dark.svg target=_blank alt=data8><img src=/posts/lowland-multimodality-detection/img/data8-dark.svg width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data9-light.svg target=_blank alt=data9><img src=/posts/lowland-multimodality-detection/img/data9-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data9-dark.svg target=_blank alt=data9><img src=/posts/lowland-multimodality-detection/img/data9-dark.svg width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/lowland-multimodality-detection/img/data10-light.svg target=_blank alt=data10><img src=/posts/lowland-multimodality-detection/img/data10-light.svg width=800>
</a><a class="img-dark hidden" href=/posts/lowland-multimodality-detection/img/data10-dark.svg target=_blank alt=data10><img src=/posts/lowland-multimodality-detection/img/data10-dark.svg width=800></a></div><h3 id=implementation-notes>Implementation notes</h3><p>You can find a reference C# implementation of this algorithm in
the latest nightly version (0.3.0-nightly.54+) of <a href=https://github.com/AndreyAkinshin/perfolizer>perfolizer</a>
(you need <a href=https://github.com/AndreyAkinshin/perfolizer/blob/dc9d9a3997c0b395575cff06ec8af2a98b3b2bb7/src/Perfolizer/Perfolizer/Mathematics/Multimodality/LowlandModalityDetector.cs><code>LowlandModalityDetector</code></a>).
As I said in the beginning, it doesn&rsquo;t require any parameter tuning.
It should just work without any adjustments.
However, there are a few points of customization that you can use to adopt this algorithm to some very specific corner cases.</p><ul><li><strong>Sensitivity</strong><br>By default, we mark a pond as deep water when its area is larger than the area of the groundwater beneath it.
We can express this via the following equations: <code>Area(DeepWater) > 0.5 * (Area(DeepWater)+Area(Groundwater))</code>.
Let&rsquo;s call <code>0.5</code> in this equation <em>sensitivity</em>.
It defines how large the water area should be to be considered as deep water.
By manipulating this parameter, you can tune how sensitive the algorithm is.
However, based on my experience, <code>0.5</code> looks like an optimal value.</li><li><strong>Precision</strong><br>It&rsquo;s typically enough to calculate 101 quantile values (99 percentiles + minimum + maximum) to build the QRDE-HD as a step function.
However, if you expect an extremely large number of modes
(e.g., 200 modes and extremely large data sets that clearly highlights all of these modes),
you can increase the number of evaluated quantiles.
Note that it may significantly reduce the algorithm performance.</li><li><strong>PDF</strong><br>I suggest using the QRDE-HD to estimate the PDF.
However, other kinds of estimations may be considered.
I do not recommend using classic histograms and kernel density estimations because of the smoothness problems.
However, it may make sense to experiment with other kinds of the PDF estimators (e.g., QRDE based on other smooth quantile estimators).
In general, the algorithm supports any kind of density plots or histograms.</li></ul><h3 id=conclusion>Conclusion</h3><p>The suggested algorithm allows detecting multimodality phenomena based on the given sample.
You can try to use it via <a href=https://github.com/AndreyAkinshin/perfolizer>perfolizer</a> or implement it yourself.
Here are some of the main advantages of this approach:</p><ul><li><strong>Detailing</strong><br>It not only detects the exact number of modes but also provides their exact locations.</li><li><strong>Just works without tuning</strong><br>It works out of the box, and it doesn&rsquo;t require parameter tuning</li><li><strong>Close peak support</strong><br>It allows detecting modes even when they are extremely close to each other</li><li><strong>Robustness and reliability</strong><br>It&rsquo;s pretty robust, and it works reliably even on noisy samples (at least, on my data sets ☺)</li><li><strong>Natural results</strong><br>The set of detected modes matches thoughts of real people when they manually explore distribution plots</li></ul><p>If you decide to try this algorithm on your data sets, I will be happy to get your feedback!</p><h3 id=references>References</h3><ul><li><b id=Gregg2015>[Gregg2015]</b><br>B. Gregg, 2015.
Frequency Trails: Modes and Modality.<br><a href=http://www.brendangregg.com/FrequencyTrails/modes.html>http://www.brendangregg.com/FrequencyTrails/modes.html</a></li><li><b id=Harrell1982>[Harrell1982]</b><br>Harrell, F.E. and Davis, C.E., 1982. A new distribution-free quantile estimator.
<em>Biometrika</em>, 69(3), pp.635-640.<br><a href=https://pdfs.semanticscholar.org/1a48/9bb74293753023c5bb6bff8e41e8fe68060f.pdf>https://pdfs.semanticscholar.org/1a48/9bb74293753023c5bb6bff8e41e8fe68060f.pdf</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Inspired by the problem &ldquo;Rain&rdquo; from XIV All-Russian Olympiad for schoolchildren in computer science a.k.a. ROI-2002 (Day 1, Problem 2).
The problem description can be found <a href=https://neerc.ifmo.ru/school/archive/2001-2002.html>here</a> (In Russian). The problem solution can be found <a href=http://svgimnazia1.grodno.by/sinica/Book_ABC/Book_ABC_pascal/olimp_resh/olimp_resh55.htm>here</a> (In Russian).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><br><br></div></div><script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");const imagesDark=document.querySelectorAll(".img-dark"),imagesLight=document.querySelectorAll(".img-light");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?(themeToggleLightIcon.classList.remove("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")})):(themeToggleDarkIcon.classList.remove("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}));function toggleTheme(e){e?(document.documentElement.classList.add("dark"),themeToggleLightIcon.classList.remove("hidden"),themeToggleDarkIcon.classList.add("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")}),imagesLight.forEach(e=>{e.classList.add("hidden")})):(document.documentElement.classList.remove("dark"),themeToggleDarkIcon.classList.remove("hidden"),themeToggleLightIcon.classList.add("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}),imagesDark.forEach(e=>{e.classList.add("hidden")}))}themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),imagesLight.forEach(e=>{e.classList.toggle("hidden")}),imagesDark.forEach(e=>{e.classList.toggle("hidden")}),event.altKey?(localStorage.removeItem("color-theme"),toggleTheme(window.matchMedia("(prefers-color-scheme: dark)").matches)):localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))}),window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{if(localStorage.getItem("color-theme")===null){const t=e.matches?"dark":"light";toggleTheme(t==="dark")}})</script><footer class="z-20 p-4 md:p-6 mt-3 w-full border-t border-gray-200 shadow flex items-center justify-center dark:border-gray-600 bg-white dark:bg-zinc-800"><span class="dark:text-gray-400 text-center">© 2013—2024 <span class=whitespace-nowrap>Andrey Akinshin</span></span><ul class="flex flex-wrap items-center px-4 mb-1"><li><a href=https://github.com/AndreyAkinshin><svg class="fai fai-link w-5 h-5"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a></li><li><a href=https://twitter.com/andrey_akinshin><svg class="fai fai-link w-5 h-5"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></li><li><a href=https://aakinshin.net/posts/index.xml><svg class="fai fai-link w-5 h-5"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a></li></ul><div class=main-content><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+=" has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></body></html>