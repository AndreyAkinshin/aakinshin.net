<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.123.7"><meta name=author content='Andrey Akinshin'><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content='Mathematics,Statistics,normality,Central Limit Theorem'><title>Normality is a myth | Andrey Akinshin</title>
<meta name=description content="In many statistical papers, you can find the following phrase: &amp;ldquo;assuming that we have a normal distribution.&amp;rdquo; Probably, you saw plots of the normal distribution density function in some statistics textbooks, it looks like this:
The normal distri..."><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link href=https://aakinshin.net/css/main.min.d6826f88345af20c0769a313557ec1355465a3091b1f9869b936f2504c4dfe26.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body class="flex flex-col min-h-screen"><nav><div class="container flex flex-wrap items-center justify-between mx-auto max-w-6xl h-12 px-2 xs:px-6"><div class="flex items-center justify-start justify-self-start"><div class="flex nav-item h-12 w-12 items-center justify-center"><a class=text-white href=https://aakinshin.net/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/posts/>Posts</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/research/>Research</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/library/>Library</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/about/>About</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/search/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#magnifying-glass"/></svg></a></div></div><button id=theme-toggle type=button title="Alt+Click to match OS color theme" class="nav-item p-3 content-center justify-self-end"><svg id="theme-toggle-dark-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></div></nav><div class="main container mx-auto max-w-6xl px-6 flex-grow"><div class=main-post><h1 class=blog-post-title id=post-title>Normality is a myth</h1><div class="flex flex-wrap justify-start items-center"><svg class="fai text-accent-l dark:text-front-d"><title>Date</title><use xlink:href="/img/fa/all.svg#calendar-days"/></svg>
<time datetime=2019-10-09>October 9, 2019</time><svg class="fai ml-3 text-accent-l dark:text-front-d"><title>Tags</title><use xlink:href="/img/fa/all.svg#tag"/></svg><div class="flex flex-wrap gap-y-1"><a class=label-link href=https://aakinshin.net/tags/mathematics/><svg class="rating-icon"><use xlink:href="/img/fa/all.svg#math"/></svg>
Mathematics
</a><a class=label-link href=https://aakinshin.net/tags/statistics/><svg class="rating-icon"><use xlink:href="/img/fa/all.svg#statistics"/></svg>
Statistics
</a><a class=label-link href=https://aakinshin.net/tags/normality/>normality
</a><a class=label-link href=https://aakinshin.net/tags/central-limit-theorem/>Central Limit Theorem</a></div></div><br><div class=main-content><p>In many statistical papers, you can find the following phrase: &ldquo;assuming that we have a normal distribution.&rdquo;
Probably, you saw plots of the normal distribution density function in some statistics textbooks,
it looks like this:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/normality/img/normal-light.png target=_blank alt=normal><img src=/posts/normality/img/normal-light.png width=800>
</a><a class="img-dark hidden" href=/posts/normality/img/normal-dark.png target=_blank alt=normal><img src=/posts/normality/img/normal-dark.png width=800></a></div><p>The normal distribution is a pretty user-friendly mental model when we are trying to interpret the statistical metrics
like mean and standard deviation.
However, it may also be an insidious and misleading model when your distribution is not normal.
There is a great sentence in the <a href=https://doi.org/10.1093/biomet/34.3-4.209>&ldquo;Testing for normality&rdquo;</a> paper by R.C. Geary, 1947 (the quote was found <a href=https://garstats.wordpress.com/2019/06/17/myth/>here</a>):</p><blockquote><p>Normality is a myth; there never was, and never will be, a normal distribution.</p></blockquote><p>I 100% agree with this statement.
At least, if you are working with performance distributions
(that are based on the multiple iterations of your benchmarks that measure the performance metrics of your applications),
you should forget about normality.
That&rsquo;s how a typical performance distribution looks like
(I built the below picture based on a real benchmark that measures the load time of assemblies
when we open the <a href=https://github.com/OrchardCMS/Orchard>Orchard</a> solution in <a href=https://www.jetbrains.com/rider/>Rider</a> on Linux):</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/normality/img/performance-light.png target=_blank alt=performance><img src=/posts/normality/img/performance-light.png width=800>
</a><a class="img-dark hidden" href=/posts/normality/img/performance-dark.png target=_blank alt=performance><img src=/posts/normality/img/performance-dark.png width=800></a></div><p>Of course, <em>some</em> of the performance distributions look similar to the normal distribution.
And you can apply statistical approaches that assume normality to such distributions.
And these approaches may provide correct results in many cases.
If you are working with a single benchmark and manually check the results,
you will be probably lucky enough and get correct results which will strengthen your faith to the fact
that it&rsquo;s OK to use such approaches all the time.
However, if you are working with thousands of performance tests and you are trying to use such approaches automatically,
you will probably get wrong results for some of these tests <em>all the time</em>.</p><p>The worst thing about the real performance distributions (that are often right-skewed and multimodal) is that
it makes all of the statistical metrics (like the mean) misleading.
Let&rsquo;s look at an example of <a href=https://github.com/dotnet/BenchmarkDotNet>BenchmarkDotNet</a> summary table:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-md data-lang=md><span class=line><span class=cl>| Method |     Mean |    Error |   StdDev |   Median |
</span></span><span class=line><span class=cl>|------- |---------:|---------:|---------:|---------:|
</span></span><span class=line><span class=cl>|      A | 136.2 ms | 19.30 ms | 56.92 ms | 107.0 ms |
</span></span><span class=line><span class=cl>|      B | 133.7 ms |  4.14 ms | 12.20 ms | 130.2 ms |
</span></span></code></pre></div><p>From my observations, when the Mean value is presented,
people <em>usually</em> read only the Mean column and ignore other columns.
After that, they make conclusions only based on the Mean values and the normal distribution mental model.
Thus, they may think that <code>B</code> always works a little bit faster than <code>A</code> in the above table.
Now let&rsquo;s look at the &ldquo;Expectation vs. Reality&rdquo; picture for the <code>A</code> and <code>B</code> density plots:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/normality/img/misleading-light.png target=_blank alt=misleading><img src=/posts/normality/img/misleading-light.png width=800>
</a><a class="img-dark hidden" href=/posts/normality/img/misleading-dark.png target=_blank alt=misleading><img src=/posts/normality/img/misleading-dark.png width=800></a></div><p>As you can see, both distributions are heavy-tailed right-skewed distributions.
<code>A</code> has very huge outliers (that spoiled the mean value),
but most of the <code>B</code> quantiles are larger than the corresponding <code>A</code> quantiles.
BenchmarkDotNet automatically added the <code>Median</code> column to highlight this fact
(the median value for <code>B</code> is much larger than the median value for <code>A</code>).
Unfortunately, most of the people ignore such kind of additional information and
continue to imagine a normal distribution based on the mean value.</p><p>There is one more thing in perception of statistics.
When I tell people about complex multimodal heavy-tailed right-skewed distributions,
they usually reply that we have the
<a href=https://en.wikipedia.org/wiki/Central_limit_theorem>Central Limit Theorem</a>.
For some reason, people think that it will magically stabilize the mean value and
rescue us from multimodal distributions if we collect &ldquo;enough&rdquo; measurements.
Here is the definition of this theorem from <a href=https://www.dummies.com/education/math/statistics/how-the-central-limit-theorem-is-used-in-statistics/>Statistics For Dummies</a>:</p><blockquote><p>The Central Limit Theorem (CLT for short) basically says that for non-normal data, the distribution of the sample means has an approximate normal distribution, no matter what the distribution of the original data looks like, as long as the sample size is large enough (usually at least 30) and all samples have the same size.</p></blockquote><p>So, if we take many samples (sets of measurements), calculate the mean value for each sample,
and build a distribution from these mean values, it should be normal.
If you are not sure that you understand the Central Limit Theorem, I you can watch this video
that explains it with Bunnies & Dragons (the original was found <a href=https://blog.minitab.com/blog/michelle-paret/explaining-the-central-limit-theorem-with-bunnies-and-dragons-v2>here</a>):</p><div class=text-center><iframe width=800 height=450 src=https://www.youtube.com/embed/jvoxEYmQHNM frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div><p>Let&rsquo;s check how it works in practice.
I wrote a simple R script that emulates taking measurements from a &ldquo;strange&rdquo; distribution with high outliers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>library</span><span class=p>(</span><span class=n>ggplot2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>&lt;-</span> <span class=m>30</span> <span class=c1># Number of values in each sample</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>&lt;-</span> <span class=m>30</span> <span class=c1># Number of samples</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>&lt;-</span> <span class=m>16</span> <span class=c1># Number of CLT distributions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nf>set.seed</span><span class=p>(</span><span class=m>159</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Generate a single random value from a &#34;strange&#34; distribution</span>
</span></span><span class=line><span class=cl><span class=n>gen.value</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=nf>sample</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>,</span> <span class=m>1</span><span class=p>)</span> <span class=o>+</span>                               <span class=c1># Offset</span>
</span></span><span class=line><span class=cl>  <span class=nf>rbeta</span><span class=p>(</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>,</span> <span class=m>10</span><span class=p>)</span> <span class=o>*</span> <span class=nf>sample</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>,</span> <span class=m>1</span><span class=p>)</span> <span class=o>+</span>             <span class=c1># Right-skewed distribution</span>
</span></span><span class=line><span class=cl>  <span class=nf>sample</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=nf>rep</span><span class=p>(</span><span class=m>0</span><span class=p>,</span> <span class=m>50</span><span class=p>),</span> <span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>))</span> <span class=o>*</span> <span class=nf>rnorm</span><span class=p>(</span><span class=m>1</span><span class=p>,</span> <span class=m>200</span><span class=p>,</span> <span class=m>10</span><span class=p>)</span> <span class=c1># Outliers</span>
</span></span><span class=line><span class=cl><span class=c1># Generate a sample mean</span>
</span></span><span class=line><span class=cl><span class=n>gen.mean</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>()</span> <span class=nf>mean</span><span class=p>(</span><span class=nf>sapply</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=n>n</span><span class=p>,</span> <span class=kr>function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=nf>gen.value</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>&lt;-</span> <span class=nf>data.frame</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=kr>for</span> <span class=p>(</span><span class=n>i</span> <span class=kr>in</span> <span class=m>1</span><span class=o>:</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>df</span> <span class=o>&lt;-</span> <span class=nf>rbind</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=nf>data.frame</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>Experiment</span> <span class=o>=</span> <span class=nf>rep</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>m</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Time</span> <span class=o>=</span> <span class=nf>sapply</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=n>m</span><span class=p>,</span> <span class=kr>function</span><span class=p>(</span><span class=n>j</span><span class=p>)</span> <span class=nf>gen.mean</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>&lt;-</span> <span class=nf>ggplot</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=nf>aes</span><span class=p>(</span><span class=n>x</span> <span class=o>=</span> <span class=n>Time</span><span class=p>,</span> <span class=n>group</span> <span class=o>=</span> <span class=n>Experiment</span><span class=p>))</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>  <span class=nf>geom_density</span><span class=p>(</span><span class=n>fill</span> <span class=o>=</span> <span class=s>&#34;red&#34;</span><span class=p>,</span> <span class=n>alpha</span> <span class=o>=</span> <span class=m>0.4</span><span class=p>,</span> <span class=n>bw</span> <span class=o>=</span> <span class=s>&#34;SJ&#34;</span><span class=p>)</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>  <span class=nf>facet_wrap</span><span class=p>(</span><span class=o>~</span><span class=n>Experiment</span><span class=p>)</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>  <span class=nf>ylab</span><span class=p>(</span><span class=s>&#34;Density&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>In this script,
<code>gen.value</code> returns a random value from our &ldquo;strange&rdquo; distribution,
<code>gen.mean</code> generates a sample with 30 measurements (because <a href=https://stats.stackexchange.com/q/2541/261747>30 is a magic number</a>)
and returns the mean value of this sample.
Next, we perform 16 experiments.
In each experiment, we draw a distribution density function based on 30 mean values that we generated.
Here is the result:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/normality/img/clt-light.png target=_blank alt=clt><img src=/posts/normality/img/clt-light.png width=800>
</a><a class="img-dark hidden" href=/posts/normality/img/clt-dark.png target=_blank alt=clt><img src=/posts/normality/img/clt-dark.png width=800></a></div><p>As you can see, many of the generated distribution are not looking as normal.
What&rsquo;s wrong?
Maybe the Central Limit Theorem doesn&rsquo;t work?
Don&rsquo;t worry, there are no problems with the theorem.
We just didn&rsquo;t take enough measurements.
If we increase the sample sizes (<code>n</code> and <code>m</code> parameters in the above R script), the plots will be &ldquo;fixed&rdquo;:
the observed density plots will become &ldquo;more normal.&rdquo;</p><p>Now let&rsquo;s think about the sample sizes.
In each experiment, we draw a density function based on 900 measurements
(we have 30 mean values; each value was calculated based on a sample with 30 measurements).
The script works pretty fast because it generates &ldquo;fake&rdquo; data.
In real life, we typically spend at least 1 second per measurement.
If we want to build such a plot based on the real data,
we will spend 900 seconds (or 15 minutes) per an experiment.
Some of the real benchmarks may take more than 1 minute per measurement which means that
we should spend <em>15 hours</em> for the whole experiment.
In theory, if we spend a lot of hours on the measurements,
we will most likely get a plot which will be similar to the normal distribution.
In practice, we will not spend so much time per each benchmark
(especially, if we have hundreds or thousands of them).</p><p>At the conclusion, I want to highlight some important facts about the Central Limit Theorem
that people usually don&rsquo;t understand
(based on the &ldquo;The Central Limit Theorem&rdquo; section from <a href=/prodotnetbenchmarking/>my book</a>):</p><ul><li>If we do many iterations, the original distribution will not become normal,
and we can&rsquo;t interpret the mean, the variance, the skewness, and the kurtosis as in the case of normal distribution.</li><li>The range of the mean values across all samples is not always narrow,
we still can have a huge difference between the mean values in different samples.
The normal distribution based on the mean values has its own standard deviation
which depends on the sample size and can be expressed via the standard error.</li><li>The central limit theorem doesn&rsquo;t work correctly when the sample sizes are small.
For example, if you make a single measurement in each sample,
the distribution based on the mean values will have the same shape as the original distribution.</li><li>If we take a small number of samples ($n < 100$),
we will probably not see a normal distribution on the density plot for mean values.</li></ul><p>If we are speaking about the original performance distributions,
we can forget about normality at all.
The assumption of normality can work fine in some special cases,
but it will let you down in the long run.
Remember the R.C. Geary&rsquo;s words: <em>&ldquo;Normality is a myth; there never was, and never will be, a normal distribution.&rdquo;</em></p></div></div></div><script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");const imagesDark=document.querySelectorAll(".img-dark"),imagesLight=document.querySelectorAll(".img-light");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?(themeToggleLightIcon.classList.remove("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")})):(themeToggleDarkIcon.classList.remove("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}));function toggleTheme(e){e?(document.documentElement.classList.add("dark"),themeToggleLightIcon.classList.remove("hidden"),themeToggleDarkIcon.classList.add("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")}),imagesLight.forEach(e=>{e.classList.add("hidden")})):(document.documentElement.classList.remove("dark"),themeToggleDarkIcon.classList.remove("hidden"),themeToggleLightIcon.classList.add("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}),imagesDark.forEach(e=>{e.classList.add("hidden")}))}themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),imagesLight.forEach(e=>{e.classList.toggle("hidden")}),imagesDark.forEach(e=>{e.classList.toggle("hidden")}),event.altKey?(localStorage.removeItem("color-theme"),toggleTheme(window.matchMedia("(prefers-color-scheme: dark)").matches)):localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))}),window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{if(localStorage.getItem("color-theme")===null){const t=e.matches?"dark":"light";toggleTheme(t==="dark")}})</script><footer class="z-20 p-4 md:p-6 mt-3 w-full border-t border-gray-200 shadow flex items-center justify-center dark:border-gray-600 bg-white dark:bg-zinc-800"><span class="dark:text-gray-400 text-center">© 2013—2024 <span class=whitespace-nowrap>Andrey Akinshin</span></span><ul class="flex flex-wrap items-center px-4 mb-1"><li><a href=https://github.com/AndreyAkinshin><svg class="fai fai-link w-5 h-5"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a></li><li><a href=https://twitter.com/andrey_akinshin><svg class="fai fai-link w-5 h-5"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></li><li><a href=https://aakinshin.net/posts/index.xml><svg class="fai fai-link w-5 h-5"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a></li></ul><div class=main-content><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+=" has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></body></html>