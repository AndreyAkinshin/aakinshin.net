<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.74.3"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><title>Normality is a myth | Andrey Akinshin</title><link href=/css/lumen-bootstrap.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/slate-bootstrap.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><link href=/css/syntax-light.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/syntax-dark.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.26d61b7027fe55e642f0001cef94cb75b567d721086492aab6b4e32d9ee7811d.js></script><script type=module src=https://googlechromelabs.github.io/dark-mode-toggle/src/dark-mode-toggle.mjs></script><link href=https://aakinshin.net/css/fontawesome-all.min.e78467baec0179d7ccd2ef995e0c94f25b4a63a3191a93c3547e22bdf590ef5d.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.2a76db7cc14055b3bafacc363f453917ab54a818a7db2a0a9b2409c0f6bf07c5.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.a9ad336a30411929ac042537ef03ddbe3b2b28af5aa50655711f870419408467.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-ZVB6MXSX32');</script><script type=text/javascript>(function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym");ym(28700916,"init",{clickmap:true,trackLinks:true,accurateTrackBounce:true});</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-lg navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/posts/>Posts</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/pages/prodotnetbenchmarking/>Pro .NET Benchmarking</a></li><li class=nav-item></li><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub style=color:#fff></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter style=color:#fff></i></a></li><li class=nav-item><a class=nav-link href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS style=color:#fff></i></a></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class=blog-post><h2 class=blog-post-title id=post-title>Normality is a myth</h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2019-10-09>October 9, 2019</time>
&nbsp;&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/normality/ class="badge badge-info">Normality</a>
<a href=https://aakinshin.net/tags/central-limit-theorem/ class="badge badge-info">Central Limit Theorem</a>
<a href=https://aakinshin.net/tags/performance/ class="badge badge-info">Performance</a>
<a href=https://aakinshin.net/tags/r/ class="badge badge-info">R</a></span><br><br><p>In many statistical papers, you can find the following phrase: &ldquo;assuming that we have a normal distribution.&rdquo;
Probably, you saw plots of the normal distribution density function in some statistics textbooks,
it looks like this:</p><div class=mx-auto><a href=/posts/normality/img/normal-light.png target=_blank><picture>
<source theme=dark srcset=/posts/normality/img/normal-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/normality/img/normal-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/normality/img/normal-light.png></picture></a></div><br><p>The normal distribution is a pretty user-friendly mental model when we are trying to interpret the statistical metrics
like mean and standard deviation.
However, it may also be an insidious and misleading model when your distribution is not normal.
There is a great sentence in the <a href=https://doi.org/10.1093/biomet/34.3-4.209>&ldquo;Testing for normality&rdquo;</a> paper by R.C. Geary, 1947 (the quote was found <a href=https://garstats.wordpress.com/2019/06/17/myth/>here</a>):</p><blockquote><p>Normality is a myth; there never was, and never will be, a normal distribution.</p></blockquote><p>I 100% agree with this statement.
At least, if you are working with performance distributions
(that are based on the multiple iterations of your benchmarks that measure the performance metrics of your applications),
you should forget about normality.
That&rsquo;s how a typical performance distribution looks like
(I built the below picture based on a real benchmark that measures the load time of assemblies
when we open the <a href=https://github.com/OrchardCMS/Orchard>Orchard</a> solution in <a href=https://www.jetbrains.com/rider/>Rider</a> on Linux):</p><div class=mx-auto><a href=/posts/normality/img/performance-light.png target=_blank><picture>
<source theme=dark srcset=/posts/normality/img/performance-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/normality/img/performance-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/normality/img/performance-light.png></picture></a></div><br><p>Of course, <em>some</em> of the performance distributions look similar to the normal distribution.
And you can apply statistical approaches that assume normality to such distributions.
And these approaches may provide correct results in many cases.
If you are working with a single benchmark and manually check the results,
you will be probably lucky enough and get correct results which will strengthen your faith to the fact
that it&rsquo;s OK to use such approaches all the time.
However, if you are working with thousands of performance tests and you are trying to use such approaches automatically,
you will probably get wrong results for some of these tests <em>all the time</em>.</p><p>The worst thing about the real performance distributions (that are often right-skewed and multimodal) is that
it makes all of the statistical metrics (like the mean) misleading.
Let&rsquo;s look at an example of <a href=https://github.com/dotnet/BenchmarkDotNet>BenchmarkDotNet</a> summary table:</p><div class=highlight><pre class=chroma><code class=language-md data-lang=md>| Method |     Mean |    Error |   StdDev |   Median |
|------- |---------:|---------:|---------:|---------:|
|      A | 136.2 ms | 19.30 ms | 56.92 ms | 107.0 ms |
|      B | 133.7 ms |  4.14 ms | 12.20 ms | 130.2 ms |
</code></pre></div><p>From my observations, when the Mean value is presented,
people <em>usually</em> read only the Mean column and ignore other columns.
After that, they make conclusions only based on the Mean values and the normal distribution mental model.
Thus, they may think that <code>B</code> always works a little bit faster than <code>A</code> in the above table.
Now let&rsquo;s look at the &ldquo;Expectation vs. Reality&rdquo; picture for the <code>A</code> and <code>B</code> density plots:</p><div class=mx-auto><a href=/posts/normality/img/misleading-light.png target=_blank><picture>
<source theme=dark srcset=/posts/normality/img/misleading-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/normality/img/misleading-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/normality/img/misleading-light.png></picture></a></div><br><p>As you can see, both distributions are heavy-tailed right-skewed distributions.
<code>A</code> has very huge outliers (that spoiled the mean value),
but most of the <code>B</code> quantiles are larger than the corresponding <code>A</code> quantiles.
BenchmarkDotNet automatically added the <code>Median</code> column to highlight this fact
(the median value for <code>B</code> is much larger than the median value for <code>A</code>).
Unfortunately, most of the people ignore such kind of additional information and
continue to imagine a normal distribution based on the mean value.</p><p>There is one more thing in perception of statistics.
When I tell people about complex multimodal heavy-tailed right-skewed distributions,
they usually reply that we have the
<a href=https://en.wikipedia.org/wiki/Central_limit_theorem>Central Limit Theorem</a>.
For some reason, people think that it will magically stabilize the mean value and
rescue us from multimodal distributions if we collect &ldquo;enough&rdquo; measurements.
Here is the definition of this theorem from <a href=https://www.dummies.com/education/math/statistics/how-the-central-limit-theorem-is-used-in-statistics/>Statistics For Dummies</a>:</p><blockquote><p>The Central Limit Theorem (CLT for short) basically says that for non-normal data, the distribution of the sample means has an approximate normal distribution, no matter what the distribution of the original data looks like, as long as the sample size is large enough (usually at least 30) and all samples have the same size.</p></blockquote><p>So, if we take many samples (sets of measurements), calculate the mean value for each sample,
and build a distribution from these mean values, it should be normal.
If you are not sure that you understand the Central Limit Theorem, I you can watch this video
that explains it with Bunnies & Dragons (the original was found <a href=https://blog.minitab.com/blog/michelle-paret/explaining-the-central-limit-theorem-with-bunnies-and-dragons-v2>here</a>):</p><div class=text-center><iframe width=800 height=450 src=https://www.youtube.com/embed/jvoxEYmQHNM frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div><p>Let&rsquo;s check how it works in practice.
I wrote a simple R script that emulates taking measurements from a &ldquo;strange&rdquo; distribution with high outliers:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>library</span><span class=p>(</span><span class=n>ggplot2</span><span class=p>)</span>

<span class=n>n</span> <span class=o>&lt;-</span> <span class=m>30</span> <span class=c1># Number of values in each sample</span>
<span class=n>m</span> <span class=o>&lt;-</span> <span class=m>30</span> <span class=c1># Number of samples</span>
<span class=n>k</span> <span class=o>&lt;-</span> <span class=m>16</span> <span class=c1># Number of CLT distributions</span>

<span class=nf>set.seed</span><span class=p>(</span><span class=m>159</span><span class=p>)</span>
<span class=c1># Generate a single random value from a &#34;strange&#34; distribution</span>
<span class=n>gen.value</span> <span class=o>&lt;-</span> <span class=nf>function</span><span class=p>()</span>
  <span class=nf>sample</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>,</span> <span class=m>1</span><span class=p>)</span> <span class=o>+</span>                               <span class=c1># Offset</span>
  <span class=nf>rbeta</span><span class=p>(</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>,</span> <span class=m>10</span><span class=p>)</span> <span class=o>*</span> <span class=nf>sample</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>,</span> <span class=m>1</span><span class=p>)</span> <span class=o>+</span>             <span class=c1># Right-skewed distribution</span>
  <span class=nf>sample</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=nf>rep</span><span class=p>(</span><span class=m>0</span><span class=p>,</span> <span class=m>50</span><span class=p>),</span> <span class=m>1</span><span class=o>:</span><span class=m>10</span><span class=p>))</span> <span class=o>*</span> <span class=nf>rnorm</span><span class=p>(</span><span class=m>1</span><span class=p>,</span> <span class=m>200</span><span class=p>,</span> <span class=m>10</span><span class=p>)</span> <span class=c1># Outliers</span>
<span class=c1># Generate a sample mean</span>
<span class=n>gen.mean</span> <span class=o>&lt;-</span> <span class=nf>function</span><span class=p>()</span> <span class=nf>mean</span><span class=p>(</span><span class=nf>sapply</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=n>n</span><span class=p>,</span> <span class=nf>function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=nf>gen.value</span><span class=p>()))</span>

<span class=n>df</span> <span class=o>&lt;-</span> <span class=nf>data.frame</span><span class=p>()</span>
<span class=nf>for </span><span class=p>(</span><span class=n>i</span> <span class=n>in</span> <span class=m>1</span><span class=o>:</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
  <span class=n>df</span> <span class=o>&lt;-</span> <span class=nf>rbind</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=nf>data.frame</span><span class=p>(</span>
    <span class=n>Experiment</span> <span class=o>=</span> <span class=nf>rep</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>m</span><span class=p>),</span>
    <span class=n>Time</span> <span class=o>=</span> <span class=nf>sapply</span><span class=p>(</span><span class=m>1</span><span class=o>:</span><span class=n>m</span><span class=p>,</span> <span class=nf>function</span><span class=p>(</span><span class=n>j</span><span class=p>)</span> <span class=nf>gen.mean</span><span class=p>()))</span>
  <span class=p>)</span>
<span class=p>}</span>
<span class=n>p</span> <span class=o>&lt;-</span> <span class=nf>ggplot</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=nf>aes</span><span class=p>(</span><span class=n>x</span> <span class=o>=</span> <span class=n>Time</span><span class=p>,</span> <span class=n>group</span> <span class=o>=</span> <span class=n>Experiment</span><span class=p>))</span> <span class=o>+</span>
  <span class=nf>geom_density</span><span class=p>(</span><span class=n>fill</span> <span class=o>=</span> <span class=s>&#34;red&#34;</span><span class=p>,</span> <span class=n>alpha</span> <span class=o>=</span> <span class=m>0.4</span><span class=p>,</span> <span class=n>bw</span> <span class=o>=</span> <span class=s>&#34;SJ&#34;</span><span class=p>)</span> <span class=o>+</span>
  <span class=nf>facet_wrap</span><span class=p>(</span><span class=o>~</span><span class=n>Experiment</span><span class=p>)</span> <span class=o>+</span>
  <span class=nf>ylab</span><span class=p>(</span><span class=s>&#34;Density&#34;</span><span class=p>)</span>
</code></pre></div><p>In this script,
<code>gen.value</code> returns a random value from our &ldquo;strange&rdquo; distribution,
<code>gen.mean</code> generates a sample with 30 measurements (because <a href=https://stats.stackexchange.com/q/2541/261747>30 is a magic number</a>)
and returns the mean value of this sample.
Next, we perform 16 experiments.
In each experiment, we draw a distribution density function based on 30 mean values that we generated.
Here is the result:</p><div class=mx-auto><a href=/posts/normality/img/clt-light.png target=_blank><picture>
<source theme=dark srcset=/posts/normality/img/clt-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/normality/img/clt-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/normality/img/clt-light.png></picture></a></div><br><p>As you can see, many of the generated distribution are not looking as normal.
What&rsquo;s wrong?
Maybe the Central Limit Theorem doesn&rsquo;t work?
Don&rsquo;t worry, there are no problems with the theorem.
We just didn&rsquo;t take enough measurements.
If we increase the sample sizes (<code>n</code> and <code>m</code> parameters in the above R script), the plots will be &ldquo;fixed&rdquo;:
the observed density plots will become &ldquo;more normal.&rdquo;</p><p>Now let&rsquo;s think about the sample sizes.
In each experiment, we draw a density function based on 900 measurements
(we have 30 mean values; each value was calculated based on a sample with 30 measurements).
The script works pretty fast because it generates &ldquo;fake&rdquo; data.
In real life, we typically spend at least 1 second per measurement.
If we want to build such a plot based on the real data,
we will spend 900 seconds (or 15 minutes) per an experiment.
Some of the real benchmarks may take more than 1 minute per measurement which means that
we should spend <em>15 hours</em> for the whole experiment.
In theory, if we spend a lot of hours on the measurements,
we will most likely get a plot which will be similar to the normal distribution.
In practice, we will not spend so much time per each benchmark
(especially, if we have hundreds or thousands of them).</p><p>At the conclusion, I want to highlight some important facts about the Central Limit Theorem
that people usually don&rsquo;t understand
(based on the &ldquo;The Central Limit Theorem&rdquo; section from <a href=/prodotnetbenchmarking/>my book</a>):</p><ul><li>If we do many iterations, the original distribution will not become normal,
and we can&rsquo;t interpret the mean, the variance, the skewness, and the kurtosis as in the case of normal distribution.</li><li>The range of the mean values across all samples is not always narrow,
we still can have a huge difference between the mean values in different samples.
The normal distribution based on the mean values has its own standard deviation
which depends on the sample size and can be expressed via the standard error.</li><li>The central limit theorem doesn&rsquo;t work correctly when the sample sizes are small.
For example, if you make a single measurement in each sample,
the distribution based on the mean values will have the same shape as the original distribution.</li><li>If we take a small number of samples (<span class="math inline">\(n < 100\)</span>),
we will probably not see a normal distribution on the density plot for mean values.</li></ul><p>If we are speaking about the original performance distributions,
we can forget about normality at all.
The assumption of normality can work fine in some special cases,
but it will let you down in the long run.
Remember the R.C. Geary&rsquo;s words: <em>&ldquo;Normality is a myth; there never was, and never will be, a normal distribution."</em></p><br><br><div class=mx-auto>Share:<div class=faicon><a href="https://twitter.com/intent/tweet?text=Normality%20is%20a%20myth&url=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f&via=andrey_akinshin&related=andrey_akinshin" rel=nofollow target=_blank title="Share on Twitter"><i class="fab fa-twitter fa-2x" title=Twitter></i></a></div><div class=faicon><a href="https://www.reddit.com/submit?url=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f&title=Normality%20is%20a%20myth" target=_blank title="Share on Reddit"><i class="fab fa-reddit fa-2x"></i></a></div><div class=faicon><a href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f" target=_blank title="Share on HackerNews"><i class="fab fa-hacker-news fa-2x"></i></a></div><div class=faicon><a href="https://facebook.com/sharer.php?u=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f" rel=nofollow target=_blank title="Share on Facebook"><i class="fab fa-facebook fa-2x"></i></a></div><div class=faicon><a href="http://vk.com/share.php?url=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f" target=_blank title="Share on VKontakte"><i class="fab fa-vk fa-2x"></i></a></div><div class=faicon><a href="https://getpocket.com/save?url=https%3a%2f%2faakinshin.net%2fposts%2fnormality%2f&title=Normality%20is%20a%20myth" target=_blank title="Add to Pocket"><i class="fab fa-get-pocket fa-2x"></i></a></div></div><hr></div></div></main></div><footer class=blog-footer><div class=container><p>&copy; 2013–2020 Andrey Akinshin | <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.ea4f51ac7f59c8e13f4df787df946eb1a447174ce2f1fc5349483de93a607ecb.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/anchor.min.js></script><script src=https://aakinshin.net/js/custom.min.11932490dde776463ed165b345838c701accc0cfdedf2e0868f13415f41f0872.js></script></body></html>