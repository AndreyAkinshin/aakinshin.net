<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.123.3"><meta name=author content='Andrey Akinshin'><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content='Mathematics,Statistics,Research,Research: Building trimmed Harrell-Davis quantile estimator'><title>Trimmed Harrell-Davis quantile estimator based on the highest density interval of the given width | Andrey Akinshin</title>
<meta name=description content="Traditional quantile estimators that are based on one or two order statistics are a common way to estimate distribution quantiles based on the given samples. These estimators are robust, but their statistical efficiency is not always good enough. A more eff..."><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link href=https://aakinshin.net/css/main.min.5108205eaabc89b26ffd82359e0fe4fb2b5e8c9825e57ed51687cc18a3c75b9b.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"></head><body><nav><div class="container flex flex-wrap items-center justify-between mx-auto max-w-6xl h-12 px-2 xs:px-6"><div class="flex items-center justify-start justify-self-start"><div class="flex nav-item h-12 w-12 items-center justify-center"><a class=text-white href=https://aakinshin.net/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#house-chimney"/></svg></a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/posts/>Posts</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/research/>Research</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/library/>Library</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/about/>About</a></div><div class="flex nav-item h-12 items-center"><a class=text-white href=https://aakinshin.net/search/><svg class="fai w-5 h-5 pr-0 mb-0.5"><use xlink:href="/img/fa/all.svg#magnifying-glass"/></svg></a></div></div><button id=theme-toggle type=button title="Alt+Click to match OS color theme" class="nav-item p-3 content-center justify-self-end"><svg id="theme-toggle-dark-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-6 h-6" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></div></nav><div class="main container mx-auto max-w-6xl px-6"><div class=main-post><h1 class=blog-post-title id=post-title>Trimmed Harrell-Davis Quantile Estimator Based on the Highest Density Interval of the Given Width</h1><div class="flex flex-wrap justify-start items-center"><svg class="fai text-accent-l dark:text-front-d"><title>Date</title><use xlink:href="/img/fa/all.svg#calendar-days"/></svg>
<time datetime=2021-10-19>October 19, 2021</time><svg class="fai ml-3 text-accent-l dark:text-front-d"><title>Tags</title><use xlink:href="/img/fa/all.svg#tag"/></svg><div class="flex flex-wrap gap-y-1"><a class=label-link href=https://aakinshin.net/tags/mathematics/><svg class="rating-icon"><use xlink:href="/img/fa/all.svg#math"/></svg>
Mathematics
</a><a class=label-link href=https://aakinshin.net/tags/statistics/>Statistics
</a><a class=label-link href=https://aakinshin.net/tags/research/>Research
</a><a class=label-link href=https://aakinshin.net/tags/research-thdqe/>Research: Building Trimmed Harrell-Davis Quantile Estimator</a></div></div><br><div class="main-content mb-3 px-2 py-1 rounded border text-alert-text-l border-alert-frame-l dark:text-alert-text-d dark:border-alert-frame-d">Update: this blog post is a part of research that aimed to build a statistically efficient and robust quantile estimator. A <a href=/posts/pub-thdqe/>paper with final results</a> is available in <em>Communications in Statistics - Simulation and Computation</em> (DOI: <a href=https://www.tandfonline.com/doi/abs/10.1080/03610918.2022.2050396>10.1080/03610918.2022.2050396</a>). A preprint is available on arXiv: <a href=https://arxiv.org/abs/2111.11776>arXiv:2111.11776 [stat.ME]</a>. Some information in this blog post can be obsolete: please, use the official paper as the primary reference.</div><div class=main-content><p>Traditional quantile estimators that are based on one or two order statistics are a common way to estimate
distribution quantiles based on the given samples.
These estimators are robust, but their statistical efficiency is not always good enough.
A more efficient alternative is the Harrell-Davis quantile estimator which uses
a weighted sum of all order statistics.
Whereas this approach provides more accurate estimations for the light-tailed distributions, it&rsquo;s not robust.
To be able to customize the trade-off between statistical efficiency and robustness,
we could consider <em>a trimmed modification of the Harrell-Davis quantile estimator</em>.
In this approach, we discard order statistics with low weights according to
the highest density interval of the beta distribution.</p><h3 id=introduction>Introduction</h3><p>We consider a problem of quantile estimation for the given sample.
Let $x$ be a sample with $n$ elements: $x = \{ x_1, x_2, \ldots, x_n \}$.
We assume that all sample elements are sorted ($x_1 \leq x_2 \leq \ldots \leq x_n$) so that
we could treat the $i^\textrm{th}$ element $x_i$ as the $i^\textrm{th}$ order statistic $x_{(i)}$.
Based on the given sample, we want to build an estimation of the $p^\textrm{th}$ quantile $Q(p)$.</p><p>The traditional way to do this is to use a single order statistic
or a linear combination of two subsequent order statistics.
This approach could be implemented in various ways.
A classification of the most popular implementations could be found in <a href=https://aakinshin.net/library/papers/hyndman1996/><svg class="rating-icon"><use xlink:href="/img/fa/all.svg#paper"/></svg>hyndman1996</a>.
In this paper, Rob J. Hyndman and Yanan Fan describe nine types of traditional quantile estimators
which are used in statistical computer packages.
The most popular approach in this taxonomy is Type 7 which is used by default in R, Julia, NumPy, and Excel:</p>$$
Q_{\operatorname{HF7}}(p) = x_{\lfloor h \rfloor}+(h-\lfloor h \rfloor)(x_{\lfloor h \rfloor+1})-x_{\lfloor h \rfloor},
\quad h = (n-1)p+1.
$$<p>Traditional quantile estimators have simple implementations and a good robustness level.
However, their statistical efficiency is not always good enough:
the obtained estimations could noticeably differ from the true distribution quantile values.
The gap between the estimated and true values could be decreased by increasing the number of used order statistics.
In <a href=https://aakinshin.net/library/papers/harrell1982/><svg class="rating-icon"><use xlink:href="/img/fa/all.svg#paper"/></svg>harrell1982</a>, Frank E. Harrell and C. E. Davis suggest estimating quantiles using
a weighted sum of all order statistics:</p>$$
Q_{\operatorname{HD}}(p) = \sum_{i=1}^{n} W_{\operatorname{HD},i} \cdot x_i,\quad
W_{\operatorname{HD},i} = I_{i/n}(\alpha, \beta) - I_{(i-1)/n}(\alpha, \beta),
$$<p>where $I_x(\alpha, \beta)$ is the regularized incomplete beta function,
$\alpha = (n+1)p$, $\;\beta = (n+1)(1-p)$.
To get a better understanding of this approach,
we could look at the probability density function of the beta distribution $\operatorname{Beta}(\alpha, \beta)$.
If we split the $[0;1]$ interval into $n$ segments of equal width,
we can define $W_{\operatorname{HD},i}$ as the area under curve in the $i^\textrm{th}$ segment.
Since $I_x(\alpha, \beta)$ is the cumulative distribution function of $\operatorname{Beta}(\alpha, \beta)$,
we can express $W_{\operatorname{HD},i}$ as $I_{i/n}(\alpha, \beta) - I_{(i-1)/n}(\alpha, \beta)$.</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/beta-light.png target=_blank alt=beta><img src=/posts/thdqe-hdi/img/beta-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/beta-dark.png target=_blank alt=beta><img src=/posts/thdqe-hdi/img/beta-dark.png width=800></a></div><p>The Harrell-Davis quantile estimator shows decent statistical efficiency in the case of light-tailed distributions:
its estimations are much more precise than estimations of the traditional quantile estimators.
However, the improved efficiency has a price: $Q_{\operatorname{HD}}$ is not robust.
Since the estimation is a weighted sum of all order statistics with positive weights,
a single corrupted element may spoil all the quantile estimations, including the median.
It may become a severe drawback in the case of heavy-tailed distributions in which
it&rsquo;s a typical situation when we have a few extremely large outliers.
In such cases, we use the median instead of the mean as a measure of central tendency
because of its robustness.
Indeed, if we estimate the median using the traditional quantile estimators like $Q_{\operatorname{HF7}}$,
its asymptotical breakdown point is 0.5.
Unfortunately, if we switch to $Q_{\operatorname{HD}}$, the breakdown point becomes zero
so that we completely lose the median robustness.</p><p>Another severe drawback of $Q_{\operatorname{HD}}$ is its computational complexity.
If we have a sorted array of numbers,
a traditional quantile estimation could be computed using $O(1)$ simple operations.
If we estimate the quantiles using $Q_{\operatorname{HD}}$, we need $O(n)$ operations.
Moreover, these operations involve computation of $I_x(\alpha, \beta)$ values
which are pretty expensive from the computational point of view.
If we want to estimate millions of quantile estimations,
$Q_{\operatorname{HD}}$ may have a noticeable impact on the application performance.</p><p>Neither $Q_{\operatorname{HF7}}$ nor $Q_{\operatorname{HD}}$ fit all kinds of problem.
$Q_{\operatorname{HF7}}$ is simple, robust, and computationally fast,
but its statistical efficiency doesn&rsquo;t always satisfy the business requirements.
$Q_{\operatorname{HD}}$ could provide better statistical efficiency,
but it&rsquo;s computationally slow and not robust.</p><p>To get a reasonable trade-off between $Q_{\operatorname{HF7}}$ and $Q_{\operatorname{HD}}$,
we consider a trimmed modification of the Harrell-Davis quantile estimator.
The core idea is simple:
we take the classic Harrell-Davis quantile estimator,
find the highest density interval of the underlying beta distribution,
discard all the order statistics outside the interval,
and calculate a weighted sum of the order statistics within the interval.
The obtained quantile estimation is more robust than $Q_{\operatorname{HD}}$ (because it doesn&rsquo;t use extreme values)
and typically more statistically efficient than $Q_{\operatorname{HF7}}$
(because it uses more than only two order statistics).
Let&rsquo;s discuss this approach in detail.</p><h3 id=the-trimmed-harrell-davis-quantile-estimator>The trimmed Harrell-Davis quantile estimator</h3><p>The estimators based on one or two order statistics are not efficient enough because they use too few sample elements.
The estimators based on all order statistics are not robust enough because they use too many sample elements.
It looks reasonable to consider a quantile estimator based on a variable number of order statistics.
This number should be large enough to ensure decent statistical efficiency
but not too large to exclude possible extreme outliers.</p><p>A robust alternative to the mean is the trimmed mean.
The idea behind it is simple: we should discard some sample elements at both ends
and use only the middle order statistics.
With this approach, we can customize the trade-off between robustness and statistical efficiency
by controlling the number of the discarded elements.
If we apply the same idea to $Q_{\operatorname{HD}}$,
we can build a trimmed modification of the Harrell-Davis quantile estimator.
Let&rsquo;s denote it as $Q_{\operatorname{THD}}$.</p><p>In the case of the trimmed mean, we typically discard the same number of elements on each side.
We can&rsquo;t do the same for $Q_{\operatorname{THD}}$
because the array of order statistic weights $\{ W_{\operatorname{HD},i} \}$ is asymmetric.
It looks reasonable to drop the elements with the lowest weights and keep the elements with the highest weights.
Since the weights are assigned according to the beta distribution,
the range of order statistics with the highest weight concentration could be found
using the beta distribution highest density interval.
Thus, once we fix the proportion of dropped/kept elements,
we should find the highest density interval of the given width.
Let&rsquo;s denote the interval as $[L;R]$ where $R-L=D$.
The order statistics weights for $Q_{\operatorname{THD}}$ should be defined
using a part of the beta distribution within this interval.
It gives us the truncated beta distribution $\operatorname{TBeta}(\alpha, \beta, L, R)$:</p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/tbeta-light.png target=_blank alt=tbeta><img src=/posts/thdqe-hdi/img/tbeta-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/tbeta-dark.png target=_blank alt=tbeta><img src=/posts/thdqe-hdi/img/tbeta-dark.png width=800></a></div><p>We know the CDF for $\operatorname{Beta}(\alpha, \beta)$ which is used in $Q_{\operatorname{HD}}$:
$F_{\operatorname{HD}}(x) = I_x(\alpha, \beta)$.
For $Q_{\operatorname{THD}}$,
we need the CDF for $\operatorname{TBeta}(\alpha, \beta, L, R)$ which could be easily found:</p>$$
F_{\operatorname{THD}}(x) = \begin{cases}
0 & \textrm{for }\, x < L,\\
\big( F_{\operatorname{HD}}(x) - F_{\operatorname{HD}}(L) \big) /
\big( F_{\operatorname{HD}}(R) - F_{\operatorname{HD}}(L) \big)
& \textrm{for }\, L \leq x \leq R,\\
1 & \textrm{for }\, R < x.
\end{cases}
$$<p>The final $Q_{\operatorname{THD}}$ equation has the same form as $Q_{\operatorname{HD}}$:</p>$$
Q_{\operatorname{THD}} = \sum_{i=1}^{n} W_{\operatorname{THD},i} \cdot x_i, \quad
W_{\operatorname{THD},i} = F_{\operatorname{THD}}(i / n) - F_{\operatorname{THD}}((i - 1) / n).
$$<p>There is only one thing left to do:
we should choose an appropriate width $D$ of the beta distribution highest density interval.
In practical application, this value should be chosen based on the given problem:
researchers should <em>carefully</em> analyze business requirements,
describe desired robustness level via setting the breakdown point,
and come up with a $D$ value that satisfies the initial requirements.</p><p>However, if we have absolutely no information about the problem, the underlying distribution,
and the robustness requirements, we can use the following rule of thumb which gives the starting point:
$D=1/\sqrt{n}$.
We denote $Q_{\operatorname{THD}}$ with such a $D$ value as $Q_{\operatorname{THD-SQRT}}$.
In most cases, it gives an acceptable trade-off between the statistical efficiency and the robustness level.
Also, $Q_{\operatorname{THD-SQRT}}$ has a practically reasonable computational complexity:
$O(\sqrt{n})$ instead of $O(n)$ for $Q_{\operatorname{HD}}$.
For example, if $n=10000$, we have to process only 100 sample elements and calculate 101 values of $I_x(\alpha, \beta)$.</p><p><strong>An example</strong>
Let&rsquo;s say we have the following sample:</p>$$
x = \{ -0.565, -0.106, -0.095, 0.363, 0.404, 0.633, 1.371, 1.512, 2.018, 100\,000 \}.
$$<p>Nine elements were randomly taken from the standard normal distribution $\mathcal{N}(0, 1)$.
The last element $x_{10}$ is an outlier.
The weight coefficient for $Q_{\operatorname{HD}}$ an $Q_{\operatorname{THD-SQRT}}$
are presented in the following table:</p><table><thead><tr><th align=right>$i$</th><th align=right>$x_i$</th><th align=right>$W_{\operatorname{HD},i}$</th><th align=right>$W_{\operatorname{THD-SQRT},i}$</th></tr></thead><tbody><tr><td align=right>1</td><td align=right>-0.565</td><td align=right>0.0005</td><td align=right>0</td></tr><tr><td align=right>2</td><td align=right>-0.106</td><td align=right>0.0146</td><td align=right>0</td></tr><tr><td align=right>3</td><td align=right>-0.095</td><td align=right>0.0727</td><td align=right>0</td></tr><tr><td align=right>4</td><td align=right>0.363</td><td align=right>0.1684</td><td align=right>0.1554</td></tr><tr><td align=right>5</td><td align=right>0.404</td><td align=right>0.2438</td><td align=right>0.3446</td></tr><tr><td align=right>6</td><td align=right>0.633</td><td align=right>0.2438</td><td align=right>0.3446</td></tr><tr><td align=right>7</td><td align=right>1.371</td><td align=right>0.1684</td><td align=right>0.1554</td></tr><tr><td align=right>8</td><td align=right>1.512</td><td align=right>0.0727</td><td align=right>0</td></tr><tr><td align=right>9</td><td align=right>2.018</td><td align=right>0.0146</td><td align=right>0</td></tr><tr><td align=right>10</td><td align=right>100000.000</td><td align=right>0.0005</td><td align=right>0</td></tr></tbody></table><p>Here are the corresponding quantile estimations:</p>$$
Q_{\operatorname{HD}}(0.5) \approx 51.9169, \quad Q_{\operatorname{THD}}(0.5) \approx 0.6268.
$$<p>As we can see, $Q_{\operatorname{HD}}$ is heavily affected by the outlier $x_{10}$.
Meanwhile, $Q_{\operatorname{THD}}$ gives a reasonable median estimation
because it uses a weighted sum of four middle order statistics.</p><h3 id=beta-distribution-highest-density-interval-of-the-given-width>Beta distribution highest density interval of the given width</h3><p>In order to build the truncated beta distribution for $Q_{\operatorname{THD}}$,
we have to find the $\operatorname{Beta}(\alpha, \beta)$ highest density interval of the required width $D$.
Thus, for the given $\alpha, \beta, D$, we should provide an interval $[L;R]$:</p>$$
\operatorname{BetaHDI}(\alpha, \beta, D) = [L; R].
$$<p>Let&rsquo;s briefly discuss how to do this.
First of all, we should calculate the mode $M$ of $\operatorname{Beta}(\alpha, \beta)$:</p>$$
M = \operatorname{Mode}_{\alpha, \beta} =
\begin{cases}
\{0, 1 \} \textrm{ or any value in } (0, 1) & \textrm{for }\, \alpha \leq 1,\, \beta \leq 1, & \textit{(Degenerate case)}
  0   & \textrm{for }\, \alpha \leq 1,\, \beta > 1, & \textit{(Left border case)} \\
  1 & \textrm{for }\, \alpha > 1,\, \beta \leq 1, & \textit{(Right border case)} \\
 \frac{\alpha - 1}{\alpha + \beta - 2} & \textrm{for }\, \alpha > 1,\, \beta > 1. & \textit{(Middle case)} \\
\end{cases}
$$<div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/hdi-light.png target=_blank alt=hdi><img src=/posts/thdqe-hdi/img/hdi-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/hdi-dark.png target=_blank alt=hdi><img src=/posts/thdqe-hdi/img/hdi-dark.png width=800></a></div><p>The actual value of $\operatorname{BetaHDI}(\alpha, \beta, D)$ depends on the specific case from the above list
which defines the mode location.
Three of these cases are easy to handle:</p><ul><li>Degenerate case $\alpha \leq 1, \beta \leq 1$:
There is only one way to get such a situation: $n = 1, p = 0.5$.
Since such a sample contains a single element, it doesn&rsquo;t matter how we choose the interval.</li><li>Left border case $\alpha \leq 1, \, \beta > 1$:
The mode equals zero, so the interval should be &ldquo;attached to the left border&rdquo;:
$\operatorname{BetaHDI}(\alpha, \beta, D) = [0; D]$.</li><li>Right border case $\alpha > 1, \, \beta \leq 1$:
The mode equals one, so the interval should be &ldquo;attached to the right border&rdquo;:
$\operatorname{BetaHDI}(\alpha, \beta, D) = [1 - D; 1]$</li></ul><p>The fourth case is the middle case ($\alpha > 1,\, \beta > 1$),
the HDI should be inside $(0;1)$.
Since the density function of the beta distribution is a unimodal function, it consists of two segments:
a monotonically increasing segment $[0, M]$ and
a monotonically decreasing segment $[M, 1]$.
The HDI $[L;R]$ should contain the mode, so</p>$$
L \in [0; M], \quad
R \in [M; 1].
$$<p>Since $R - L = D$, we could also conclude that</p>$$
L = R - D \in [M - D; 1 - D], \quad
R = L + D \in [D; M + D].
$$<p>Thus,</p>$$
L \in [\max(0, M - D);\; \min(M, 1 - D)], \quad
R \in [\max(M, D);\; \min(1, M + D)].
$$<p>The density function of the beta distribution is also known:</p>$$
f(x) = \dfrac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{\textrm{B}(\alpha, \beta)}, \quad
\textrm{B}(\alpha, \beta) = \dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}.
$$<p>It&rsquo;s easy to see that for the highest density interval $[L; R]$, the following condition is true:</p>$$
f(L) = f(R).
$$<p>The left border $L$ of this interval could be found as a solution of the following equation:</p>$$
f(t) = f(t + D), \quad \textrm{where }\, t \in [\max(0, M - D);\; \min(M, 1 - D)].
$$<p>The left side of the equation is monotonically increasing, the right side is monotonically decreasing.
The equation has exactly one solution which could be easily found numerically using the binary search algorithm.</p><h4 id=simulation-study>Simulation study</h4><p>Let&rsquo;s perform a few numerical simulations and see how $Q_{\operatorname{THD}}$ works in action.</p><h4 id=simulation-1>Simulation 1</h4><p>Let&rsquo;s explore the distribution of estimation errors of
$Q_{\operatorname{HF7}}$, $Q_{\operatorname{HD}}$, and $Q_{\operatorname{THD-SQRT}}$.
We consider a contaminated normal distribution which is a mixture of two normal distributions:
$(1 - \varepsilon)\mathcal{N}(0, \sigma^2) + \varepsilon\mathcal{N}(0, c\sigma^2)$.
For our simulation, we use $\varepsilon = 0.01,\; \sigma = 1,\; c = 1\,000\,000$.
We generate $10\,000$ samples of size 7 randomly taken from the considered distribution.
For each sample, we estimate the median using
$Q_{\operatorname{HF7}}$, $Q_{\operatorname{HD}}$, and $Q_{\operatorname{THD-SQRT}}$.
Thus, we have $10\,000$ of median estimations for each estimator.
Next, we evaluate lower and higher percentiles for each group of estimations.
The results are presented in the following table:</p><table><thead><tr><th align=right>quantile</th><th align=right>HF7</th><th align=right>HD</th><th align=right>THD-SQRT</th></tr></thead><tbody><tr><td align=right>0.00</td><td align=right>-1.6921648</td><td align=right>-87.6286082</td><td align=right>-1.6041220</td></tr><tr><td align=right>0.01</td><td align=right>-1.1054591</td><td align=right>-9.8771723</td><td align=right>-1.0261234</td></tr><tr><td align=right>0.02</td><td align=right>-0.9832125</td><td align=right>-5.2690083</td><td align=right>-0.9067884</td></tr><tr><td align=right>0.03</td><td align=right>-0.9037046</td><td align=right>-1.7742334</td><td align=right>-0.8298706</td></tr><tr><td align=right>0.04</td><td align=right>-0.8346268</td><td align=right>-0.9921591</td><td align=right>-0.7586603</td></tr><tr><td align=right>0.05</td><td align=right>-0.7773634</td><td align=right>-0.8599139</td><td align=right>-0.7141364</td></tr><tr><td align=right>0.95</td><td align=right>0.7740584</td><td align=right>0.8062170</td><td align=right>0.7060375</td></tr><tr><td align=right>0.96</td><td align=right>0.8172518</td><td align=right>0.8964743</td><td align=right>0.7540437</td></tr><tr><td align=right>0.97</td><td align=right>0.8789283</td><td align=right>1.1240294</td><td align=right>0.8052421</td></tr><tr><td align=right>0.98</td><td align=right>0.9518048</td><td align=right>4.3675475</td><td align=right>0.8824462</td></tr><tr><td align=right>0.99</td><td align=right>1.0806293</td><td align=right>10.4132583</td><td align=right>0.9900912</td></tr><tr><td align=right>1.00</td><td align=right>2.0596785</td><td align=right>140.5802861</td><td align=right>1.7060750</td></tr></tbody></table><p>As we can see, approximately 2% of all $Q_{\operatorname{HD}}$ results exceed 10 by their absolute values
(while the true median value is zero).
Meanwhile, the maximum absolute value of the $Q_{\operatorname{THD-SQRT}}$ median estimations is approximately $1.7$.
Thus, $Q_{\operatorname{THD-SQRT}}$ is much more resistant to outliers than $Q_{\operatorname{HD}}$.</p><h4 id=simulation-2>Simulation 2</h4><p>Let&rsquo;s compare the statistical efficiency of $Q_{\operatorname{HD}}$ and $Q_{\operatorname{THD}}$.
We evaluate the relative efficiency of these estimators against $Q_{\operatorname{HF7}}$
which is a conventional baseline in such experiments.
For the $p^\textrm{th}$ quantile, the classic relative efficiency can be calculated
as the ratio of the estimator mean squared errors ($\operatorname{MSE}$):</p>$$
\operatorname{Efficiency}(p) =
\dfrac{\operatorname{MSE}(Q_{HF7}, p)}{\operatorname{MSE}(Q_{\textrm{Target}}, p)} =
\dfrac{\operatorname{E}[(Q_{HF7}(p) - \theta(p))^2]}{\operatorname{E}[(Q_{\textrm{Target}}(p) - \theta(p))^2]},
$$<p>where $\theta(p)$ is the true quantile value.
We conduct this simulation according to the following scheme:</p><ul><li>We consider a bunch of different symmetric and asymmetric, light-tailed and heavy-tailed distributions.</li><li>We enumerate all the percentile values $p$ from 0.01 to 0.99.</li><li>For each distribution, we generate 200 random samples of the given size.
For each sample, we estimate the $p^\textrm{th}$ percentile using
$Q_{\operatorname{HF7}}$, $Q_{\operatorname{HD}}$, and $Q_{\operatorname{THD-SQRT}}$.
For each estimator, we calculate the arithmetic average of $(Q(p) - \theta(p))^2$.</li><li>$\operatorname{MSE}$ is not a robust metric, so we wouldn&rsquo;t get reproducible output in such an experiment.
To achieve more stable results, we repeat the previous step 101 times and take the median across
$\operatorname{E}[(Q(p) - \theta(p))^2]$ values for each estimator.
This median is our estimation of $\operatorname{MSE}(Q, p)$.</li><li>We evaluate the relative efficiency of $Q_{\operatorname{HD}}$ and $Q_{\operatorname{THD-SQRT}}$
against $Q_{\operatorname{HF7}}$.</li></ul><p>Here are the results of this simulation for $n=\{5, 10, 20\}$:</p><p><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/efficiency5-light.png target=_blank alt=efficiency5><img src=/posts/thdqe-hdi/img/efficiency5-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/efficiency5-dark.png target=_blank alt=efficiency5><img src=/posts/thdqe-hdi/img/efficiency5-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/efficiency10-light.png target=_blank alt=efficiency10><img src=/posts/thdqe-hdi/img/efficiency10-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/efficiency10-dark.png target=_blank alt=efficiency10><img src=/posts/thdqe-hdi/img/efficiency10-dark.png width=800></a></div><div class="flex my-7 justify-center"><a class="img-light hidden" href=/posts/thdqe-hdi/img/efficiency20-light.png target=_blank alt=efficiency20><img src=/posts/thdqe-hdi/img/efficiency20-light.png width=800>
</a><a class="img-dark hidden" href=/posts/thdqe-hdi/img/efficiency20-dark.png target=_blank alt=efficiency20><img src=/posts/thdqe-hdi/img/efficiency20-dark.png width=800></a></div></p><p>As we can see, $Q_{\operatorname{THD-SQRT}}$ is not so efficient as $Q_{\operatorname{HD}}$
in the case of light-tailed distributions.
However, in the case of heavy-tailed distributions,
$Q_{\operatorname{THD-SQRT}}$ has better efficiency than $Q_{\operatorname{HD}}$
because estimations of $Q_{\operatorname{HD}}$ are corrupted by outliers.</p><h4 id=conclusion>Conclusion</h4><p>There is no perfect quantile estimator that fits all kinds of problems.
The choice of a specific estimator has to be made
based on the knowledge of the domain area and the properties of the target distributions.
$Q_{\operatorname{HD}}$ is a good alternative to $Q_{\operatorname{HF7}}$ in the light-tailed distributions
because it has higher statistical efficiency.
However, if extreme outliers may appear, estimations of $Q_{\operatorname{HD}}$ could be heavily corrupted.
$Q_{\operatorname{THD}}$ could be used as
a reasonable trade-off between $Q_{\operatorname{HF7}}$ and $Q_{\operatorname{HD}}$.
In most cases, $Q_{\operatorname{THD}}$ has better efficiency than $Q_{\operatorname{HF7}}$
and it&rsquo;s also more resistant to outliers than $Q_{\operatorname{HD}}$.
By customizing the width $D$ of the highest density interval, we could set the desired breakdown point
according to the research goals.
Also, $Q_{\operatorname{THD}}$ has better computational efficiency than $Q_{\operatorname{HD}}$
which makes it a faster option in practical applications.</p><h4 id=reference-implementation>Reference implementation</h4><p>Here is an R implementation of the suggested estimator:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=n>getBetaHdi</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>width</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>eps</span> <span class=o>&lt;-</span> <span class=m>1e-9</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>a</span> <span class=o>&lt;</span> <span class=m>1</span> <span class=o>+</span> <span class=n>eps</span> <span class=o>&amp;</span> <span class=n>b</span> <span class=o>&lt;</span> <span class=m>1</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span> <span class=c1># Degenerate case</span>
</span></span><span class=line><span class=cl>    <span class=kr>return</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=kc>NA</span><span class=p>,</span> <span class=kc>NA</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>a</span> <span class=o>&lt;</span> <span class=m>1</span> <span class=o>+</span> <span class=n>eps</span> <span class=o>&amp;</span> <span class=n>b</span> <span class=o>&gt;</span> <span class=m>1</span><span class=p>)</span> <span class=c1># Left border case</span>
</span></span><span class=line><span class=cl>    <span class=kr>return</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=m>0</span><span class=p>,</span> <span class=n>width</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>a</span> <span class=o>&gt;</span> <span class=m>1</span> <span class=o>&amp;</span> <span class=n>b</span> <span class=o>&lt;</span> <span class=m>1</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span> <span class=c1># Right border case</span>
</span></span><span class=line><span class=cl>    <span class=kr>return</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=m>1</span> <span class=o>-</span> <span class=n>width</span><span class=p>,</span> <span class=m>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>width</span> <span class=o>&gt;</span> <span class=m>1</span> <span class=o>-</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=kr>return</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=m>0</span><span class=p>,</span> <span class=m>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Middle case</span>
</span></span><span class=line><span class=cl>  <span class=n>mode</span> <span class=o>&lt;-</span> <span class=p>(</span><span class=n>a</span> <span class=o>-</span> <span class=m>1</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>a</span> <span class=o>+</span> <span class=n>b</span> <span class=o>-</span> <span class=m>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>pdf</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=nf>dbeta</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=n>l</span> <span class=o>&lt;-</span> <span class=nf>uniroot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>f</span> <span class=o>=</span> <span class=kr>function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=nf>pdf</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>-</span> <span class=nf>pdf</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>width</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>lower</span> <span class=o>=</span> <span class=nf>max</span><span class=p>(</span><span class=m>0</span><span class=p>,</span> <span class=n>mode</span> <span class=o>-</span> <span class=n>width</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>upper</span> <span class=o>=</span> <span class=nf>min</span><span class=p>(</span><span class=n>mode</span><span class=p>,</span> <span class=m>1</span> <span class=o>-</span> <span class=n>width</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>tol</span> <span class=o>=</span> <span class=m>1e-9</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span><span class=o>$</span><span class=n>root</span>
</span></span><span class=line><span class=cl>  <span class=n>r</span> <span class=o>&lt;-</span> <span class=n>l</span> <span class=o>+</span> <span class=n>width</span>
</span></span><span class=line><span class=cl>  <span class=kr>return</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=n>l</span><span class=p>,</span> <span class=n>r</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>thdquantile</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>probs</span><span class=p>,</span> <span class=n>width</span> <span class=o>=</span> <span class=m>1</span> <span class=o>/</span> <span class=nf>sqrt</span><span class=p>(</span><span class=nf>length</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span> <span class=nf>sapply</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=kr>function</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>n</span> <span class=o>&lt;-</span> <span class=nf>length</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>n</span> <span class=o>==</span> <span class=m>0</span><span class=p>)</span> <span class=kr>return</span><span class=p>(</span><span class=kc>NA</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=kr>if</span> <span class=p>(</span><span class=n>n</span> <span class=o>==</span> <span class=m>1</span><span class=p>)</span> <span class=kr>return</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>&lt;-</span> <span class=nf>sort</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>a</span> <span class=o>&lt;-</span> <span class=p>(</span><span class=n>n</span> <span class=o>+</span> <span class=m>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>  <span class=n>b</span> <span class=o>&lt;-</span> <span class=p>(</span><span class=n>n</span> <span class=o>+</span> <span class=m>1</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=m>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>hdi</span> <span class=o>&lt;-</span> <span class=nf>getBetaHdi</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>width</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>hdiCdf</span> <span class=o>&lt;-</span> <span class=nf>pbeta</span><span class=p>(</span><span class=n>hdi</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>cdf</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>xs</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>xs[xs</span> <span class=o>&lt;=</span> <span class=n>hdi[1]]</span> <span class=o>&lt;-</span> <span class=n>hdi[1]</span>
</span></span><span class=line><span class=cl>    <span class=n>xs[xs</span> <span class=o>&gt;=</span> <span class=n>hdi[2]]</span> <span class=o>&lt;-</span> <span class=n>hdi[2</span><span class=nf>]
</span></span></span><span class=line><span class=cl><span class=nf>    </span><span class=p>(</span><span class=nf>pbeta</span><span class=p>(</span><span class=n>xs</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=o>-</span> <span class=n>hdiCdf[1]</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>hdiCdf[2]</span> <span class=o>-</span> <span class=n>hdiCdf[1]</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>iL</span> <span class=o>&lt;-</span> <span class=nf>floor</span><span class=p>(</span><span class=n>hdi[1]</span> <span class=o>*</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>iR</span> <span class=o>&lt;-</span> <span class=nf>ceiling</span><span class=p>(</span><span class=n>hdi[2]</span> <span class=o>*</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>cdfs</span> <span class=o>&lt;-</span> <span class=nf>cdf</span><span class=p>(</span><span class=n>iL</span><span class=o>:</span><span class=n>iR</span><span class=o>/</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>W</span> <span class=o>&lt;-</span> <span class=nf>tail</span><span class=p>(</span><span class=n>cdfs</span><span class=p>,</span> <span class=m>-1</span><span class=p>)</span> <span class=o>-</span> <span class=nf>head</span><span class=p>(</span><span class=n>cdfs</span><span class=p>,</span> <span class=m>-1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>sum</span><span class=p>(</span><span class=n>x</span><span class=nf>[</span><span class=p>(</span><span class=n>iL</span><span class=m>+1</span><span class=p>)</span><span class=o>:</span><span class=n>iR]</span> <span class=o>*</span> <span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></div><p>A C# implementation could be found in <a href=https://github.com/AndreyAkinshin/perfolizer>perfolizer</a> 0.3.0-nightly.105+.</p><h3 id=references>References</h3><ul><li><b id=Hyndman1996>[Hyndman1996]</b><br>Hyndman, R. J. and Fan, Y. 1996. Sample quantiles in statistical packages, <em>American Statistician</em> 50, 361–365.<br><a href=https://doi.org/10.2307/2684934>https://doi.org/10.2307/2684934</a></li><li><b id=Harrell1982>[Harrell1982]</b><br>Harrell, F.E. and Davis, C.E., 1982. A new distribution-free quantile estimator.
<em>Biometrika</em>, 69(3), pp.635-640.<br><a href=https://doi.org/10.2307/2335999>https://doi.org/10.2307/2335999</a></li></ul></div><hr><h3 id=references>References (2)</h3><ol><li><a href=https://aakinshin.net/library/papers/ title="Library / Papers">Papers
</a>/
<a href=https://aakinshin.net/library/papers/harrell1982/>A New Distribution-Free Quantile Estimator
</a>(1982)<br>&nbsp;&nbsp;&nbsp;&nbsp;
by
<a href=https://aakinshin.net/library/authors/frank-e-harrell/>Frank E Harrell</a>, <a href=https://aakinshin.net/library/authors/c-e-davis/>C E Davis</a>
<a href=https://aakinshin.net/tags/mathematics/><svg class="rating-icon"><title>Mathematics</title><use xlink:href="/img/fa/all.svg#math"/></svg>
</a><a href=https://aakinshin.net/tags/endorsed/><svg class="rating-icon"><title>Endorsed</title><use xlink:href="/img/fa/all.svg#endorsed"/></svg></a></li><li><a href=https://aakinshin.net/library/papers/ title="Library / Papers">Papers
</a>/
<a href=https://aakinshin.net/library/papers/hyndman1996/>Sample Quantiles in Statistical Packages
</a>(1996)<br>&nbsp;&nbsp;&nbsp;&nbsp;
by
<a href=https://aakinshin.net/library/authors/rob-j-hyndman/>Rob J Hyndman</a>, <a href=https://aakinshin.net/library/authors/yanan-fan/>Yanan Fan</a>
<a href=https://aakinshin.net/tags/mathematics/><svg class="rating-icon"><title>Mathematics</title><use xlink:href="/img/fa/all.svg#math"/></svg></a></li></ol></div></div><script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");const imagesDark=document.querySelectorAll(".img-dark"),imagesLight=document.querySelectorAll(".img-light");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?(themeToggleLightIcon.classList.remove("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")})):(themeToggleDarkIcon.classList.remove("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}));function toggleTheme(e){e?(document.documentElement.classList.add("dark"),themeToggleLightIcon.classList.remove("hidden"),themeToggleDarkIcon.classList.add("hidden"),imagesDark.forEach(e=>{e.classList.remove("hidden")}),imagesLight.forEach(e=>{e.classList.add("hidden")})):(document.documentElement.classList.remove("dark"),themeToggleDarkIcon.classList.remove("hidden"),themeToggleLightIcon.classList.add("hidden"),imagesLight.forEach(e=>{e.classList.remove("hidden")}),imagesDark.forEach(e=>{e.classList.add("hidden")}))}themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),imagesLight.forEach(e=>{e.classList.toggle("hidden")}),imagesDark.forEach(e=>{e.classList.toggle("hidden")}),event.altKey?(localStorage.removeItem("color-theme"),toggleTheme(window.matchMedia("(prefers-color-scheme: dark)").matches)):localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))}),window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{if(localStorage.getItem("color-theme")===null){const t=e.matches?"dark":"light";toggleTheme(t==="dark")}})</script><footer class="z-20 p-4 md:p-6 mt-3 w-full border-t border-gray-200 shadow flex items-center justify-center dark:border-gray-600 bg-white dark:bg-zinc-800"><span class="dark:text-gray-400 text-center">© 2013—2024 <span class=whitespace-nowrap>Andrey Akinshin</span></span><ul class="flex flex-wrap items-center px-4 mb-1"><li><a href=https://github.com/AndreyAkinshin><svg class="fai fai-link w-5 h-5"><title>GitHub</title><use xlink:href="/img/fa/all.svg#github"/></svg></a></li><li><a href=https://twitter.com/andrey_akinshin><svg class="fai fai-link w-5 h-5"><title>Twitter</title><use xlink:href="/img/fa/all.svg#twitter"/></svg></a></li><li><a href=https://aakinshin.net/posts/index.xml><svg class="fai fai-link w-5 h-5"><title>RSS</title><use xlink:href="/img/fa/all.svg#rss"/></svg></a></li></ul><div class=main-content><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+=" has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></body></html>