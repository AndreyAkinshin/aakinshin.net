<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.74.3"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content><title>Andrey Akinshin's blog</title><meta name=description content="Andrey Akinshin's blog"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=/css/lumen-bootstrap.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/slate-bootstrap.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><link href=/css/syntax-light.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/syntax-dark.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.26d61b7027fe55e642f0001cef94cb75b567d721086492aab6b4e32d9ee7811d.js></script><script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.882998740b9c4c24677576b997bde5a487246e2e3bd72128a0e922eaa59db029.mjs></script><link href=https://aakinshin.net/css/fontawesome-all.min.50da8736f05b35aac9691de67fe993cbe01b7d88fcf0cb682c3a8f39933fe4a3.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.2a76db7cc14055b3bafacc363f453917ab54a818a7db2a0a9b2409c0f6bf07c5.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.ae812fc71daf5160d927febda5a991cee6c361345e3f685d3736931ed7537986.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-ZVB6MXSX32');</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-41419012-5"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-41419012-5');</script><script type=text/javascript>(function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym");ym(28700916,"init",{clickmap:true,trackLinks:true,accurateTrackBounce:true});</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class=nav-item><a class=nav-link id=nav-link-posts href=https://aakinshin.net/posts/>Posts</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li><li class=nav-item><a class=nav-link id=nav-link-pdnb href=https://aakinshin.net/prodotnetbenchmarking/>Pro .NET Benchmarking</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" data-toggle=dropdown href=# role=button aria-haspopup=true aria-expanded=false>EN</a><div class=dropdown-menu><a class=dropdown-item href=https://aakinshin.net/ru/>RU</a></div></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class=blog-post><h2 class=blog-post-title><a href=/posts/sfakianakis-verginis-quantile-estimator/>Sfakianakis-Verginis quantile estimator</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-03-09>March 9, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a></span><br><br><p>There are dozens of different ways to estimate quantiles.
One of these ways is to use the Sfakianakis-Verginis quantile estimator.
To be more specific, it&rsquo;s a family of three estimators.
If we want to estimate the <span class="math inline">\(p^\textrm{th}\)</span> quantile for sample <span class="math inline">\(X\)</span>,
we can use one of the following equations:</p><p><span class="math display">\[\begin{split}
\operatorname{SV1}_p =&
\frac{B_0}{2} \big( X_{(1)}+X_{(2)}-X_{(3)} \big) +
\sum_{i=1}^{n} \frac{B_i+B_{i-1}}{2} X_{(i)} +
\frac{B_n}{2} \big(- X_{(n-2)}+X_{(n-1)}-X_{(n)} \big),\\
\operatorname{SV2}_p =& \sum_{i=1}^{n} B_{i-1} X_{(i)} + B_n \cdot \big(2X_{(n)} - X_{(n-1)}\big),\\
\operatorname{SV3}_p =& \sum_{i=1}^n B_i X_{(i)} + B_0 \cdot \big(2X_{(1)}-X_{(2)}\big).
\end{split}
\]</span></p><p>where <span class="math inline">\(B_i = B(i; n, p)\)</span> is probability mass function os the binomial distribution <span class="math inline">\(B(n, p)\)</span>,
<span class="math inline">\(X_{(i)}\)</span> are order statistics of the sample <span class="math inline">\(X\)</span>.</p><p>In this post, I derive these equations following the paper
<a href=https://doi.org/10.1080/03610910701790491>&ldquo;A new family of nonparametric quantile estimators&rdquo;</a>
by Michael E. Sfakianakis and Dimitris G. Verginis.
Also, I add some additional explanations,
reconstruct missing steps,
simplify the final equations,
and provide reference implementations in C# and R.</p><br><a href=/posts/sfakianakis-verginis-quantile-estimator/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/winsorized-hdqe/>Winsorized modifications of the Harrell-Davis quantile estimator</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-03-02>March 2, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/winsorizing/ class="badge badge-info">Winsorizing</a>
<a href=https://aakinshin.net/tags/harrell-davis/ class="badge badge-info">Harrell-Davis</a></span><br><br><p>The Harrell-Davis quantile estimator is one of my favorite quantile estimators
because of its <a href=https://en.wikipedia.org/wiki/Efficiency_(statistics)>efficiency</a>.
It has a small mean square error which allows getting accurate estimations.
However, it has a severe drawback: it&rsquo;s not robust.
Indeed, since the estimator includes all sample elements with positive weights,
its <a href=https://en.wikipedia.org/wiki/Robust_statistics#Breakdown_point>breakdown point</a> is zero.</p><p>In this post, I want to suggest modifications of the Harrell-Davis quantile estimator
which increases its <em>robustness</em> keeping almost the same level of <em>efficiency</em>.</p><br><a href=/posts/winsorized-hdqe/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/misleading-stddev/>Misleading standard deviation</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-23>February 23, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/standard-deviation/ class="badge badge-info">Standard Deviation</a></span><br><br><p>The <a href=https://en.wikipedia.org/wiki/Standard_deviation>standard deviation</a> may be an extremely misleading metric.
Even minor deviations from normality could make it completely unreliable and deceiving.
Let me demonstrate this problem using an example.</p><p>Below you can see three density plots of some distributions.
Could you guess their standard deviations?</p><div class=row><div class=mx-auto><a href=/posts/misleading-stddev/img/density1-light.png target=_blank class=imgldlink alt=density1><picture>
<source theme=dark srcset=/posts/misleading-stddev/img/density1-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/misleading-stddev/img/density1-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/misleading-stddev/img/density1-light.png></picture></a></div></div><br><p>The correct answers are <span class="math inline">\(1.0, 3.0, 11.0\)</span>.
And here is a more challenging problem: could you match these values with the corresponding distributions?</p><br><a href=/posts/misleading-stddev/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/unbiased-mad-hd/>Unbiased median absolute deviation based on the Harrell-Davis quantile estimator</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-16>February 16, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/harrell-davis/ class="badge badge-info">Harrell-Davis</a></span><br><br><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
In the previous post, I <a href=https://aakinshin.net/posts/unbiased-mad/>showed</a>
how to use the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>unbiased</a>
version of the <span class="math inline">\(\textrm{MAD}\)</span> estimator
as a robust alternative to the standard deviation.
&ldquo;Unbiasedness&rdquo; means that such estimator&rsquo;s expected value equals the true value of the standard deviation.
Unfortunately, there is such thing as the <a href=https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>bias–variance tradeoff</a>:
when we remove the bias of the <span class="math inline">\(\textrm{MAD}\)</span> estimator,
we increase its variance and mean squared error (<span class="math inline">\(\textrm{MSE}\)</span>).</p><p>In this post, I want to suggest a more <a href=https://en.wikipedia.org/wiki/Efficiency_(statistics)>efficient</a>
unbiased <span class="math inline">\(\textrm{MAD}\)</span> estimator.
It&rsquo;s also a consistent estimator for the standard deviation, but it has smaller <span class="math inline">\(\textrm{MSE}\)</span>.
To build this estimator,
we should replace the classic &ldquo;straightforward&rdquo; median estimator with the Harrell-Davis quantile estimator
and adjust bias-correction factors.
Let&rsquo;s discuss this approach in detail.</p><br><a href=/posts/unbiased-mad-hd/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/unbiased-mad/>Unbiased median absolute deviation</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-09>February 9, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a></span><br><br><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
For distribution <span class="math inline">\(X\)</span>, it can be calculated as follows:</p><p><span class="math display">\[\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]</span></p><p>where <span class="math inline">\(C\)</span> is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the <span class="math inline">\(\textrm{MAD}\)</span> as a <a href=https://en.wikipedia.org/wiki/Consistent_estimator>consistent estimator</a>
for the standard deviation under the normal distribution,
we should set</p><p><span class="math display">\[C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
\]</span></p><p>where <span class="math inline">\(\Phi^{-1}\)</span> is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If <span class="math inline">\(X\)</span> is the normal distribution, we get <span class="math inline">\(\textrm{MAD} = \sigma\)</span> where <span class="math inline">\(\sigma\)</span> is the standard deviation.</p><p>Not let&rsquo;s consider a sample <span class="math inline">\(x = \{ x_1, x_2, \ldots x_n \}\)</span>.
Let&rsquo;s denote the median absolute deviation for a sample of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\textrm{MAD}_n\)</span>.
The corresponding equation looks similar to the definition of <span class="math inline">\(\textrm{MAD}\)</span> for a distribution:</p><p><span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
\]</span></p><p>Let&rsquo;s assume that <span class="math inline">\(\textrm{median}\)</span> is the straightforward definition of the median
(if <span class="math inline">\(n\)</span> is odd, the median is the middle element of the sorted sample,
if <span class="math inline">\(n\)</span> is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use <span class="math inline">\(C_n = C_{\infty}\)</span> for extremely large sample sizes.
However, for small <span class="math inline">\(n\)</span>, <span class="math inline">\(\textrm{MAD}_n\)</span> becomes a <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>biased estimator</a>.
If we want to get an unbiased version, we should adjust the value of <span class="math inline">\(C_n\)</span>.</p><p>In this post, we look at the possible approaches and learn the way to get the exact value of <span class="math inline">\(C_n\)</span>
that makes <span class="math inline">\(\textrm{MAD}_n\)</span> unbiased estimator of the median absolute deviation for any <span class="math inline">\(n\)</span>.</p><br><a href=/posts/unbiased-mad/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/comparing-distributions-using-gamma-es/>Comparing distribution quantiles using gamma effect size</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-02>February 2, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/effect-size/ class="badge badge-info">Effect Size</a>
<a href=https://aakinshin.net/tags/gamma-effect-size/ class="badge badge-info">Gamma Effect Size</a></span><br><br><p>There are several ways to describe the difference between two distributions.
Here are a few examples:</p><ul><li>Effect sizes based on differences between means (e.g., Cohen&rsquo;s d, Glass&rsquo; Δ, Hedges&rsquo; g)</li><li><a href=https://aakinshin.net/posts/shift-and-ratio-functions/>The shift and ration functions</a> that
estimate differences between matched quantiles.</li></ul><p>In one of the previous post, I <a href=https://aakinshin.net/posts/nonparametric-effect-size/>described</a>
the gamma effect size which is defined not for the mean but for quantiles.
In this post, I want to share a few case studies that demonstrate
how the suggested metric combines the advantages of the above approaches.</p><br><a href=/posts/comparing-distributions-using-gamma-es/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/cohend-and-outliers/>A single outlier could completely distort your Cohen's d value</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-26>January 26, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/effect-size/ class="badge badge-info">Effect Size</a>
<a href=https://aakinshin.net/tags/gamma-effect-size/ class="badge badge-info">Gamma Effect Size</a></span><br><br><p><a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&rsquo;s d</a> is a popular way to estimate
the <a href=https://en.wikipedia.org/wiki/Effect_size>effect size</a> between two samples.
It works excellent for perfectly normal distributions.
Usually, people think that slight deviations from normality
shouldn&rsquo;t produce a noticeable impact on the result.
Unfortunately, it&rsquo;s not always true.
In fact, a single outlier value can completely distort the result even in large samples.</p><p>In this post, I will present some illustrations for this problem and will show how to fix it.</p><br><a href=/posts/cohend-and-outliers/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/partitioning-heaps-quantile-estimator2/>Better moving quantile estimations using the partitioning heaps</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-19>January 19, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a></span><br><br><p>In one of the previous posts, I <a href=https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/>have discussed</a> the Hardle-Steiger method.
This algorithm allows estimating <a href=https://en.wikipedia.org/wiki/Moving_average#Moving_median>the moving median</a>
using <span class="math inline">\(O(L)\)</span> memory and <span class="math inline">\(O(log(L))\)</span> element processing complexity (where <span class="math inline">\(L\)</span> is the window size).
Also, I have shown how to adapt this approach to estimate <em>any</em> moving quantile.</p><p>In this post, I&rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the <a href=https://en.wikipedia.org/wiki/Order_statistic>order statistics</a>
which is the <span class="math inline">\(k\textrm{th}\)</span> smallest element from the sample.
It means that the estimated quantile value always equals one of the last <span class="math inline">\(L\)</span> observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for <span class="math inline">\(x = \{4, 5, 6, 7\}\)</span>,
some estimators return <span class="math inline">\(5.5\)</span> (which is the arithmetical mean of <span class="math inline">\(5\)</span> and <span class="math inline">\(6\)</span>)
instead of <span class="math inline">\(5\)</span> or <span class="math inline">\(6\)</span> (which are order statistics).</p><p>Let&rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.</p><br><a href=/posts/partitioning-heaps-quantile-estimator2/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/mp2-quantile-estimator/>MP² quantile estimator: estimating the moving median without storing values</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-12>January 12, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a></span><br><br><p>In one of the previous posts, I <a href=https://aakinshin.net/posts/p2-quantile-estimator/>described</a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.</p><p>After the publication, I got a lot of questions about <em>moving</em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I <a href=https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/>wrote</a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last <span class="math inline">\(L\)</span> numbers
(<span class="math inline">\(L\)</span> is known as the window size).
However, it requires <span class="math inline">\(O(L)\)</span> memory, and it takes <span class="math inline">\(O(log(L))\)</span> time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&rsquo;t allow implementing low-overhead telemetry in the case of large <span class="math inline">\(L\)</span>.</p><p>In this post, I&rsquo;m going to present a moving modification of the P² quantile estimator.
Let&rsquo;s call it MP² (moving P²).
It requires <span class="math inline">\(O(1)\)</span> memory, it takes <span class="math inline">\(O(1)\)</span> to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.</p><p>Let&rsquo;s discuss MP² in detail!</p><br><a href=/posts/mp2-quantile-estimator/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/cs-mad-hd-gumbel/>Case study: Accuracy of the MAD estimation using the Harrell-Davis quantile estimator (Gumbel distribution)</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-05>January 5, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/harrell-davis/ class="badge badge-info">Harrell-Davis</a></span><br><br><p>In some of my previous posts, I used
the <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (MAD)
to describe the distribution dispersion:</p><ul><li><a href=https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/>DoubleMAD outlier detector based on the Harrell-Davis quantile estimator</a></li><li><a href=https://aakinshin.net/posts/nonparametric-effect-size/>Nonparametric Cohen&rsquo;s d-consistent effect size</a></li><li><a href=https://aakinshin.net/posts/qad/>Quantile absolute deviation: estimating statistical dispersion around quantiles</a></li></ul><p>The MAD estimation depends on the chosen median estimator:
we may get different MAD values with different median estimators.
To get better accuracy,
I always encourage readers to use the Harrell-Davis quantile estimator
instead of the classic Type 7 quantile estimator.</p><p>In this case study, I decided to compare these two quantile estimators using
the <a href=https://en.wikipedia.org/wiki/Gumbel_distribution>Gumbel distribution</a>
(it&rsquo;s a good model for slightly right-skewed distributions).
According to the performed Monte Carlo simulation,
the Harrell-Davis quantile estimator always has better accuracy:</p><div class=row><div class=mx-auto><a href=/posts/cs-mad-hd-gumbel/img/summary-light.png target=_blank class=imgldlink alt=summary><picture>
<source theme=dark srcset=/posts/cs-mad-hd-gumbel/img/summary-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/cs-mad-hd-gumbel/img/summary-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/cs-mad-hd-gumbel/img/summary-light.png></picture></a></div></div><br><br><a href=/posts/cs-mad-hd-gumbel/>Read more</a><br><br><hr></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/8/>8</a></li><li class=page-item><a href=/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/8/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></main></div><footer class=blog-footer><div class=container><p>&copy; 2013–2021 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub style=color:#fff></i></a><a href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter style=color:#fff></i></a><a href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS style=color:#fff></i></a>|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.082780cb948cc57eabbc0d80172a7a5347d5572b599938b4c9876dfd7978b424.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/anchor.min.js></script><script src=https://aakinshin.net/js/custom.min.11932490dde776463ed165b345838c701accc0cfdedf2e0868f13415f41f0872.js></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true,},options:{skipHtmlTags:['script','noscript','style','textarea','pre'],ignoreHtmlClass:'tex2jax_ignore',processHtmlClass:'tex2jax_process'}};</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>