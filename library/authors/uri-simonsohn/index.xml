<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Uri Simonsohn on Andrey Akinshin</title><link>https://aakinshin.net/library/authors/uri-simonsohn/</link><description>Recent content in Uri Simonsohn on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://aakinshin.net/library/authors/uri-simonsohn/index.xml" rel="self" type="application/rss+xml"/><item><title>Easy to publish</title><link>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/9ef2c0c7-a5da-4d76-8169-2d2d97aae36a/</guid><description>Our job as scientists is to discover truths about the world. We generate hypotheses, collect data, and examine whether or not the data are consistent with those hypotheses. Although we aspire to always be accurate, errors are inevitable. Perhaps the most costly error is a false positive, the incorrect rejection of a null hypothesis. First, once they appear in the literature, false positives are particularly persistent. Because null results have many possible causes, failures to replicate previous findings are never conclusive. Furthermore, because it is uncommon for prestigious journals to publish null findings or exact replications, researchers have little incentive to even attempt them. Second, false positives waste resources: They inspire investment in fruitless research programs and can lead to ineffective policy changes. Finally, a field known for publishing false positives risks losing its credibility. In this article, we show that despite the nominal endorsement of a maximum false-positive rate of 5% (i.e., p ≤ .05), current standards for disclosing details of data collection and analyses make false positives vastly more likely. In fact, it is unacceptably easy to publish “statistically significant” evidence consistent with any hypothesis.
(Emphasis is mine.)</description></item><item><title>False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant</title><link>https://aakinshin.net/library/papers/simmons2011/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/simmons2011/</guid><description>Reference Joseph P Simmons, Leif D Nelson, Uri Simonsohn “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant” (2011) // Psychological Science. Publisher: SAGE Publications. Vol. 22. No 11. Pp. 1359–1366. DOI: 10.1177/0956797611417632
Bib @Article{simmons2011, title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}, volume = {22}, issn = {1467-9280}, url = {http://dx.doi.org/10.1177/0956797611417632}, doi = {10.1177/0956797611417632}, number = {11}, journal = {Psychological Science}, publisher = {SAGE Publications}, author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri}, year = {2011}, month = {oct}, pages = {1359–1366} }</description></item><item><title>Guidelines for reviewers</title><link>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/1d8a5c7b-087b-4a6e-b60e-f144dd8a0f0b/</guid><description>We propose the following four guidelines for reviewers.
Reviewers should ensure that authors follow the requirements. Review teams are the gatekeepers of the scientific community, and they should encourage authors not only to rule out alternative explanations, but also to more convincingly demonstrate that their findings are not due to chance alone. This means prioritizing transparency over tidiness; if a wonderful study is partially marred by a peculiar exclusion or an inconsistent condition, those imperfections should be retained. If reviewers require authors to follow these requirements, they will. Reviewers should be more tolerant of imperfections in results. One reason researchers exploit researcher degrees of freedom is the unreasonable expectation we often impose as reviewers for every data pattern to be (significantly) as predicted. Underpowered studies with perfect results are the ones that should invite extra scrutiny. Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions. Even if authors follow all of our guidelines, they will necessarily still face arbitrary decisions. For example, should they subtract the baseline measure of the dependent variable from the final result or should they use the baseline measure as a covariate? When there is no obviously correct way to answer questions like this, the reviewer should ask for alternatives. For example, reviewer reports might include questions such as, “Do the results also hold if the baseline measure is instead used as a covariate?” Similarly, reviewers should ensure that arbitrary decisions are used consistently across studies (e.g., “Do the results hold for Study 3 if gender is entered as a covariate, as was done in Study 2?”).5 If a result holds only for one arbitrary specification, then everyone involved has learned a great deal about the robustness (or lack thereof) of the effect. If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication.</description></item><item><title>It Does Not Follow: Evaluating the One-Off Publication Bias Critiques by Francis (2012a, 2012b, 2012c, 2012d, 2012e, in press)</title><link>https://aakinshin.net/library/papers/simonsohn2012/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/simonsohn2012/</guid><description>Reference Uri Simonsohn “It Does Not Follow: Evaluating the One-Off Publication Bias Critiques by Francis (2012a, 2012b, 2012c, 2012d, 2012e, in press)” (2012) // Perspectives on Psychological Science. Publisher: SAGE Publications. Vol. 7. No 6. Pp. 597–599. DOI: 10.1177/1745691612463399
Bib @Article{simonsohn2012, title = {It Does Not Follow: Evaluating the One-Off Publication Bias Critiques by Francis (2012a, 2012b, 2012c, 2012d, 2012e, in press)}, volume = {7}, issn = {1745-6924}, url = {http://dx.doi.org/10.1177/1745691612463399}, doi = {10.1177/1745691612463399}, number = {6}, journal = {Perspectives on Psychological Science}, publisher = {SAGE Publications}, author = {Simonsohn, Uri}, year = {2012}, month = {nov}, pages = {597–599} }</description></item><item><title>Requirements for authors</title><link>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/2915ea66-476d-46bc-a79a-43ebe0c730e2/</guid><description>We propose the following six requirements for authors.
Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. Following this requirement may mean reporting the outcome of power calculations or disclosing arbitrary rules, such as “we decided to collect 100 observations” or “we decided to collect as many observations as we could before the end of the semester.” The rule itself is secondary, but it must be determined ex ante and be reported. Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification. This requirement offers extra protection for the first requirement. Samples smaller than 20 per cell are simply not powerful enough to detect most effects, and so there is usually no good reason to decide in advance to collect such a small number of observations. Smaller samples, it follows, are much more likely to reflect interim data analysis and a flexible termination rule. In addition, as Figure 1 shows, larger minimum sample sizes can lessen the impact of violating Requirement 1. Authors must list all variables collected in a study. This requirement prevents researchers from reporting only a convenient subset of the many measures that were collected, allowing readers and reviewers to easily identify possible researcher degrees of freedom. Because authors are required to just list those variables rather than describe them in detail, this requirement increases the length of an article by only a few words per otherwise shrouded variable. We encourage authors to begin the list with “only,” to assure readers that the list is exhaustive (e.g., “participants reported only their age and gender”). Authors must report all experimental conditions, including failed manipulations. This requirement prevents authors from selectively choosing only to report the condition comparisons that yield results that are consistent with their hypothesis.</description></item></channel></rss>