<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Alex Reinhart on Andrey Akinshin</title><link>https://aakinshin.net/library/authors/alex-reinhart/</link><description>Recent content in Alex Reinhart on Andrey Akinshin</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://aakinshin.net/library/authors/alex-reinhart/index.xml" rel="self" type="application/rss+xml"/><item><title>Analytically Calculating Power Can Be Difﬁcult or Downright Impossible</title><link>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ff049ff8-818f-4005-a0df-a2104d3c4fb5/</guid><description>Math is another possible explanation for why power calculations are so uncommon: analytically calculating power can be difﬁcult or downright impossible. Techniques for calculating power are not frequently taught in intro statistics courses. And some commercially available statistical software does not come with power calculation functions. It is possible to avoid hairy mathematics by simply simulating thousands of artiﬁcial datasets with the effect size you expect and running your statistical tests on the simulated data. The power is simply the fraction of datasets for which you obtain a statistically signiﬁcant result. But this approach requires programming experience, and simulating realistic data can be tricky.</description></item><item><title>Counterintuitiveness of Significance Testing</title><link>https://aakinshin.net/library/quotes/87d5d946-bcbf-4c7d-921f-d7d2f680a5d4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/87d5d946-bcbf-4c7d-921f-d7d2f680a5d4/</guid><description>This is a counterintuitive feature of significance testing: if you want to prove that your drug works, you do so by showing the data is inconsistent with the drug not working.</description></item><item><title>Critique of McClintock's Study of Human Menstrual Cycles</title><link>https://aakinshin.net/library/quotes/79121729-78ab-4ef3-a3da-409b3596273c/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/79121729-78ab-4ef3-a3da-409b3596273c/</guid><description>McClintock’s study of human menstrual cycles went something like this:
Find groups of women who live together in close contactfor instance, college students in dormitories. Every month or so, ask each woman when her last menstrual period began and to list the other women with whom she spent the most time. Use these lists to split the women into groups that tend to spend time together. For each group of women, see how far the average woman’s period start date deviates from the average. Small deviations would mean the women’s cycles were aligned, all starting at around the same time. Then the researchers tested whether the deviations decreased over time, which would indicate that the women were synchronizing. To do this, they checked the mean deviation at ﬁve different points throughout the study, testing whether the deviation decreased more than could be expected by chance.
Unfortunately, the statistical test they used assumed that if there was no synchronization, the deviations would randomly increase and decrease from one period to another. But imagine two women in the study who start with aligned cycles. One has an average gap of 28 days between periods and the other a gap of roughly 30 days. Their cycles will diverge consistently over the course of the study, starting two days apart, then four days, and so on, with only a bit of random variation because periods are not perfectly timed. Similarly, two women can start the study not aligned but gradually align.
For comparison, if you’ve ever been stuck in traffic, you’ve probably seen how two turn signals blinking at different rates will gradually synchronize and then go out of phase again. If you’re stuck at the intersection long enough, you’ll see this happen multiple times. But to the best of my knowledge, there are no turn signal pheromones.</description></item><item><title>How to Lie with Smoking Statistics</title><link>https://aakinshin.net/library/quotes/c7d9734a-45b7-4537-a692-93abb79a4e25/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/c7d9734a-45b7-4537-a692-93abb79a4e25/</guid><description>Attempting to capitalize on Huff’s respected status, the tobacco industry commissioned him to testify before Congress and then to write a book, tentatively titled &amp;lsquo;How to Lie with Smoking Statistics&amp;rsquo;, covering the many statistical and logical errors alleged to be found in the surgeon general’s report. Huff completed a manuscript, for which he was paid more than $9,000 > (roughly $60,000 in 2014 dollars) by tobacco companies and which was positively reviewed by University of Chicago statistician (and paid tobacco industry consultant) K.A. Brownlee. Although it was never published, it’s likely that Huff’s friendly, accessible style would have made a strong impression on the public, providing talking points for watercooler arguments.
Read more: reinhart2014.</description></item><item><title>Huff and Puff</title><link>https://aakinshin.net/library/papers/reinhart2014/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/papers/reinhart2014/</guid><description>Reference Alex Reinhart “Huff and Puff” (2014) // Significance. Publisher: Oxford University Press (OUP). Vol. 11. No 4. Pp. 28–33. DOI: 10.1111/j.1740-9713.2014.00765.x
Bib @Article{reinhart2014, title = {Huff and Puff}, volume = {11}, issn = {1740-9713}, url = {http://dx.doi.org/10.1111/j.1740-9713.2014.00765.x}, doi = {10.1111/j.1740-9713.2014.00765.x}, number = {4}, journal = {Significance}, publisher = {Oxford University Press (OUP)}, author = {Reinhart, Alex}, year = {2014}, month = {oct}, pages = {28–33} }</description></item><item><title>Journal’s Impact Factor and Overestimation of Effect Sizes</title><link>https://aakinshin.net/library/quotes/544f89c6-188a-488a-a13a-585caf62ef81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/544f89c6-188a-488a-a13a-585caf62ef81/</guid><description>Some evidence suggests a correlation between a journal’s impact factor (a rough measure of its prominence and importance) and the factor by which its studies overestimate effect sizes. Studies that produce less “exciting” results are closer to the truth but less interesting to a major journal editor.</description></item><item><title>Noise and Real Effects</title><link>https://aakinshin.net/library/quotes/ca6fb0ae-cfb4-406b-b47d-603f05d13cd6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/ca6fb0ae-cfb4-406b-b47d-603f05d13cd6/</guid><description>A statistically insignificant difference could be nothing but noise, or it could represent a real effect that can be pinned down only with more data.</description></item><item><title>On the Triumph of Mediocrity in Business</title><link>https://aakinshin.net/library/quotes/44145d2a-d5dc-41ea-8376-174d942ed091/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/44145d2a-d5dc-41ea-8376-174d942ed091/</guid><description>A final, famous example dates back to 1933, when the field of mathematical statistics was in its infancy. Horace Secrist, a statistics professor at Northwestern University, published The Triumph of Mediocrity in Business, which argued that unusually successful businesses tend to become less successful and unsuccessful businesses tend to become more successful: proof that businesses trend toward mediocrity. This was not a statistical artifact, he argued, but a result of competitive market forces. Secrist supported his argument with reams of data and numerous charts and graphs and even cited some of Galton’s work in regression to the mean. Evidently, Secrist did not understand Galton’s point.</description></item><item><title>Sphygmomanometer</title><link>https://aakinshin.net/library/quotes/a2525cc5-5f08-47f3-a5af-da84d9e94d9b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/a2525cc5-5f08-47f3-a5af-da84d9e94d9b/</guid><description> Or perhaps I’m worried that my sphygmomanometers are not perfectly calibrated, so I measure with a different one each day.
I just wanted an excuse to use the word sphygmomanometer.</description></item><item><title>Statistical Reforms</title><link>https://aakinshin.net/library/quotes/efcae53d-3220-4ef9-9a3d-2d2925a667c2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/efcae53d-3220-4ef9-9a3d-2d2925a667c2/</guid><description>In recent years there have been many advocates for statistical reform, and naturally there is disagreement among them on the best method to address these problems. Some insist that p values, which I will show are frequently misleading and confusing, should be abandoned altogether; others advocate a “new statistics” based on confidence intervals. Still others suggest a switch to new Bayesian methods that give more-interpretable results, while others believe statistics as it’s currently taught is just fine but used poorly.</description></item><item><title>Statistics Done Wrong: The Woefully Complete Guide</title><link>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/books/reinhart-statistics-done-wrong/</guid><description/></item><item><title>Tentative Exploratory Findings</title><link>https://aakinshin.net/library/quotes/976fa15f-840e-4f3c-a764-76f2fc7dd657/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/976fa15f-840e-4f3c-a764-76f2fc7dd657/</guid><description>But aimlessly exploring data means a lot of opportunities for false positives and truth inflation. If in your explorations you find an interesting correlation, the standard procedure is to collect a new dataset and test the hypothesis again. Testing an independent dataset will filter out false positives and leave any legitimate discoveries standing. (Of course, you’ll need to ensure your test dataset is sufficiently powered to replicate your findings.) And so exploratory findings should be considered tentative until confirmed.
If you don’t collect a new dataset or your new dataset is strongly related to the old one, truth inflation will come back to bite you in the butt.</description></item><item><title>Truth Inflation</title><link>https://aakinshin.net/library/quotes/48424c6e-3e04-4281-87fd-2b2ca1ef057d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/48424c6e-3e04-4281-87fd-2b2ca1ef057d/</guid><description>This effect, known as truth inflation, type M error (M for magnitude), or the winner’s curse, occurs in fields where many researchers conduct similar experiments and compete to publish the most “exciting” results: pharmacological trials, epidemiological studies, gene association studies (“gene A causes condition B”), and psychological studies often show symptoms, along with some of the most-cited papers in the medical literature.</description></item><item><title>Truth Inflation and Groundbreaking Effect Sizes in top-ranked Journals</title><link>https://aakinshin.net/library/quotes/3107d5e7-d81b-48f9-926e-591261332ef5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/3107d5e7-d81b-48f9-926e-591261332ef5/</guid><description>Consider also that top-ranked journals, such as Nature and Science, prefer to publish studies with groundbreaking results—meaning large effect sizes in novel fields with little prior research. This is a perfect combination for chronic truth inflation. Some evidence suggests a correlation between a journal’s impact factor (a rough measure of its prominence and importance) and the factor by which its studies overestimate effect sizes. Studies that produce less “exciting” results are closer to the truth but less interesting to a major journal editor. brembs2013 siontis2011</description></item><item><title>Your Eyeball</title><link>https://aakinshin.net/library/quotes/36f7123e-0f36-4dc6-9699-12bcf8a85455/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aakinshin.net/library/quotes/36f7123e-0f36-4dc6-9699-12bcf8a85455/</guid><description>Your eyeball is not a well-deﬁned statistical procedure.</description></item></channel></rss>