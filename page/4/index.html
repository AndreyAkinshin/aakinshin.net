<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.74.3"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content><title>Andrey Akinshin's blog (Page 4)</title><meta name=description content="Andrey Akinshin's blog"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=/css/lumen-bootstrap.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/slate-bootstrap.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><link href=/css/syntax-light.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=/css/syntax-dark.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.26d61b7027fe55e642f0001cef94cb75b567d721086492aab6b4e32d9ee7811d.js></script><script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.882998740b9c4c24677576b997bde5a487246e2e3bd72128a0e922eaa59db029.mjs></script><link href=https://aakinshin.net/css/fontawesome-all.min.50da8736f05b35aac9691de67fe993cbe01b7d88fcf0cb682c3a8f39933fe4a3.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.2a76db7cc14055b3bafacc363f453917ab54a818a7db2a0a9b2409c0f6bf07c5.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.ae812fc71daf5160d927febda5a991cee6c361345e3f685d3736931ed7537986.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-ZVB6MXSX32');</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-41419012-5"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-41419012-5');</script><script type=text/javascript>(function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym");ym(28700916,"init",{clickmap:true,trackLinks:true,accurateTrackBounce:true});</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class=nav-item><a class=nav-link id=nav-link-posts href=https://aakinshin.net/posts/>Posts</a></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li><li class=nav-item><a class=nav-link id=nav-link-pdnb href=https://aakinshin.net/prodotnetbenchmarking/>Pro .NET Benchmarking</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" data-toggle=dropdown href=# role=button aria-haspopup=true aria-expanded=false>EN</a><div class=dropdown-menu><a class=dropdown-item href=https://aakinshin.net/ru/>RU</a></div></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class=blog-post><h2 class=blog-post-title><a href=/posts/discrete-performance-distributions/>Discrete performance distributions</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-06-15>June 15, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a></span><br><br><p>When we collect software performance measurements, we get a bunch of time intervals.
Typically, we tend to interpret time values as continuous values.
However, the obtained values are actually discrete due to the limited resolution of our measurement tool.
In simple cases, we can treat these discrete values as continuous and get meaningful results.
Unfortunately, discretization may produce strange phenomena like pseudo-multimodality or zero dispersion.
If we want to set up a reliable system that automatically analyzes such distributions,
we should be aware of such problems so we could correctly handle them.</p><p>In this post, I want to share a few of discretization problems in real-life performance data sets
(based on the <a href=https://www.jetbrains.com/rider/>Rider</a> performance tests).</p><br><a href=/posts/discrete-performance-distributions/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/nonparametric-effect-size2/>Customization of the nonparametric Cohen's d-consistent effect size</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-06-08>June 8, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/effect-size/ class="badge badge-info">Effect Size</a>
<a href=https://aakinshin.net/tags/gamma-effect-size/ class="badge badge-info">Gamma Effect Size</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a></span><br><br><p>One year ago, I publish a post called <a href=https://aakinshin.net/posts/nonparametric-effect-size/>Nonparametric Cohen's d-consistent effect size</a>.
During this year, I got a lot of internal and external feedback from
my own statistical experiments and
<a href=https://twitter.com/ViljamiSairanen/status/1400457118340108293>people</a>
<a href=https://sherbold.github.io/autorank/autorank/>who</a>
<a href=https://github.com/Ramon-Diaz/Thesis-Project/blob/85df6b11050c7e05c4394d873585f701a7e3f32e/_util.py#L100>tried</a>
to use the suggested approach.
It seems that the nonparametric version of Cohen&rsquo;s d works much better with real-life not-so-normal data.
While the classic Cohen&rsquo;s d based on
the non-robust arithmetic mean and
the <a href=https://aakinshin.net/posts/misleading-stddev/>non-robust standard deviation</a>
can be easily <a href=https://aakinshin.net/posts/cohend-and-outliers/>corrupted by a single outlier</a>,
my approach is much more resistant to unexpected extreme values.
Also, it allows exploring
<a href=https://aakinshin.net/posts/comparing-distributions-using-gamma-es/>the difference between specific quantiles of considered samples</a>,
which can be useful in the non-parametric case.</p><p>However, I wasn&rsquo;t satisfied with the results of all of my experiments.
While I still like the basic idea
(replace the mean with the median; replace the standard deviation with the median absolute deviation),
it turned out that the final results heavily depend on the used quantile estimator.
To be more specific, the original Harrell-Davis quantile estimator is not always optimal;
in most cases, it&rsquo;s better to replace it with its <a href=https://aakinshin.net/posts/trimmed-hdqe/>trimmed</a> modification.
However, the particular choice of the quantile estimators depends on the situation.
Also, the consistency constant for the median absolute deviation
should be adjusted according to the current sample size and the used quantile estimator.
Of course, it also can be replaced by other dispersion estimators
that can be used as consistent estimators of the standard deviation.</p><p>In this post, I want to get a brief overview of possible customizations of the suggested metrics.</p><br><a href=/posts/nonparametric-effect-size2/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/robust-statistical-efficiency/>Robust alternative to statistical efficiency</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-06-01>June 1, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/statistical-efficiency/ class="badge badge-info">Statistical efficiency</a></span><br><br><p>Statistical efficiency is a common measure of the quality of an estimator.
Typically, it&rsquo;s expressed via the mean square error (<span class="math inline">\(\operatorname{MSE}\)</span>).
For the given estimator <span class="math inline">\(T\)</span> and the true parameter value <span class="math inline">\(\theta\)</span>,
the <span class="math inline">\(\operatorname{MSE}\)</span> can be expressed as follows:</p><p><span class="math display">\[\operatorname{MSE}(T) = \operatorname{E}[(T-\theta)^2]
\]</span></p><p>In numerical simulations, the <span class="math inline">\(\operatorname{MSE}\)</span> can&rsquo;t be used as a robust metric
because its breakdown point is zero
(a corruption of a single measurement leads to a corrupted result).
Typically, it&rsquo;s not a problem for light-tailed distributions.
Unfortunately, in the heavy-tailed case,
the <span class="math inline">\(\operatorname{MSE}\)</span> becomes an unreliable and unreproducible metric
because it can be easily spoiled by a single outlier.</p><p>I suggest an alternative way to compare statistical estimators.
Instead of using non-robust <span class="math inline">\(\operatorname{MSE}\)</span>,
we can use robust quantile estimations of the absolute error distribution.
In this post, I want to share numerical simulations
that show a problem of irreproducible <span class="math inline">\(\operatorname{MSE}\)</span> values
and how they can be replaced by reproducible quantile values.</p><br><a href=/posts/robust-statistical-efficiency/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/customized-wthdqe/>Improving the efficiency of the Harrell-Davis quantile estimator for special cases using custom winsorizing and trimming strategies</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-05-25>May 25, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/statistical-efficiency/ class="badge badge-info">Statistical efficiency</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a>
<a href=https://aakinshin.net/tags/winsorizing/ class="badge badge-info">Winsorizing</a>
<a href=https://aakinshin.net/tags/trimming/ class="badge badge-info">Trimming</a>
<a href=https://aakinshin.net/tags/small-samples/ class="badge badge-info">Small samples</a>
<a href=https://aakinshin.net/tags/research-thdqe/ class="badge badge-info">Research: Building trimmed Harrell-Davis quantile estimator</a></span><br><br><p><strong>Update: this blog post is a part of research that aimed to build a statistically efficient and robust quantile estimator.
A <a href=https://aakinshin.net/posts/preprint-thdqe/>preprint with final results</a> is available on arXiv:
<a href=https://arxiv.org/abs/2111.11776>arXiv:2111.11776 [stat.ME]</a>.</strong></p><p>Let&rsquo;s say we want to
<strong>estimate the median</strong>
based on a <strong>small sample</strong> (3 <span class="math inline">\(\leq n \leq 7\)</span>)
from a <strong>right-skewed heavy-tailed distribution</strong>
with <strong>high statistical efficiency</strong>.</p><p>The traditional median estimator is the most robust estimator, but it&rsquo;s not the most efficient one.
Typically, the Harrell-Davis quantile estimator provides better efficiency,
but it&rsquo;s not robust (its breakdown point is zero),
so it may have worse efficiency in the given case.
The <a href=https://aakinshin.net/posts/winsorized-hdqe/>winsorized</a> and <a href=https://aakinshin.net/posts/trimmed-hdqe/>trimmed</a>
modifications of the Harrell-Davis quantile estimator provide a good trade-off
between efficiency and robustness, but they require a proper winsorizing/trimming rule.
A reasonable choice of such a rule for medium-size samples is based on the highest density interval of the Beta function
(as described <a href=https://aakinshin.net/posts/winsorized-hdqe/>here</a>).
Unfortunately, this approach may be suboptimal for small samples.
E.g., if we use the 99% highest density interval to estimate the median,
it starts to trim sample values only for <span class="math inline">\(n \geq 8\)</span>.</p><p>In this post, we are going to discuss custom winsorizing/trimming strategies for special cases of the quantile estimation problem.</p><br><a href=/posts/customized-wthdqe/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/hd-sv-no-efficiency/>Comparing the efficiency of the Harrell-Davis, Sfakianakis-Verginis, and Navruz-Özdemir quantile estimators</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-05-18>May 18, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/statistical-efficiency/ class="badge badge-info">Statistical efficiency</a></span><br><br><p>In the previous posts, I discussed the statistical efficiency of different quantile estimators
(<a href=https://aakinshin.net/posts/hdqe-efficiency/>Efficiency of the Harrell-Davis quantile estimator</a> and
<a href=https://aakinshin.net/posts/wthdqe-efficiency/>Efficiency of the winsorized and trimmed Harrell-Davis quantile estimators</a>).</p><p>In this post, I continue this research and compare the efficiency of
the Harrell-Davis quantile estimator,
the <a href=https://aakinshin.net/posts/sfakianakis-verginis-quantile-estimator/>Sfakianakis-Verginis quantile estimators</a>, and
the <a href=https://aakinshin.net/posts/navruz-ozdemir-quantile-estimator/>Navruz-Özdemir quantile estimator</a>.</p><div class=row><div class=mx-auto><a href=/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png target=_blank class=imgldlink alt=LightAndHeavy_N10_Efficiency><picture>
<source theme=dark srcset=/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/hd-sv-no-efficiency/img/LightAndHeavy_N10_Efficiency-light.png></picture></a></div></div><br><br><a href=/posts/hd-sv-no-efficiency/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/dispersion-exponential-smoothing/>Dispersion exponential smoothing</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-05-11>May 11, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a>
<a href=https://aakinshin.net/tags/exponential-smoothing/ class="badge badge-info">Exponential smoothing</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/iqr/ class="badge badge-info">IQR</a></span><br><br><p>In this <a href=https://aakinshin.net/posts/quantile-exponential-smoothing/>previous post</a>,
I showed how to apply exponential smoothing to quantiles
using the <a href=https://aakinshin.net/posts/weighted-quantiles/>weighted Harrell-Davis quantile estimator</a>.
This technique allows getting smooth and stable moving median estimations.
In this post, I&rsquo;m going to discuss how to use the same approach
to estimate moving dispersion.</p><br><a href=/posts/dispersion-exponential-smoothing/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/quantile-exponential-smoothing/>Quantile exponential smoothing</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-05-04>May 4, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a>
<a href=https://aakinshin.net/tags/exponential-smoothing/ class="badge badge-info">Exponential smoothing</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a></span><br><br><p>One of the popular problems in time series analysis is estimating the moving &ldquo;average&rdquo; value.
Let&rsquo;s define the &ldquo;average&rdquo; as a central tendency metric like the mean or the median.
When we talk about the moving value, we assume that we are interested in
the average value &ldquo;at the end&rdquo; of the time series
instead of the average of all available observations.</p><p>One of the most straightforward approaches to estimate the moving average is the <em>simple moving mean</em>.
Unfortunately, this approach is not robust: outliers can instantly spoil the evaluated mean value.
As an alternative, we can consider <em>simple moving median</em>.
I already discussed a few of such methods:
<a href=https://aakinshin.net/posts/mp2-quantile-estimator/>the MP² quantile estimator</a> and
<a href=https://aakinshin.net/posts/partitioning-heaps-quantile-estimator2/>a moving quantile estimator based on partitioning heaps</a>
(a modification of the Hardle-Steiger method).
When we talk about <em>simple moving averages</em>, we typically assume
that we estimate the average value over the last <span class="math inline">\(k\)</span> observations (<span class="math inline">\(k\)</span> is the <em>window size</em>).
This approach is also known as <em>unweighted moving averages</em> because
all target observations have the same weight.</p><p>As an alternative to the simple moving average, we can also consider the <em>weighted moving average</em>.
In this case, we assign a weight for each observation and aggregate the whole time series according to these weights.
A famous example of such a weight function is <em>exponential smoothing</em>.
And the simplest form of exponential smoothing is the <em>exponentially weighted moving mean</em>.
This approach estimates the weighted moving mean using exponentially decreasing weights.
Switching from the simple moving mean to the exponentially weighted moving mean provides some benefits
in terms of smoothness and estimation efficiency.</p><p>Although exponential smoothing has advantages over the simple moving mean,
it still estimates the mean value which is not robust.
We can improve the robustness of this approach if we reuse the same idea for weighted moving quantiles.
It&rsquo;s possible because the quantiles also can be estimated for weighted samples.
In one of my previous posts, I <a href=https://aakinshin.net/posts/weighted-quantiles/>showed</a> how to adapt
the Hyndman-Fan Type 7 and Harrell-Davis quantile estimators to the weighted samples.
In this post, I&rsquo;m going to show how we can use this technique to estimate
the weighted moving quantiles using exponentially decreasing weights.</p><br><a href=/posts/quantile-exponential-smoothing/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/qrde-discrete/>Improving quantile-respectful density estimation for discrete distributions using jittering</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-04-27>April 27, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/qrde/ class="badge badge-info">QRDE</a>
<a href=https://aakinshin.net/tags/density-estimation/ class="badge badge-info">Density estimation</a>
<a href=https://aakinshin.net/tags/discrete-distribution/ class="badge badge-info">Discrete distribution</a>
<a href=https://aakinshin.net/tags/ties/ class="badge badge-info">Ties</a>
<a href=https://aakinshin.net/tags/jittering/ class="badge badge-info">Jittering</a></span><br><br><p>In my previous posts, I already discussed the <a href=https://aakinshin.net/posts/kde-discrete/>problem</a> that arise
when we try to build the kernel density estimation (KDE) for samples with ties.
We may get such samples in real life from discrete or mixed discrete/continuous distributions.
Even if the original distribution is continuous,
we may observe artificial sample discretization due to a limited resolution of the measuring tool.
Such discretization may lead to inaccurate density plots due to undersmoothing.
The problem can be resolved using a nice technique called <em>jittering</em>.
I also discussed <a href=https://aakinshin.net/posts/discrete-sample-jittering/>how to apply</a> jittering to get a smoother version of KDE.</p><p>However, I&rsquo;m not a huge fan of KDE because of two reasons.
The first one is the <a href=https://aakinshin.net/posts/kde-bw/>problem of choosing a proper bandwidth value</a>.
With poorly chosen bandwidth, we can easily get oversmoothing or undersmoothing even without the discretization problem.
The second one is an inconsistency between the KDE-based probability density function and evaluated sample quantiles.
It could lead to inconsistent visualizations (e.g., KDE-based violin plots with non-KDE-based quantile values)
or it could introduce problems for algorithms that require density function and quantile values at the same time.
The inconsistency could be resolved using <a href=https://aakinshin.net/posts/qrde-hd/>quantile-respectful density estimation</a> (QRDE).
This kind of estimation builds the density function which matches the evaluated sample quantiles.
To get a smooth QRDE, we also need a smooth quantile estimator like the Harrell-Davis quantile estimator.
The robustness and componential efficiency of this approach can be improved using
the <a href=https://aakinshin.net/posts/winsorized-hdqe/>winsorized</a> and <a href=https://aakinshin.net/posts/trimmed-hdqe/>trimmed</a>
modifications of the Harrell-Davis quantile estimator
(which also have a <a href=https://aakinshin.net/posts/wthdqe-efficiency/>decent statistical efficiency level</a>).</p><p>Unfortunately, the straightforward QRDE calculation is not always applicable for samples with ties
because it&rsquo;s impossible to build an &ldquo;honest&rdquo; density function for discrete distributions
without using the Dirac delta function.
This is a severe problem for QRDE-based algorithms like the
<a href=https://aakinshin.net/posts/lowland-multimodality-detection/>lowland multimodality detection algorithm</a>.
In this post, I will show how jittering could help to solve this problem and get a smooth QRDE on samples with ties.</p><br><a href=/posts/qrde-discrete/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/discrete-sample-jittering/>How to build a smooth density estimation for a discrete sample using jittering</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-04-20>April 20, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/density-estimation/ class="badge badge-info">Density estimation</a>
<a href=https://aakinshin.net/tags/discrete-distributions/ class="badge badge-info">Discrete distributions</a>
<a href=https://aakinshin.net/tags/ties/ class="badge badge-info">Ties</a>
<a href=https://aakinshin.net/tags/jittering/ class="badge badge-info">Jittering</a></span><br><br><p>Let&rsquo;s say you have a sample with tied values.
If you draw a kernel density estimation (KDE) for such a sample,
you may get a serrated pattern like this:</p><div class=row><div class=mx-auto><a href=/posts/discrete-sample-jittering/img/intro-light.png target=_blank class=imgldlink alt=intro><picture>
<source theme=dark srcset=/posts/discrete-sample-jittering/img/intro-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/discrete-sample-jittering/img/intro-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/discrete-sample-jittering/img/intro-light.png></picture></a></div></div><br><p>KDE requires samples from continuous distributions
while tied values arise in discrete or mixture distributions.
Even if the original distribution is continuous,
you may observe artificial sample discretization due to a limited resolution of the measuring tool.
This effect may lead to distorted density plots like in the above picture.</p><p>The problem could be solved using a nice technique called <em>jittering</em>.
In the simplest case, jittering just adds random noise to each measurement.
Such a trick removes all ties from the sample and allows building a smooth density estimation.</p><p>However, there are many different ways to apply jittering.
The trickiest question here is how to choose proper noise values.
In this post, I want to share one of my favorite jittering approaches.
It generates a non-randomized noise pattern with a low risk of noticeable sample corruption.</p><br><a href=/posts/discrete-sample-jittering/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/kde-discrete/>Kernel density estimation and discrete values</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-04-13>April 13, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>&nbsp;
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/density-estimation/ class="badge badge-info">Density estimation</a>
<a href=https://aakinshin.net/tags/discrete-distribution/ class="badge badge-info">Discrete distribution</a>
<a href=https://aakinshin.net/tags/ties/ class="badge badge-info">Ties</a>
<a href=https://aakinshin.net/tags/kde/ class="badge badge-info">KDE</a></span><br><br><p>Kernel density estimation (KDE) is a popular technique of data visualization.
Based on the given sample, it allows estimating the probability density function (PDF) of the underlying distribution.
Here is an example of KDE for <code>x = {3.82, 4.61, 4.89, 4.91, 5.31, 5.6, 5.66, 7.00, 7.00, 7.00}</code>
(normal kernel, Sheather & Jones bandwidth selector):</p><div class=row><div class=mx-auto><a href=/posts/kde-discrete/img/intro-light.png target=_blank class=imgldlink alt=intro><picture>
<source theme=dark srcset=/posts/kde-discrete/img/intro-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/kde-discrete/img/intro-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/kde-discrete/img/intro-light.png></picture></a></div></div><br><p>KDE is a simple and straightforward way to build a PDF, but it&rsquo;s not always the best one.
In addition to my <a href=https://aakinshin.net/posts/kde-bw/>concerns about bandwidth selection</a>,
continuous use of KDE creates an illusion that all distributions are smooth and continuous.
In practice, it&rsquo;s not always true.</p><p>In the above picture, the distribution looks pretty continuous.
However, the picture hides the fact that we have three <code>7.00</code> elements in the original sample.
With continuous distributions, the probability of getting tied observations (that have the same value) is almost zero.
If a sample contains ties, we are most likely working with
either a discrete distribution or a mixture of discrete and continuous distributions.
A KDE for such a sample may significantly differ from the actual PDF.
Thus, this technique may mislead us instead of providing insights about the true underlying distribution.</p><p>In this post, we discuss the usage of PDF and PMF with continuous and discrete distributions.
Also, we look at examples of corrupted density estimation plots for distributions with discrete features.</p><br><a href=/posts/kde-discrete/>Read more</a><br><br><hr></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/3/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item active"><a class=page-link href=/page/4/>4</a></li><li class=page-item><a class=page-link href=/page/5/>5</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/13/>13</a></li><li class=page-item><a href=/page/5/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/13/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></main></div><footer class=blog-footer><div class=container><p>&copy; 2013&mdash;2022 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub></i></a><a href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter></i></a><a href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS></i></a>|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.082780cb948cc57eabbc0d80172a7a5347d5572b599938b4c9876dfd7978b424.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/anchor.min.js></script><script src=https://aakinshin.net/js/custom.min.11932490dde776463ed165b345838c701accc0cfdedf2e0868f13415f41f0872.js></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true,},options:{skipHtmlTags:['script','noscript','style','textarea','pre'],ignoreHtmlClass:'tex2jax_ignore',processHtmlClass:'tex2jax_process'}};</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>