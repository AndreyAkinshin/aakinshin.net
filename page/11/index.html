<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.103.0-DEV"><meta name=author content="Andrey Akinshin"><link href=/img/favicon.ico rel=icon type=image/x-icon><meta name=keywords content><title>Andrey Akinshin's blog (Page 11)</title><meta name=description content="Andrey Akinshin's blog"><meta name=twitter:site content="@andrey_akinshin"><meta name=twitter:creator content="@andrey_akinshin"><link href=https://aakinshin.net/sass/bootstrap-light.min.css theme=light rel=stylesheet type=text/css media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><link href=https://aakinshin.net/sass/bootstrap-dark.min.css theme=dark rel=stylesheet type=text/css media="(prefers-color-scheme: dark)"><script src=https://aakinshin.net/js/theme-before.min.d7f6578e23ad3f0c33c589f9bcb34f995b45121f1f3ce888869c86b26826c845.js></script>
<script type=module src=https://aakinshin.net/js/dark-mode-toggle.min.9e68d431432744c07a43381a8a833ffe0921cc0e006d84ad97f0b0d43c5cd82d.mjs></script>
<link href=https://aakinshin.net/css/fontawesome-all.min.178e95bba6ea2e9a90838cd646658f4bf6667a6ceaa057cb587bf7e6eb8412d3.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/about.min.60773ab7266ecc6e9625306f57c0a82a219b011dc3ea2d83763d1ecdf11c86d2.css rel=stylesheet type=text/css media=all><link href=https://aakinshin.net/css/blog.min.7e186db80d8c431d196b3e0718afb0636b82a269a8c92521c11a55267a24fc27.css rel=stylesheet type=text/css media=all><link rel=alternate type=application/rss+xml href=https://aakinshin.net/posts/index.xml title="RSS Feed"><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVB6MXSX32"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZVB6MXSX32")</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-41419012-5"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-41419012-5")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(28700916,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/28700916 style=position:absolute;left:-9999px alt></div></noscript><script src=/js/jquery-3.3.1.slim.min.js></script></head><body class="d-flex flex-column min-vh-100"><div class=bg-primary><div class="container bg-primary"><nav class="navbar navbar-expand-sm navbar-dark bg-primary"><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link id=nav-link-blog href=https://aakinshin.net/><i class="fas fa-home" title=Home style=color:#fff></i></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-expanded=false>Blog</a><ul class="dropdown-menu bg-primary"><li><a class=dropdown-item href=https://aakinshin.net/posts/>All Posts</a></li><li><a class=dropdown-item href=https://aakinshin.net/tags/statistics/>Posts about Statistics</a></li><li></li><a class=dropdown-item href=https://aakinshin.net/tags/>All Tags</a></li></ul></li><li class=nav-item><a class=nav-link id=nav-link-about href=https://aakinshin.net/about/>About</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-expanded=false>EN</a><ul class="dropdown-menu bg-primary"><li></li><a class=dropdown-item href=https://aakinshin.net/ru/>RU</a></li></ul></li></ul><ul class="navbar-nav ms-auto"><li class=nav-item><dark-mode-toggle permanent=true></dark-mode-toggle></li></ul></nav></div></div><div class=container><main id=main><div class=blog-main><div class=blog-post><h2 class=blog-post-title><a href=/posts/unbiased-mad/>Unbiased median absolute deviation</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-09>February 9, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/research-unbiased-mad/ class="badge badge-info">Research: Unbiased median absolute deviation</a></span><br><br><p>The <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (<span class="math inline">\(\textrm{MAD}\)</span>)
is a robust measure of scale.
For distribution <span class="math inline">\(X\)</span>, it can be calculated as follows:</p><p><span class="math display">\[\textrm{MAD} = C \cdot \textrm{median}(|X - \textrm{median}(X)|)
\]</span></p><p>where <span class="math inline">\(C\)</span> is a constant scale factor.
This metric can be used as a robust alternative to the standard deviation.
If we want to use the <span class="math inline">\(\textrm{MAD}\)</span> as a <a href=https://en.wikipedia.org/wiki/Consistent_estimator>consistent estimator</a>
for the standard deviation under the normal distribution,
we should set</p><p><span class="math display">\[C = C_{\infty} = \dfrac{1}{\Phi^{-1}(3/4)} \approx 1.4826022185056.
\]</span></p><p>where <span class="math inline">\(\Phi^{-1}\)</span> is the quantile function of the standard normal distribution
(or the inverse of the cumulative distribution function).
If <span class="math inline">\(X\)</span> is the normal distribution, we get <span class="math inline">\(\textrm{MAD} = \sigma\)</span> where <span class="math inline">\(\sigma\)</span> is the standard deviation.</p><p>Now let&rsquo;s consider a sample <span class="math inline">\(x = \{ x_1, x_2, \ldots x_n \}\)</span>.
Let&rsquo;s denote the median absolute deviation for a sample of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\textrm{MAD}_n\)</span>.
The corresponding equation looks similar to the definition of <span class="math inline">\(\textrm{MAD}\)</span> for a distribution:</p><p><span class="math display">\[\textrm{MAD}_n = C_n \cdot \textrm{median}(|x - \textrm{median}(x)|).
\]</span></p><p>Let&rsquo;s assume that <span class="math inline">\(\textrm{median}\)</span> is the straightforward definition of the median
(if <span class="math inline">\(n\)</span> is odd, the median is the middle element of the sorted sample,
if <span class="math inline">\(n\)</span> is even, the median is the arithmetic average of the two middle elements of the sorted sample).
We still can use <span class="math inline">\(C_n = C_{\infty}\)</span> for extremely large sample sizes.
However, for small <span class="math inline">\(n\)</span>, <span class="math inline">\(\textrm{MAD}_n\)</span> becomes a <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>biased estimator</a>.
If we want to get an unbiased version, we should adjust the value of <span class="math inline">\(C_n\)</span>.</p><p>In this post, we look at the possible approaches and learn the way to get the exact value of <span class="math inline">\(C_n\)</span>
that makes <span class="math inline">\(\textrm{MAD}_n\)</span> unbiased estimator of the median absolute deviation for any <span class="math inline">\(n\)</span>.</p><br><a href=/posts/unbiased-mad/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/comparing-distributions-using-gamma-es/>Comparing distribution quantiles using gamma effect size</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-02-02>February 2, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/effect-size/ class="badge badge-info">Effect Size</a>
<a href=https://aakinshin.net/tags/research-gamma-es/ class="badge badge-info">Research: Gamma Effect Size</a></span><br><br><p>There are several ways to describe the difference between two distributions.
Here are a few examples:</p><ul><li>Effect sizes based on differences between means (e.g., Cohen&rsquo;s d, Glass&rsquo; Δ, Hedges&rsquo; g)</li><li><a href=https://aakinshin.net/posts/shift-and-ratio-functions/>The shift and ration functions</a> that
estimate differences between matched quantiles.</li></ul><p>In one of the previous post, I <a href=https://aakinshin.net/posts/nonparametric-effect-size/>described</a>
the gamma effect size which is defined not for the mean but for quantiles.
In this post, I want to share a few case studies that demonstrate
how the suggested metric combines the advantages of the above approaches.</p><br><a href=/posts/comparing-distributions-using-gamma-es/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/cohend-and-outliers/>A single outlier could completely distort your Cohen's d value</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-26>January 26, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/effect-size/ class="badge badge-info">Effect Size</a>
<a href=https://aakinshin.net/tags/research-gamma-es/ class="badge badge-info">Research: Gamma Effect Size</a></span><br><br><p><a href="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen&rsquo;s d</a> is a popular way to estimate
the <a href=https://en.wikipedia.org/wiki/Effect_size>effect size</a> between two samples.
It works excellent for perfectly normal distributions.
Usually, people think that slight deviations from normality
shouldn&rsquo;t produce a noticeable impact on the result.
Unfortunately, it&rsquo;s not always true.
In fact, a single outlier value can completely distort the result even in large samples.</p><p>In this post, I will present some illustrations for this problem and will show how to fix it.</p><br><a href=/posts/cohend-and-outliers/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/partitioning-heaps-quantile-estimator2/>Better moving quantile estimations using the partitioning heaps</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-19>January 19, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a></span><br><br><p>In one of the previous posts, I <a href=https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/>have discussed</a> the Hardle-Steiger method.
This algorithm allows estimating <a href=https://en.wikipedia.org/wiki/Moving_average#Moving_median>the moving median</a>
using <span class="math inline">\(O(L)\)</span> memory and <span class="math inline">\(O(log(L))\)</span> element processing complexity (where <span class="math inline">\(L\)</span> is the window size).
Also, I have shown how to adapt this approach to estimate <em>any</em> moving quantile.</p><p>In this post, I&rsquo;m going to present further improvements.
The Hardle-Steiger method always returns the <a href=https://en.wikipedia.org/wiki/Order_statistic>order statistics</a>
which is the <span class="math inline">\(k\textrm{th}\)</span> smallest element from the sample.
It means that the estimated quantile value always equals one of the last <span class="math inline">\(L\)</span> observed numbers.
However, many of the classic quantile estimators use two elements.
For example, if we want to estimate the median for <span class="math inline">\(x = \{4, 5, 6, 7\}\)</span>,
some estimators return <span class="math inline">\(5.5\)</span> (which is the arithmetical mean of <span class="math inline">\(5\)</span> and <span class="math inline">\(6\)</span>)
instead of <span class="math inline">\(5\)</span> or <span class="math inline">\(6\)</span> (which are order statistics).</p><p>Let&rsquo;s learn how to implement a moving version of such estimators using
the partitioning heaps from the Hardle-Steiger method.</p><br><a href=/posts/partitioning-heaps-quantile-estimator2/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/mp2-quantile-estimator/>MP² quantile estimator: estimating the moving median without storing values</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-12>January 12, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a>
<a href=https://aakinshin.net/tags/research-p2qe/ class="badge badge-info">Research: P² quantile estimator</a></span><br><br><p>In one of the previous posts, I <a href=https://aakinshin.net/posts/p2-quantile-estimator/>described</a> the P² quantile estimator.
It allows estimating quantiles on a stream of numbers without storing them.
Such sequential (streaming/online) quantile estimators are useful in software telemetry because
they help to evaluate the median and other distribution quantiles without a noticeable memory footprint.</p><p>After the publication, I got a lot of questions about <em>moving</em> sequential quantile estimators.
Such estimators return quantile values not for the whole stream of numbers,
but only for the recent values.
So, I <a href=https://aakinshin.net/posts/partitioning-heaps-quantile-estimator/>wrote</a> another post about
a quantile estimator based on a partitioning heaps (inspired by the Hardle-Steiger method).
This algorithm gives you the exact value of any order statistics for the last <span class="math inline">\(L\)</span> numbers
(<span class="math inline">\(L\)</span> is known as the window size).
However, it requires <span class="math inline">\(O(L)\)</span> memory, and it takes <span class="math inline">\(O(log(L))\)</span> time to process each element.
This may be acceptable in some cases.
Unfortunately, it doesn&rsquo;t allow implementing low-overhead telemetry in the case of large <span class="math inline">\(L\)</span>.</p><p>In this post, I&rsquo;m going to present a moving modification of the P² quantile estimator.
Let&rsquo;s call it MP² (moving P²).
It requires <span class="math inline">\(O(1)\)</span> memory, it takes <span class="math inline">\(O(1)\)</span> to process each element,
and it supports windows of any size.
Of course, we have a trade-off with the estimation accuracy:
it returns a quantile approximation instead of the exact order statistics.
However, in most cases, the MP² estimations are pretty accurate from the practical point of view.</p><p>Let&rsquo;s discuss MP² in detail!</p><br><a href=/posts/mp2-quantile-estimator/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/cs-mad-hd-gumbel/>Case study: Accuracy of the MAD estimation using the Harrell-Davis quantile estimator (Gumbel distribution)</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2021-01-05>January 5, 2021</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/case-study/ class="badge badge-info">Case study</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/mad/ class="badge badge-info">MAD</a>
<a href=https://aakinshin.net/tags/harrell-davis-quantile-estimator/ class="badge badge-info">Harrell-Davis quantile estimator</a></span><br><br><p>In some of my previous posts, I used
the <a href=https://en.wikipedia.org/wiki/Median_absolute_deviation>median absolute deviation</a> (MAD)
to describe the distribution dispersion:</p><ul><li><a href=https://aakinshin.net/posts/harrell-davis-double-mad-outlier-detector/>DoubleMAD outlier detector based on the Harrell-Davis quantile estimator</a></li><li><a href=https://aakinshin.net/posts/nonparametric-effect-size/>Nonparametric Cohen&rsquo;s d-consistent effect size</a></li><li><a href=https://aakinshin.net/posts/qad/>Quantile absolute deviation: estimating statistical dispersion around quantiles</a></li></ul><p>The MAD estimation depends on the chosen median estimator:
we may get different MAD values with different median estimators.
To get better accuracy,
I always encourage readers to use the Harrell-Davis quantile estimator
instead of the classic Type 7 quantile estimator.</p><p>In this case study, I decided to compare these two quantile estimators using
the <a href=https://en.wikipedia.org/wiki/Gumbel_distribution>Gumbel distribution</a>
(it&rsquo;s a good model for slightly right-skewed distributions).
According to the performed Monte Carlo simulation,
the Harrell-Davis quantile estimator always has better accuracy:</p><div class=row><div class=mx-auto><a href=/posts/cs-mad-hd-gumbel/img/summary-light.png target=_blank class=imgldlink alt=summary><picture><source theme=dark srcset=/posts/cs-mad-hd-gumbel/img/summary-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/cs-mad-hd-gumbel/img/summary-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/cs-mad-hd-gumbel/img/summary-light.png></picture></a></div></div><br><br><a href=/posts/cs-mad-hd-gumbel/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/partitioning-heaps-quantile-estimator/>Fast implementation of the moving quantile based on the partitioning heaps</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2020-12-29>December 29, 2020</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/moving-quantile/ class="badge badge-info">Moving Quantile</a></span><br><br><p>Imagine you have a time series.
Let&rsquo;s say, after each new observation, you want to know an &ldquo;average&rdquo; value across the last <span class="math inline">\(L\)</span> observations.
Such a metric is known as <a href=https://en.wikipedia.org/wiki/Moving_average>a moving average</a>
(or rolling/running average).</p><p>The most popular moving average example is <a href=https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average>the moving mean</a>.
It&rsquo;s easy to efficiently implement this metric.
However, it has a major drawback: it&rsquo;s not robust.
Outliers can easily spoil the moving mean and transform it into a meaningless and untrustable metric.</p><p>Fortunately, we have a good alternative: <a href=https://en.wikipedia.org/wiki/Moving_average#Moving_median>the moving median</a>.
Typically, it generates a stable and smooth series of values.
In the below figure, you can see the difference between the moving mean and the moving median on noisy data.</p><div class=row><div class=mx-auto><a href=/posts/partitioning-heaps-quantile-estimator/img/example-light.png target=_blank class=imgldlink alt=example><picture><source theme=dark srcset=/posts/partitioning-heaps-quantile-estimator/img/example-dark.png media="(prefers-color-scheme: dark)"><source theme=light srcset=/posts/partitioning-heaps-quantile-estimator/img/example-light.png media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img class="mx-auto d-block img-fluid" width=800 src=/posts/partitioning-heaps-quantile-estimator/img/example-light.png></picture></a></div></div><br><p>The moving median also has a drawback: it&rsquo;s not easy to efficiently implement it.
Today we going to discuss the Hardle-Steiger method to estimate the median
(memory: <span class="math inline">\(O(L)\)</span>, element processing complexity: <span class="math inline">\(O(log(L))\)</span>, median estimating complexity: <span class="math inline">\(O(1)\)</span>).
Also, we will learn how to calculate <em>the moving quantiles</em> based on this method.</p><p>In this post, you will find the following:</p><ul><li>An overview of the Hardle-Steiger method</li><li>A simple way to implement the Hardle-Steiger method</li><li>Moving quantiles inspired by the Hardle-Steiger method</li><li>How to process initial elements</li><li>Reference C# implementation</li></ul><br><a href=/posts/partitioning-heaps-quantile-estimator/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/quantile-ci-coverage/>Coverage of quantile confidence intervals</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2020-12-22>December 22, 2020</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/coverage/ class="badge badge-info">Coverage</a>
<a href=https://aakinshin.net/tags/confidence-interval/ class="badge badge-info">Confidence Interval</a></span><br><br><p>There is a common <a href=https://en.wikipedia.org/wiki/Confidence_interval#Misunderstandings>misunderstanding</a>
that a 95% confidence interval is an interval that covers the true parameter value with 95% probability.
Meanwhile, the correct definition assumes that
the true parameter value will be covered by 95% of 95% confidence intervals <em>in the long run</em>.
These two statements sound similar, but there is a huge difference between them.
95% in this context is not a property of a single confidence interval.
Once you get a calculated interval, it may cover the true value (100% probability) or
it may don&rsquo;t cover it (0% probability).
In fact, 95% is a <em>prediction</em> about the percentage of <em>future</em> confidence intervals
that cover the true value <em>in the long run</em>.</p><p>However, even if you know the correct definition, you still may experience some troubles.
The first thing people usually forgot is the &ldquo;long run&rdquo; part.
For example, if we collected 100 samples and calculated a 95% confidence interval of a parameter for each of them,
we shouldn&rsquo;t expect that 95 of these intervals cover the true parameter value.
In fact, we can observe a situation when none of these intervals covers the true value.
Of course, this is an unlikely event, but if you automatically perform thousands of different experiments,
you will definitely get some extreme situations.</p><p>The second thing that may create trouble is the &ldquo;prediction&rdquo; part.
If weather forecasters predicted that it will rain tomorrow, this does not mean that it will rain tomorrow.
The same works for statistical predictions.
The actual prediction reliability may depend on many factors.
If you estimate confidence intervals around the mean for the normal distribution, you are most likely safe.
However, if you estimate confidence intervals around quantiles for non-parametric distributions,
you should care about the following things:</p><ul><li>The used approach to estimate confidence intervals</li><li>The underlying distribution</li><li>The sample size</li><li>The position of the target quantile</li></ul><p>I <a href=https://aakinshin.net/posts/weighted-quantiles-ci/>have already showed</a> how to estimate
the confidence interval around the given quantile using the Maritz-Jarrett method.
It&rsquo;s time to verify the reliability of this approach.
In this post, I&rsquo;m going to show some Monte-Carlo simulations that evaluate the coverage percentage in different situations.</p><br><a href=/posts/quantile-ci-coverage/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/statistics-for-performance/>Statistical approaches for performance analysis</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2020-12-15>December 15, 2020</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/performance-analysis/ class="badge badge-info">Performance analysis</a></span><br><br><p>Software performance is a complex discipline that requires knowledge in different areas
from benchmarking to the internals of modern runtimes, operating systems, and hardware.
Surprisingly, the most difficult challenges in performance analysis are not about programming,
they are about mathematical statistics!</p><p>Many software developers can drill into performance problems and implement excellent optimizations,
but they are not always know how to correctly verify these optimizations.
This may not look like a problem in the case of a single performance investigation.
However, the situation became worse when developers try to set up an infrastructure that
should automatically find performance problems or prevent degradations from merging.
In order to make such an infrastructure reliable and useful,
it&rsquo;s crucial to achieve an extremely low false-positive rate (otherwise, it&rsquo;s not trustable)
and be able to detect most of the degradations (otherwise, it&rsquo;s not so useful).
It&rsquo;s not easy if you don&rsquo;t know which statistical approaches should be used.
If you try to google it, you may find thousands of papers about statistics,
but only a small portion of them really works in practice.</p><p>In this post, I want to share some approaches that I use for performance analysis in everyday life.
I have been analyzing performance distributions for the last seven years,
and I have found a lot of approaches, metrics, and tricks which nice to have
in your statistical toolbox.
I would not say that all of them are must have to know,
but they can definitely help you to improve the reliability of your statistical checks
in different problems of performance analysis.
Consider the below list as a letter to a younger version of myself with a brief list of topics that are good to learn.</p><br><a href=/posts/statistics-for-performance/>Read more</a><br><br><hr></div><div class=blog-post><h2 class=blog-post-title><a href=/posts/weighted-quantiles-ci/>Quantile confidence intervals for weighted samples</a></h2><span class=blog-post-meta><div class=faicon><i class="far fa-calendar-alt"></i></div><time datetime=2020-12-08>December 8, 2020</time>
&nbsp;&nbsp;
<i class="fas fa-tag"></i>
&nbsp;
<a href=https://aakinshin.net/tags/mathematics/ class="badge badge-info">Mathematics</a>
<a href=https://aakinshin.net/tags/statistics/ class="badge badge-info">Statistics</a>
<a href=https://aakinshin.net/tags/research/ class="badge badge-info">Research</a>
<a href=https://aakinshin.net/tags/research-wqe/ class="badge badge-info">Research: Weighted quantile estimators</a>
<a href=https://aakinshin.net/tags/quantile/ class="badge badge-info">Quantile</a>
<a href=https://aakinshin.net/tags/confidence-interval/ class="badge badge-info">Confidence Interval</a></span><br><br><p><strong>Update 2021-07-06:
the approach was updated using the <a href=https://aakinshin.net/posts/kish-ess-weighted-quantiles/>Kish&rsquo;s effective sample size</a>.</strong></p><p>When you work with non-parametric distributions,
quantile estimations are essential to get the main distribution properties.
Once you get the estimation values, you may be interested in measuring the accuracy of these estimations.
Without it, it&rsquo;s hard to understand how trustable the obtained values are.
One of the most popular ways to evaluate accuracy is confidence interval estimation.</p><p>Now imagine that you collect some measurements every day.
Each day you get a small sample of values that is not enough to get the accurate daily quantile estimations.
However, the full time-series over the last several weeks has a decent size.
You suspect that past measurements should be similar to today measurements,
but you are not 100% sure about it.
You feel a temptation to extend the up-to-date sample by the previously collected values,
but it may spoil the estimation (e.g., in the case of recent change points or positive/negative trends).</p><p>One of the possible approaches in this situation is to use <em>weighted samples</em>.
This assumes that we add past measurements to the &ldquo;today sample,&rdquo;
but these values should have smaller weight.
The older measurement we take, the smaller weight it gets.
If you have consistent values across the last several days,
this approach works like a charm.
If you have any recent changes, you can detect such situations by huge confidence intervals
due to the sample inconsistency.</p><p>So, how do we estimate confidence intervals around quantiles for the weighted samples?
In one of the previous posts, I have already shown how to <a href=https://aakinshin.net/posts/weighted-quantiles/>estimate quantiles on weighted samples</a>.
In this post, I will show how to estimate quantile confidence intervals for weighted samples.</p><br><a href=/posts/weighted-quantiles-ci/>Read more</a><br><br><hr></div></div><div class=paginator><ul class="pagination pagination-default"><li class=page-item><a href=/ aria-label=First class=page-link role=button><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/10/ aria-label=Previous class=page-link role=button><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a href=/page/9/ aria-label="Page 9" class=page-link role=button>9</a></li><li class=page-item><a href=/page/10/ aria-label="Page 10" class=page-link role=button>10</a></li><li class="page-item active"><a aria-current=page aria-label="Page 11" class=page-link role=button>11</a></li><li class=page-item><a href=/page/12/ aria-label="Page 12" class=page-link role=button>12</a></li><li class=page-item><a href=/page/13/ aria-label="Page 13" class=page-link role=button>13</a></li><li class=page-item><a href=/page/12/ aria-label=Next class=page-link role=button><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/18/ aria-label=Last class=page-link role=button><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></main></div><footer class="blog-footer mt-auto"><div class=container><p>&copy; 2013&mdash;2022 Andrey Akinshin
|
<a href=https://github.com/AndreyAkinshin><i class="fab fa-github" title=GitHub></i></a>
<a href=https://twitter.com/andrey_akinshin><i class="fab fa-twitter" title=Twitter></i></a>
<a href=https://aakinshin.net/posts/index.xml><i class="fas fa-rss" title=RSS></i></a>
|
<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></p></div></footer><script src=https://aakinshin.net/js/theme-after.min.c26550621ac13c2221d7f6f5e6fe862345d3919723c50d730893e3e4856ecf35.js></script>
<script src=/js/bootstrap.bundle.min.js></script>
<script src=/js/anchor.min.js></script>
<script src=https://aakinshin.net/js/custom.min.f6e5d13fe39f305056cc743ee4cb8d117c1aba26176e58c82c79a480d02fe4b7.js></script>
<script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>